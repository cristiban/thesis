@article{abbottSynapticDepressionCortical1997,
  title = {Synaptic {{Depression}} and {{Cortical Gain Control}}},
  author = {Abbott, L. F. and Varela, J. A. and Sen, Kamal and Nelson, S. B.},
  date = {1997-01-10},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {275},
  number = {5297},
  pages = {221--224},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.275.5297.221},
  url = {https://www.science.org/doi/10.1126/science.275.5297.221},
  urldate = {2025-01-27},
  abstract = {Cortical neurons receive synaptic inputs from thousands of afferents that fire action potentials at rates ranging from less than 1 hertz to more than 200 hertz. Both the number of afferents and their large dynamic range can mask changes in the spatial and temporal pattern of synaptic activity, limiting the ability of a cortical neuron to respond to its inputs. Modeling work based on experimental measurements indicates that short-term depression of intracortical synapses provides a dynamic gain-control mechanism that allows equal percentage rate changes on rapidly and slowly firing afferents to produce equal postsynaptic responses. Unlike inhibitory and adaptive mechanisms that reduce responsiveness to all inputs, synaptic depression is input-specific, leading to a dramatic increase in the sensitivity of a neuron to subtle changes in the firing patterns of its afferents.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/B3BJZH8K/Abbott et al. - 1997 - Synaptic Depression and Cortical Gain Control.pdf}
}

@article{abbottSynapticDepressionCortical1997a,
  title = {Synaptic {{Depression}} and {{Cortical Gain Control}}},
  author = {Abbott, L. F. and Varela, J. A. and Sen, Kamal and Nelson, S. B.},
  date = {1997-01-10},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {275},
  number = {5297},
  pages = {221--224},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.275.5297.221},
  url = {https://www.science.org/doi/10.1126/science.275.5297.221},
  urldate = {2025-01-27},
  abstract = {Cortical neurons receive synaptic inputs from thousands of afferents that fire action potentials at rates ranging from less than 1 hertz to more than 200 hertz. Both the number of afferents and their large dynamic range can mask changes in the spatial and temporal pattern of synaptic activity, limiting the ability of a cortical neuron to respond to its inputs. Modeling work based on experimental measurements indicates that short-term depression of intracortical synapses provides a dynamic gain-control mechanism that allows equal percentage rate changes on rapidly and slowly firing afferents to produce equal postsynaptic responses. Unlike inhibitory and adaptive mechanisms that reduce responsiveness to all inputs, synaptic depression is input-specific, leading to a dramatic increase in the sensitivity of a neuron to subtle changes in the firing patterns of its afferents.},
  file = {/home/cristi/Zotero/storage/9HLFYRSP/Abbott et al. - 1997 - Synaptic Depression and Cortical Gain Control.pdf}
}

@article{adlerComparingBayesianNonBayesian2018,
  title = {Comparing {{Bayesian}} and Non-{{Bayesian}} Accounts of Human Confidence Reports},
  author = {Adler, William T. and Ma, Wei Ji},
  editor = {Gershman, Samuel J.},
  date = {2018-11-13},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {14},
  number = {11},
  pages = {e1006572},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006572},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1006572},
  urldate = {2025-01-20},
  langid = {english},
  file = {/home/cristi/Zotero/storage/8LC9T7P2/Adler and Ma - 2018 - Comparing Bayesian and non-Bayesian accounts of human confidence reports.pdf;/home/cristi/Zotero/storage/CZCUDX8G/Adler_ma_2018.pdf}
}

@article{adlerComparingBayesianNonBayesian2018a,
  title = {Comparing {{Bayesian}} and {{Non-Bayesian Accounts}} of {{Human Confidence Reports}}},
  author = {Adler, William T. and Ma, Wei Ji},
  editor = {Gershman, Samuel J.},
  date = {2018-11-13},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {14},
  number = {11},
  pages = {e1006572},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006572},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1006572},
  urldate = {2025-01-20},
  file = {/home/cristi/Zotero/storage/VRTUG84F/Adler_ma_2018.pdf;/home/cristi/Zotero/storage/YV3RP36F/Adler and Ma - 2018 - Comparing Bayesian and non-Bayesian accounts of human confidence reports.pdf}
}

@article{aertsenSpectroTemporalReceptiveField1981,
  title = {The {{Spectro-Temporal Receptive Field}}: {{A}} Functional Characteristic of Auditory Neurons},
  shorttitle = {The {{Spectro-Temporal Receptive Field}}},
  author = {Aertsen, A. M. H. J. and Johannesma, P. I. M.},
  date = {1981},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybern.},
  volume = {42},
  number = {2},
  pages = {133--143},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00336731},
  url = {http://link.springer.com/10.1007/BF00336731},
  urldate = {2025-01-16},
  langid = {english},
  file = {/home/cristi/Zotero/storage/VZMK3Q7A/Aertsen and Johannesma - 1981 - The Spectro-Temporal Receptive Field A functional characteristic of auditory neurons.pdf}
}

@article{aertsenSpectroTemporalReceptiveField1981a,
  title = {The {{Spectro-Temporal Receptive Field}}: {{A Functional Characteristic}} of {{Auditory Neurons}}},
  shorttitle = {The {{Spectro-Temporal Receptive Field}}},
  author = {Aertsen, A. M. H. J. and Johannesma, P. I. M.},
  date = {1981},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybern.},
  volume = {42},
  number = {2},
  pages = {133--143},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00336731},
  url = {http://link.springer.com/10.1007/BF00336731},
  urldate = {2025-01-16},
  file = {/home/cristi/Zotero/storage/DC3IKJEW/Aertsen and Johannesma - 1981 - The Spectro-Temporal Receptive Field A functional characteristic of auditory neurons.pdf}
}

@article{aertsenSpectrotemporalReceptiveFields1980,
  title = {Spectro-Temporal Receptive Fields of Auditory Neurons in the Grassfrog: {{I}}. {{Characterization}} of Tonal and Natural Stimuli},
  shorttitle = {Spectro-Temporal Receptive Fields of Auditory Neurons in the Grassfrog},
  author = {Aertsen, A. M. H. J. and Johannesma, P. I. M.},
  date = {1980-11},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybernetics},
  volume = {38},
  number = {4},
  pages = {223--234},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00337015},
  url = {http://link.springer.com/10.1007/BF00337015},
  urldate = {2025-01-16},
  langid = {english},
  file = {/home/cristi/Zotero/storage/LV2Q6BEW/Aertsen and Johannesma - 1980 - Spectro-temporal receptive fields of auditory neurons in the grassfrog I. Characterization of tonal.pdf}
}

@article{aertsenSpectrotemporalReceptiveFields1980a,
  title = {Spectro-Temporal Receptive Fields of Auditory Neurons in the Grassfrog: {{II}}. {{Analysis}} of the Stimulus-Event Relation for Tonal Stimuli},
  shorttitle = {Spectro-Temporal Receptive Fields of Auditory Neurons in the Grassfrog},
  author = {Aertsen, A. M. H. J. and Johannesma, P. I. M. and Hermes, D. J.},
  date = {1980-11},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybernetics},
  volume = {38},
  number = {4},
  pages = {235--248},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00337016},
  url = {http://link.springer.com/10.1007/BF00337016},
  urldate = {2025-01-16},
  langid = {english},
  file = {/home/cristi/Zotero/storage/DCFET4BV/Aertsen et al. - 1980 - Spectro-temporal receptive fields of auditory neurons in the grassfrog II. Analysis of the stimulus.pdf}
}

@article{aertsenSpectroTemporalReceptiveFields1980b,
  title = {Spectro-{{Temporal Receptive Fields}} of {{Auditory Neurons}} in the {{Grassfrog}}: {{II}}. {{Analysis}} of the {{Stimulus-Event Relation}} for {{Tonal Stimuli}}},
  shorttitle = {Spectro-{{Temporal Receptive Fields}} of {{Auditory Neurons}} in the {{Grassfrog}}},
  author = {Aertsen, A. M. H. J. and Johannesma, P. I. M. and Hermes, D. J.},
  date = {1980-11},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybernetics},
  volume = {38},
  number = {4},
  pages = {235--248},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00337016},
  url = {http://link.springer.com/10.1007/BF00337016},
  urldate = {2025-01-16},
  file = {/home/cristi/Zotero/storage/28HFICFN/Aertsen et al. - 1980 - Spectro-temporal receptive fields of auditory neurons in the grassfrog II. Analysis of the stimulus.pdf}
}

@article{aertsenSpectroTemporalReceptiveFields1980c,
  title = {Spectro-{{Temporal Receptive Fields}} of {{Auditory Neurons}} in the {{Grassfrog}}: {{I}}. {{Characterization}} of {{Tonal}} and {{Natural Stimuli}}},
  shorttitle = {Spectro-{{Temporal Receptive Fields}} of {{Auditory Neurons}} in the {{Grassfrog}}},
  author = {Aertsen, A. M. H. J. and Johannesma, P. I. M.},
  date = {1980-11},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybernetics},
  volume = {38},
  number = {4},
  pages = {223--234},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00337015},
  url = {http://link.springer.com/10.1007/BF00337015},
  urldate = {2025-01-16},
  file = {/home/cristi/Zotero/storage/8C83SGHR/Aertsen and Johannesma - 1980 - Spectro-temporal receptive fields of auditory neurons in the grassfrog I. Characterization of tonal.pdf}
}

@article{ahrensNonlinearitiesContextualInfluences2008,
  title = {Nonlinearities and {{Contextual Influences}} in {{Auditory Cortical Responses Modeled}} with {{Multilinear Spectrotemporal Methods}}},
  author = {Ahrens, Misha B. and Linden, Jennifer F. and Sahani, Maneesh},
  date = {2008-02-20},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {28},
  number = {8},
  pages = {1929--1942},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3377-07.2008},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.3377-07.2008},
  urldate = {2024-12-12},
  abstract = {The relationship between a sound and its neural representation in the auditory cortex remains elusive. Simple measures such as the frequency response area or frequency tuning curve provide little insight into the function of the auditory cortex in complex sound environments. Spectrotemporal receptive field (STRF) models, despite their descriptive potential, perform poorly when used to predict auditory cortical responses, showing that nonlinear features of cortical response functions, which are not captured by STRFs, are functionally important. We introduce a new approach to the description of auditory cortical responses, using multilinear modeling methods. These descriptions simultaneously account for several nonlinearities in the stimulus–response functions of auditory cortical neurons, including adaptation, spectral interactions, and nonlinear sensitivity to sound level. The models reveal multiple inseparabilities in cortical processing of time lag, frequency, and sound level, and suggest functional mechanisms by which auditory cortical neurons are sensitive to stimulus context. By explicitly modeling these contextual influences, the models are able to predict auditory cortical responses more accurately than are STRF models. In addition, they can explain some forms of stimulus dependence in STRFs that were previously poorly understood.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/CHX4BXXL/Ahrens et al. - 2008 - Nonlinearities and Contextual Influences in Audito.pdf}
}

@article{ahrensNonlinearitiesContextualInfluences2008a,
  title = {Nonlinearities and {{Contextual Influences}} in {{Auditory Cortical Responses Modeled}} with {{Multilinear Spectrotemporal Methods}}},
  author = {Ahrens, Misha B. and Linden, Jennifer F. and Sahani, Maneesh},
  date = {2008-02-20},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {28},
  number = {8},
  pages = {1929--1942},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3377-07.2008},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.3377-07.2008},
  urldate = {2024-12-12},
  abstract = {The relationship between a sound and its neural representation in the auditory cortex remains elusive. Simple measures such as the frequency response area or frequency tuning curve provide little insight into the function of the auditory cortex in complex sound environments. Spectrotemporal receptive field (STRF) models, despite their descriptive potential, perform poorly when used to predict auditory cortical responses, showing that nonlinear features of cortical response functions, which are not captured by STRFs, are functionally important. We introduce a new approach to the description of auditory cortical responses, using multilinear modeling methods. These descriptions simultaneously account for several nonlinearities in the stimulus–response functions of auditory cortical neurons, including adaptation, spectral interactions, and nonlinear sensitivity to sound level. The models reveal multiple inseparabilities in cortical processing of time lag, frequency, and sound level, and suggest functional mechanisms by which auditory cortical neurons are sensitive to stimulus context. By explicitly modeling these contextual influences, the models are able to predict auditory cortical responses more accurately than are STRF models. In addition, they can explain some forms of stimulus dependence in STRFs that were previously poorly understood.},
  file = {/home/cristi/Zotero/storage/V9UQLR44/Ahrens et al. - 2008 - Nonlinearities and Contextual Influences in Audito.pdf}
}

@online{AIDigitalSociety,
  title = {{{AI}} and {{Digital Society}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-intelligent-technology/ai-and-digital-society},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/AXR6UYJN/ai-and-digital-society.html}
}

@online{AIDigitalSocietya,
  title = {{{AI}} and {{Digital Society}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-intelligent-technology/ai-and-digital-society},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/9QPPFI6K/ai-and-digital-society.html}
}

@article{alexanderReadinessPotentialsDriven2016,
  title = {Readiness Potentials Driven by Non-Motoric Processes},
  author = {Alexander, Prescott and Schlegel, Alexander and Sinnott-Armstrong, Walter and Roskies, Adina L. and Wheatley, Thalia and Tse, Peter Ulric},
  date = {2016-01},
  journaltitle = {Consciousness and Cognition},
  shortjournal = {Consciousness and Cognition},
  volume = {39},
  pages = {38--47},
  issn = {10538100},
  doi = {10.1016/j.concog.2015.11.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053810015300593},
  urldate = {2024-06-24},
  langid = {english},
  file = {/home/cristi/Zotero/storage/7ZXPR7Y7/Alexander et al. - 2016 - Readiness potentials driven by non-motoric process.pdf}
}

@article{alexanderReadinessPotentialsDriven2016a,
  title = {Readiness {{Potentials Driven}} by {{Non-Motoric Processes}}},
  author = {Alexander, Prescott and Schlegel, Alexander and Sinnott-Armstrong, Walter and Roskies, Adina L. and Wheatley, Thalia and Tse, Peter Ulric},
  date = {2016-01},
  journaltitle = {Consciousness and Cognition},
  shortjournal = {Consciousness and Cognition},
  volume = {39},
  pages = {38--47},
  issn = {10538100},
  doi = {10.1016/j.concog.2015.11.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053810015300593},
  urldate = {2024-06-24},
  file = {/home/cristi/Zotero/storage/LVWF6B5U/Alexander et al. - 2016 - Readiness potentials driven by non-motoric process.pdf}
}

@online{alonsoHexMazePreviousKnowledge2018,
  title = {The {{HexMaze}}: {{A}} Previous Knowledge and Schema Task for Mice},
  shorttitle = {The {{HexMaze}}},
  author = {Alonso, Alejandra and Bokeria, Levan and Van Der Meij, Jacqueline and Samanta, Anumita and Eichler, Ronny and Spooner, Patrick and Lobato, Irene Navarro and Genzel, Lisa},
  date = {2018-10-11},
  doi = {10.1101/441048},
  url = {http://biorxiv.org/lookup/doi/10.1101/441048},
  urldate = {2024-04-20},
  abstract = {Abstract           New information is rarely learned in isolation, instead most of what we experience can be incorporated into or uses previous knowledge networks in some form. However, most rodent laboratory tasks assume the animal to be naïve with no previous experience influencing the results. Previous knowledge in form of a schema can facilitate knowledge acquisition and accelerate systems consolidation: memories become more rapidly hippocampal independent and instead rely more on the prefrontal cortex. Here, we developed a new spatial navigation task where food locations are learned in a large, gangway maze – the HexMaze. Analysing performance across sessions as well as on specific trials, we can show simple memory effects as well as multiple effects of previous knowledge accelerating both online learning and performance increases over offline periods. Importantly, we are the first to show that schema build-up is dependent on how much time passes, not how often the animal is trained.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/cristi/Zotero/storage/M2FNI25F/Alonso et al. - 2018 - The HexMaze A previous knowledge and schema task .pdf}
}

@online{alonsoHexMazePreviousKnowledge2018a,
  title = {The {{HexMaze}}: {{A Previous Knowledge}} and {{Schema Task}} for {{Mice}}},
  shorttitle = {The {{HexMaze}}},
  author = {Alonso, Alejandra and Bokeria, Levan and Van Der Meij, Jacqueline and Samanta, Anumita and Eichler, Ronny and Spooner, Patrick and Lobato, Irene Navarro and Genzel, Lisa},
  date = {2018-10-11},
  doi = {10.1101/441048},
  url = {http://biorxiv.org/lookup/doi/10.1101/441048},
  urldate = {2024-04-20},
  abstract = {Abstract New information is rarely learned in isolation, instead most of what we experience can be incorporated into or uses previous knowledge networks in some form. However, most rodent laboratory tasks assume the animal to be naïve with no previous experience influencing the results. Previous knowledge in form of a schema can facilitate knowledge acquisition and accelerate systems consolidation: memories become more rapidly hippocampal independent and instead rely more on the prefrontal cortex. Here, we developed a new spatial navigation task where food locations are learned in a large, gangway maze – the HexMaze. Analysing performance across sessions as well as on specific trials, we can show simple memory effects as well as multiple effects of previous knowledge accelerating both online learning and performance increases over offline periods. Importantly, we are the first to show that schema build-up is dependent on how much time passes, not how often the animal is trained.},
  file = {/home/cristi/Zotero/storage/QWYL73VX/Alonso et al. - 2018 - The HexMaze A previous knowledge and schema task .pdf}
}

@online{amodeiConcreteProblemsAI2016,
  title = {Concrete {{Problems}} in {{AI Safety}}},
  author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
  date = {2016},
  doi = {10.48550/ARXIV.1606.06565},
  url = {https://arxiv.org/abs/1606.06565},
  urldate = {2025-03-26},
  abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/home/cristi/Zotero/storage/AMV3VC7B/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf}
}

@article{anguita-ruizEXplainableArtificialIntelligence2020,
  title = {{{eXplainable Artificial Intelligence}} ({{XAI}}) for the Identification of Biologically Relevant Gene Expression Patterns in Longitudinal Human Studies, Insights from Obesity Research},
  author = {Anguita-Ruiz, Augusto and Segura-Delgado, Alberto and Alcalá, Rafael and Aguilera, Concepción M. and Alcalá-Fdez, Jesús},
  editor = {Althouse, Benjamin},
  date = {2020-04-10},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {16},
  number = {4},
  pages = {e1007792},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007792},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1007792},
  urldate = {2025-01-31},
  langid = {english},
  file = {/home/cristi/Zotero/storage/MFIACSSS/Anguita-Ruiz et al. - 2020 - eXplainable Artificial Intelligence (XAI) for the identification of biologically relevant gene expre.pdf}
}

@article{anguita-ruizEXplainableArtificialIntelligence2020a,
  title = {{{eXplainable Artificial Intelligence}} ({{XAI}}) for the {{Identification}} of {{Biologically Relevant Gene Expression Patterns}} in {{Longitudinal Human Studies}}, {{Insights}} from {{Obesity Research}}},
  author = {Anguita-Ruiz, Augusto and Segura-Delgado, Alberto and Alcalá, Rafael and Aguilera, Concepción M. and Alcalá-Fdez, Jesús},
  editor = {Althouse, Benjamin},
  date = {2020-04-10},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {16},
  number = {4},
  pages = {e1007792},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007792},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1007792},
  urldate = {2025-01-31},
  file = {/home/cristi/Zotero/storage/YZQQHRJD/Anguita-Ruiz et al. - 2020 - eXplainable Artificial Intelligence (XAI) for the identification of biologically relevant gene expre.pdf}
}

@online{ArtificialIntelligenceCognitive,
  title = {Artificial {{Intelligence}}: {{Cognitive Computing}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-cognitive-computing},
  urldate = {2022-10-08},
  file = {/home/cristi/Zotero/storage/VBFTUPHN/artificial-intelligence-cognitive-computing.html}
}

@online{ArtificialIntelligenceCognitivea,
  title = {Artificial {{Intelligence}}: {{Cognitive Computing}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-cognitive-computing},
  urldate = {2022-10-08},
  file = {/home/cristi/Zotero/storage/2KF9SI26/artificial-intelligence-cognitive-computing.html}
}

@online{ArtificialIntelligenceRadboud,
  title = {Artificial {{Intelligence}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/bachelors/artificial-intelligence},
  urldate = {2022-09-12},
  file = {/home/cristi/Zotero/storage/ZU6AIY99/artificial-intelligence.html}
}

@online{ArtificialIntelligenceRadbouda,
  title = {Artificial {{Intelligence}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/294PWT3E/artificial-intelligence.html}
}

@online{ArtificialIntelligenceRadboudb,
  title = {Artificial {{Intelligence}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/9T2M6EQ6/artificial-intelligence.html}
}

@online{ArtificialIntelligenceRadboudc,
  title = {Artificial {{Intelligence}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/bachelors/artificial-intelligence},
  urldate = {2022-09-12},
  file = {/home/cristi/Zotero/storage/K3SR632Z/artificial-intelligence.html}
}

@article{baillargeonPsychologicalReasoningInfancy2016,
  title = {Psychological {{Reasoning}} in {{Infancy}}},
  author = {Baillargeon, Renée and Scott, Rose M. and Bian, Lin},
  date = {2016-01-04},
  journaltitle = {Annual Review of Psychology},
  shortjournal = {Annu. Rev. Psychol.},
  volume = {67},
  number = {1},
  pages = {159--186},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-010213-115033},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-psych-010213-115033},
  urldate = {2024-01-07},
  abstract = {Adults routinely make sense of others' actions by inferring the mental states that underlie these actions. Over the past two decades, developmental researchers have made significant advances in understanding the origins of this ability in infancy. This evidence indicates that when infants observe an agent act in a simple scene, they infer the agent's mental states and then use these mental states, together with a principle of rationality (and its corollaries of efficiency and consistency), to predict and interpret the agent's subsequent actions and to guide their own actions toward the agent. In this review, we first describe the initial demonstrations of infants' sensitivity to the efficiency and consistency principles. We then examine how infants identify novel entities as agents. Next, we summarize what is known about infants' ability to reason about agents' motivational, epistemic, and counterfactual states. Finally, we consider alternative interpretations of these findings and discuss the current controversy about the relation between implicit and explicit psychological reasoning.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/T2DVADRV/Baillargeon et al. - 2016 - Psychological Reasoning in Infancy.pdf}
}

@article{baillargeonPsychologicalReasoningInfancy2016a,
  title = {Psychological {{Reasoning}} in {{Infancy}}},
  author = {Baillargeon, Renée and Scott, Rose M. and Bian, Lin},
  date = {2016-01-04},
  journaltitle = {Annual Review of Psychology},
  shortjournal = {Annu. Rev. Psychol.},
  volume = {67},
  number = {1},
  pages = {159--186},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-010213-115033},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-psych-010213-115033},
  urldate = {2024-01-07},
  abstract = {Adults routinely make sense of others' actions by inferring the mental states that underlie these actions. Over the past two decades, developmental researchers have made significant advances in understanding the origins of this ability in infancy. This evidence indicates that when infants observe an agent act in a simple scene, they infer the agent's mental states and then use these mental states, together with a principle of rationality (and its corollaries of efficiency and consistency), to predict and interpret the agent's subsequent actions and to guide their own actions toward the agent. In this review, we first describe the initial demonstrations of infants' sensitivity to the efficiency and consistency principles. We then examine how infants identify novel entities as agents. Next, we summarize what is known about infants' ability to reason about agents' motivational, epistemic, and counterfactual states. Finally, we consider alternative interpretations of these findings and discuss the current controversy about the relation between implicit and explicit psychological reasoning.},
  file = {/home/cristi/Zotero/storage/JGGJG7PS/Baillargeon et al. - 2016 - Psychological Reasoning in Infancy.pdf}
}

@article{bansalDangerousLiaisonSexually2007,
  title = {Dangerous Liaison: Sexually Transmitted Allergic Reaction to {{Brazil}} Nuts},
  shorttitle = {Dangerous Liaison},
  author = {Bansal, A. S. and Chee, R. and Nagendran, V. and Warner, A. and Hayman, G.},
  date = {2007},
  journaltitle = {Journal of Investigational Allergology \& Clinical Immunology},
  shortjournal = {J Investig Allergol Clin Immunol},
  volume = {17},
  number = {3},
  eprint = {17583107},
  eprinttype = {pubmed},
  pages = {189--191},
  issn = {1018-9068},
  abstract = {Brazil nuts are the second most frequent cause of nut allergy in the United Kingdom. We report the case of a 20-year-old woman with documented Brazil nut allergy who developed widespread urticaria and mild dyspnea after intercourse with her boyfriend who had earlier consumed Brazil nuts. Skin prick testing with the boyfriend's semen after Brazil nut consumption confirmed significant reactivity whereas a sample before nut consumption was negative. We believe this to be the first case of a sexually transmitted allergic reaction.},
  langid = {english},
  keywords = {Adult,Bertholletia,Coitus,Female,Food Hypersensitivity,Humans,Sexually Transmitted Diseases,Skin Tests},
  file = {/home/cristi/Zotero/storage/Y6QWVVLT/Bansal et al. - 2007 - Dangerous liaison sexually transmitted allergic r.pdf}
}

@article{bansalDangerousLiaisonSexually2007a,
  title = {Dangerous {{Liaison}}: {{Sexually Transmitted Allergic Reaction}} to {{Brazil Nuts}}},
  shorttitle = {Dangerous {{Liaison}}},
  author = {Bansal, A. S. and Chee, R. and Nagendran, V. and Warner, A. and Hayman, G.},
  date = {2007},
  journaltitle = {Journal of Investigational Allergology \& Clinical Immunology},
  shortjournal = {J Investig Allergol Clin Immunol},
  volume = {17},
  number = {3},
  pages = {189--191},
  issn = {1018-9068},
  abstract = {Brazil nuts are the second most frequent cause of nut allergy in the United Kingdom. We report the case of a 20-year-old woman with documented Brazil nut allergy who developed widespread urticaria and mild dyspnea after intercourse with her boyfriend who had earlier consumed Brazil nuts. Skin prick testing with the boyfriend's semen after Brazil nut consumption confirmed significant reactivity whereas a sample before nut consumption was negative. We believe this to be the first case of a sexually transmitted allergic reaction.},
  keywords = {Adult,Bertholletia,Coitus,Female,Food Hypersensitivity,Humans,Sexually Transmitted Diseases,Skin Tests},
  annotation = {\_eprinttype: pmid},
  file = {/home/cristi/Zotero/storage/9L6HL5TN/Bansal et al. - 2007 - Dangerous liaison sexually transmitted allergic r.pdf}
}

@article{barrettAnalyzingBiologicalArtificial2019,
  title = {Analyzing Biological and Artificial Neural Networks: Challenges with Opportunities for Synergy?},
  shorttitle = {Analyzing Biological and Artificial Neural Networks},
  author = {Barrett, David Gt and Morcos, Ari S and Macke, Jakob H},
  date = {2019-04},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {55},
  pages = {55--64},
  issn = {09594388},
  doi = {10.1016/j.conb.2019.01.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438818301569},
  urldate = {2024-04-25},
  langid = {english},
  file = {/home/cristi/Zotero/storage/XB3D66K8/Barrett et al. - 2019 - Analyzing biological and artificial neural network.pdf}
}

@article{barrettAnalyzingBiologicalArtificial2019a,
  title = {Analyzing {{Biological}} and {{Artificial Neural Networks}}: {{Challenges}} with {{Opportunities}} for {{Synergy}}?},
  shorttitle = {Analyzing {{Biological}} and {{Artificial Neural Networks}}},
  author = {Barrett, David Gt and Morcos, Ari S and Macke, Jakob H},
  date = {2019-04},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  volume = {55},
  pages = {55--64},
  issn = {09594388},
  doi = {10.1016/j.conb.2019.01.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438818301569},
  urldate = {2024-04-25},
  file = {/home/cristi/Zotero/storage/WYNXQY5T/Barrett et al. - 2019 - Analyzing biological and artificial neural network.pdf}
}

@article{bastosVisualAreasExert2015,
  title = {Visual {{Areas Exert Feedforward}} and {{Feedback Influences}} through {{Distinct Frequency Channels}}},
  author = {Bastos, André~Moraes and Vezoli, Julien and Bosman, Conrado~Arturo and Schoffelen, Jan-Mathijs and Oostenveld, Robert and Dowdall, Jarrod~Robert and De~Weerd, Peter and Kennedy, Henry and Fries, Pascal},
  date = {2015-01},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {85},
  number = {2},
  pages = {390--401},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.12.018},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S089662731401099X},
  urldate = {2024-05-17},
  langid = {english},
  file = {/home/cristi/Zotero/storage/4V83S39A/Bastos et al. - 2015 - Visual Areas Exert Feedforward and Feedback Influe.pdf}
}

@article{bastosVisualAreasExert2015a,
  title = {Visual {{Areas Exert Feedforward}} and {{Feedback Influences}} through {{Distinct Frequency Channels}}},
  author = {Bastos, André Moraes and Vezoli, Julien and Bosman, Conrado Arturo and Schoffelen, Jan-Mathijs and Oostenveld, Robert and Dowdall, Jarrod Robert and De Weerd, Peter and Kennedy, Henry and Fries, Pascal},
  date = {2015-01},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {85},
  number = {2},
  pages = {390--401},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.12.018},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S089662731401099X},
  urldate = {2024-05-17},
  file = {/home/cristi/Zotero/storage/4V5B2JDA/Bastos et al. - 2015 - Visual Areas Exert Feedforward and Feedback Influe.pdf}
}

@article{battledayConvolutionalNeuralNetworks2021,
  title = {From Convolutional Neural Networks to Models of Higher‐level Cognition (and Back Again)},
  author = {Battleday, Ruairidh M. and Peterson, Joshua C. and Griffiths, Thomas L.},
  date = {2021-12},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Annals of the New York Academy of Sciences},
  volume = {1505},
  number = {1},
  pages = {55--78},
  issn = {0077-8923, 1749-6632},
  doi = {10.1111/nyas.14593},
  url = {https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14593},
  urldate = {2024-04-23},
  abstract = {Abstract             The remarkable successes of convolutional neural networks (CNNs) in modern computer vision are by now well known, and they are increasingly being explored as computational models of the human visual system. In this paper, we ask whether CNNs might also provide a basis for modeling higher‐level cognition, focusing on the core phenomena of similarity and categorization. The most important advance comes from the ability of CNNs to learn high‐dimensional representations of complex naturalistic images, substantially extending the scope of traditional cognitive models that were previously only evaluated with simple artificial stimuli. In all cases, the most successful combinations arise when CNN representations are used with cognitive models that have the capacity to transform them to better fit human behavior. One consequence of these insights is a toolkit for the integration of cognitively motivated constraints back into CNN training paradigms in computer vision and machine learning, and we review cases where this leads to improved performance. A second consequence is a roadmap for how CNNs and cognitive models can be more fully integrated in the future, allowing for flexible end‐to‐end algorithms that can learn representations from data while still retaining the structured behavior characteristic of human cognition.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/87X73A96/Battleday et al. - 2021 - From convolutional neural networks to models of hi.pdf}
}

@article{battledayConvolutionalNeuralNetworks2021a,
  title = {From {{Convolutional Neural Networks}} to {{Models}} of {{Higher}}‐level {{Cognition}} (and {{Back Again}})},
  author = {Battleday, Ruairidh M. and Peterson, Joshua C. and Griffiths, Thomas L.},
  date = {2021-12},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Annals of the New York Academy of Sciences},
  volume = {1505},
  number = {1},
  pages = {55--78},
  issn = {0077-8923, 1749-6632},
  doi = {10.1111/nyas.14593},
  url = {https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14593},
  urldate = {2024-04-23},
  abstract = {Abstract The remarkable successes of convolutional neural networks (CNNs) in modern computer vision are by now well known, and they are increasingly being explored as computational models of the human visual system. In this paper, we ask whether CNNs might also provide a basis for modeling higher‐level cognition, focusing on the core phenomena of similarity and categorization. The most important advance comes from the ability of CNNs to learn high‐dimensional representations of complex naturalistic images, substantially extending the scope of traditional cognitive models that were previously only evaluated with simple artificial stimuli. In all cases, the most successful combinations arise when CNN representations are used with cognitive models that have the capacity to transform them to better fit human behavior. One consequence of these insights is a toolkit for the integration of cognitively motivated constraints back into CNN training paradigms in computer vision and machine learning, and we review cases where this leads to improved performance. A second consequence is a roadmap for how CNNs and cognitive models can be more fully integrated in the future, allowing for flexible end‐to‐end algorithms that can learn representations from data while still retaining the structured behavior characteristic of human cognition.},
  file = {/home/cristi/Zotero/storage/R66T45PZ/Battleday et al. - 2021 - From convolutional neural networks to models of hi.pdf}
}

@book{baughHistoryEnglishLanguage2002,
  title = {A {{History}} of the {{English Language}}, {{Fifth Edition}}},
  author = {Baugh, Albert C. and Cable, Thomas},
  date = {2002},
  publisher = {Pearson Education, Inc.},
  isbn = {978-0-203-99463-4},
  langid = {english},
  annotation = {OCLC: 851647310}
}

@book{baughHistoryEnglishLanguage2002a,
  title = {A {{History}} of the {{English Language}}, {{Fifth Edition}}},
  author = {Baugh, Albert C. and Cable, Thomas},
  date = {2002},
  publisher = {Pearson Education, Inc.},
  isbn = {978-0-203-99463-4}
}

@book{bearNeuroscienceExploringBrain2016,
  title = {Neuroscience: Exploring the Brain},
  shorttitle = {Neuroscience},
  author = {Bear, Mark F. and Connors, Barry W. and Paradiso, Michael A.},
  date = {2016},
  edition = {Fourth edition},
  publisher = {Wolters Kluwer},
  location = {Philadelphia},
  abstract = {"Neuroscience: Exploring the Brain surveys the organization and function of the human nervous system. We present material at the cutting edge of neuroscience in a way that is accessible to both science and nonscience students alike. The level of the material is comparable to an introductory college text in general biology. The book is divided into four parts: Part I, Foundations; Part II, Sensory and Motor Systems; Part III, The Brain and Behavior; and Part IV, The Changing Brain. We begin Part I by introducing the modern field of neuroscience and tracing some of its historical antecedents. Then we take a close look at the structure and function of individual neurons, how they communicate chemically, and how these building blocks are arranged to form a nervous system. In Part II, we go inside the brain to examine the structure and function of the systems that serve the senses and command voluntary movements. In Part III, we explore the neurobiology of human behavior, including motivation, sex, emotion, sleep, language, attention, and mental illness. Finally, in Part IV, we look at how the environment modifies the brain, both during development and in adult learning and memory"--Provided by publisher},
  isbn = {978-0-7817-7817-6},
  pagetotal = {975},
  keywords = {Brain,Neurosciences,Spinal Cord},
  file = {/home/cristi/Zotero/storage/KGBDPBSK/Bear et al. - 2016 - Neuroscience exploring the brain.pdf}
}

@book{bearNeuroscienceExploringBrain2016a,
  title = {Neuroscience: {{Exploring}} the {{Brain}}},
  shorttitle = {Neuroscience},
  author = {Bear, Mark F. and Connors, Barry W. and Paradiso, Michael A.},
  date = {2016},
  edition = {Fourth edition},
  publisher = {Wolters Kluwer},
  location = {Philadelphia},
  abstract = {"Neuroscience: Exploring the Brain surveys the organization and function of the human nervous system. We present material at the cutting edge of neuroscience in a way that is accessible to both science and nonscience students alike. The level of the material is comparable to an introductory college text in general biology. The book is divided into four parts: Part I, Foundations; Part II, Sensory and Motor Systems; Part III, The Brain and Behavior; and Part IV, The Changing Brain. We begin Part I by introducing the modern field of neuroscience and tracing some of its historical antecedents. Then we take a close look at the structure and function of individual neurons, how they communicate chemically, and how these building blocks are arranged to form a nervous system. In Part II, we go inside the brain to examine the structure and function of the systems that serve the senses and command voluntary movements. In Part III, we explore the neurobiology of human behavior, including motivation, sex, emotion, sleep, language, attention, and mental illness. Finally, in Part IV, we look at how the environment modifies the brain, both during development and in adult learning and memory"–Provided by publisher},
  isbn = {978-0-7817-7817-6},
  pagetotal = {975},
  keywords = {Brain,Neurosciences,Spinal Cord},
  file = {/home/cristi/Zotero/storage/KLWUWSTK/Bear et al. - 2016 - Neuroscience exploring the brain.pdf}
}

@article{Bender_2011,
  title = {On Achieving and Evaluating Language-Independence in {{NLP}}},
  author = {Bender, Emily M.},
  date = {2011-10},
  journaltitle = {Linguistic Issues in Language Technology},
  volume = {6},
  doi = {10.33011/lilt.v6i.1239},
  url = {https://journals.colorado.edu/index.php/lilt/article/view/1239},
  abstract = {\&amp;lt;p\&amp;gt;Language independence is commonly presented as one of the advantages of modern, machine-learning approaches to NLP, and it is an important type of scalability.\&amp;lt;/p\&amp;gt; \&amp;lt;p\&amp;gt;In this position paper, I critically review the widespread approaches to achieving and evaluating language independence in the field of computational linguistics and argue that, on the one hand, we are not truly evaluating language independence with any systematicity and on the other hand, that truly language-independent technology requires more linguistic sophistication than is the norm.\&amp;lt;/p\&amp;gt;}
}

@article{benderAchievingEvaluatingLanguageIndependence2011,
  title = {On {{Achieving}} and {{Evaluating Language-Independence}} in {{NLP}}},
  author = {Bender, Emily M.},
  date = {2011-10-01},
  journaltitle = {Linguistic Issues in Language Technology},
  shortjournal = {LiLT},
  volume = {6},
  issn = {1945-3604},
  doi = {10.33011/lilt.v6i.1239},
  url = {https://journals.colorado.edu/index.php/lilt/article/view/1239},
  urldate = {2023-02-12},
  abstract = {Language independence is commonly presented as one of the advantages of modern, machine-learning approaches to NLP, and it is an important type of scalability.  In this position paper, I critically review the widespread approaches to achieving and evaluating language independence in the field of computational linguistics and argue that, on the one hand, we are not truly evaluating language independence with any systematicity and on the other hand, that truly language-independent technology requires more linguistic sophistication than is the norm.},
  file = {/home/cristi/Zotero/storage/5793AYRY/Bender - 2011 - On Achieving and Evaluating Language-Independence .pdf}
}

@article{benderAchievingEvaluatingLanguageIndependence2011a,
  title = {On {{Achieving}} and {{Evaluating Language-Independence}} in {{NLP}}},
  author = {Bender, Emily M.},
  date = {2011-10-01},
  journaltitle = {Linguistic Issues in Language Technology},
  shortjournal = {LiLT},
  volume = {6},
  issn = {1945-3604},
  doi = {10.33011/lilt.v6i.1239},
  url = {https://journals.colorado.edu/index.php/lilt/article/view/1239},
  urldate = {2023-02-12},
  abstract = {Language independence is commonly presented as one of the advantages of modern, machine-learning approaches to NLP, and it is an important type of scalability. In this position paper, I critically review the widespread approaches to achieving and evaluating language independence in the field of computational linguistics and argue that, on the one hand, we are not truly evaluating language independence with any systematicity and on the other hand, that truly language-independent technology requires more linguistic sophistication than is the norm.},
  file = {/home/cristi/Zotero/storage/D7GFQ2H8/Bender - 2011 - On Achieving and Evaluating Language-Independence .pdf}
}

@article{benderAchievingEvaluatingLanguageIndependence2011b,
  title = {On {{Achieving}} and {{Evaluating Language-Independence}} in {{NLP}}},
  author = {Bender, Emily M.},
  date = {2011-10},
  journaltitle = {Linguistic Issues in Language Technology},
  volume = {6},
  doi = {10.33011/lilt.v6i.1239},
  url = {https://journals.colorado.edu/index.php/lilt/article/view/1239},
  abstract = {\&amp;lt;p\&amp;gt;Language independence is commonly presented as one of the advantages of modern, machine-learning approaches to NLP, and it is an important type of scalability.\&amp;lt;/p\&amp;gt; \&amp;lt;p\&amp;gt;In this position paper, I critically review the widespread approaches to achieving and evaluating language independence in the field of computational linguistics and argue that, on the one hand, we are not truly evaluating language independence with any systematicity and on the other hand, that truly language-independent technology requires more linguistic sophistication than is the norm.\&amp;lt;/p\&amp;gt;}
}

@article{BESACIER201485,
  title = {Automatic Speech Recognition for Under-Resourced Languages: {{A}} Survey},
  author = {Besacier, Laurent and Barnard, Etienne and Karpov, Alexey and Schultz, Tanja},
  date = {2014},
  journaltitle = {Speech Communication},
  volume = {56},
  pages = {85--100},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2013.07.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639313000988},
  abstract = {Speech processing for under-resourced languages is an active field of research, which has experienced significant progress during the past decade. We propose, in this paper, a survey that focuses on automatic speech recognition (ASR) for these languages. The definition of under-resourced languages and the challenges associated to them are first defined. The main part of the paper is a literature review of the recent (last 8years) contributions made in ASR for under-resourced languages. Examples of past projects and future trends when dealing with under-resourced languages are also presented. We believe that this paper will be a good starting point for anyone interested to initiate research in (or operational development of) ASR for one or several under-resourced languages. It should be clear, however, that many of the issues and approaches presented here, apply to speech technology in general (text-to-speech synthesis for instance).},
  keywords = {Automatic pronunciation generation,Automatic speech recognition (ASR),Crosslingual acoustic modeling and adaptation,Language portability,Lexical modeling,Speech and language resources acquisition,Statistical language modeling,Under-resourced languages}
}

@article{besacierAutomaticSpeechRecognition2014,
  title = {Automatic Speech Recognition for Under-Resourced Languages: {{A}} Survey},
  shorttitle = {Automatic Speech Recognition for Under-Resourced Languages},
  author = {Besacier, Laurent and Barnard, Etienne and Karpov, Alexey and Schultz, Tanja},
  date = {2014-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {56},
  pages = {85--100},
  issn = {01676393},
  doi = {10.1016/j.specom.2013.07.008},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639313000988},
  urldate = {2023-03-17},
  langid = {english},
  file = {/home/cristi/Zotero/storage/EX3RGVR5/Besacier et al. - 2014 - Automatic speech recognition for under-resourced l.pdf}
}

@article{besacierAutomaticSpeechRecognition2014a,
  title = {Automatic {{Speech Recognition}} for {{Under-Resourced Languages}}: {{A Survey}}},
  author = {Besacier, Laurent and Barnard, Etienne and Karpov, Alexey and Schultz, Tanja},
  date = {2014},
  journaltitle = {Speech Communication},
  volume = {56},
  pages = {85--100},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2013.07.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639313000988},
  abstract = {Speech processing for under-resourced languages is an active field of research, which has experienced significant progress during the past decade. We propose, in this paper, a survey that focuses on automatic speech recognition (ASR) for these languages. The definition of under-resourced languages and the challenges associated to them are first defined. The main part of the paper is a literature review of the recent (last 8years) contributions made in ASR for under-resourced languages. Examples of past projects and future trends when dealing with under-resourced languages are also presented. We believe that this paper will be a good starting point for anyone interested to initiate research in (or operational development of) ASR for one or several under-resourced languages. It should be clear, however, that many of the issues and approaches presented here, apply to speech technology in general (text-to-speech synthesis for instance).},
  keywords = {Automatic pronunciation generation,Automatic speech recognition (ASR),Crosslingual acoustic modeling and adaptation,Language portability,Lexical modeling,Speech and language resources acquisition,Statistical language modeling,Under-resourced languages}
}

@article{besacierAutomaticSpeechRecognition2014b,
  title = {Automatic {{Speech Recognition}} for {{Under-Resourced Languages}}: {{A Survey}}},
  shorttitle = {Automatic {{Speech Recognition}} for {{Under-Resourced Languages}}},
  author = {Besacier, Laurent and Barnard, Etienne and Karpov, Alexey and Schultz, Tanja},
  date = {2014-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {56},
  pages = {85--100},
  issn = {01676393},
  doi = {10.1016/j.specom.2013.07.008},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639313000988},
  urldate = {2023-03-17},
  file = {/home/cristi/Zotero/storage/FNRFJV7U/Besacier et al. - 2014 - Automatic speech recognition for under-resourced l.pdf}
}

@book{bishopDeepLearningFoundations2024,
  title = {Deep {{Learning}}: {{Foundations}} and {{Concepts}}},
  shorttitle = {Deep {{Learning}}},
  author = {Bishop, Christopher M. and Bishop, Hugh},
  date = {2024},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-45468-4},
  url = {https://link.springer.com/10.1007/978-3-031-45468-4},
  urldate = {2025-01-19},
  isbn = {978-3-031-45467-7 978-3-031-45468-4},
  langid = {english},
  file = {/home/cristi/Zotero/storage/FXRL37G8/Bishop and Bishop - 2024 - Deep Learning Foundations and Concepts.pdf}
}

@book{bishopDeepLearningFoundations2024a,
  title = {Deep {{Learning}}: {{Foundations}} and {{Concepts}}},
  shorttitle = {Deep {{Learning}}},
  author = {Bishop, Christopher M. and Bishop, Hugh},
  date = {2024},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-45468-4},
  url = {https://link.springer.com/10.1007/978-3-031-45468-4},
  urldate = {2025-01-19},
  isbn = {978-3-031-45467-7 978-3-031-45468-4},
  file = {/home/cristi/Zotero/storage/BMVHQ89E/Bishop and Bishop - 2024 - Deep Learning Foundations and Concepts.pdf}
}

@article{bishopNeuralMechanismsAgeing2010,
  title = {Neural Mechanisms of Ageing and Cognitive Decline},
  author = {Bishop, Nicholas A. and Lu, Tao and Yankner, Bruce A.},
  date = {2010-03-25},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {464},
  number = {7288},
  pages = {529--535},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature08983},
  url = {https://www.nature.com/articles/nature08983},
  urldate = {2024-05-17},
  langid = {english},
  file = {/home/cristi/Zotero/storage/BKXUNLR2/bishop2010.pdf}
}

@article{bishopNeuralMechanismsAgeing2010a,
  title = {Neural {{Mechanisms}} of {{Ageing}} and {{Cognitive Decline}}},
  author = {Bishop, Nicholas A. and Lu, Tao and Yankner, Bruce A.},
  date = {2010-03-25},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {464},
  number = {7288},
  pages = {529--535},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature08983},
  url = {https://www.nature.com/articles/nature08983},
  urldate = {2024-05-17},
  file = {/home/cristi/Zotero/storage/4MMA3R9D/bishop2010.pdf}
}

@article{bizleyFunctionalOrganizationFerret2005,
  title = {Functional {{Organization}} of {{Ferret Auditory Cortex}}},
  author = {Bizley, Jennifer K. and Nodal, Fernando R. and Nelken, Israel and King, Andrew J.},
  date = {2005-10-01},
  journaltitle = {Cerebral Cortex},
  volume = {15},
  number = {10},
  pages = {1637--1653},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhi042},
  url = {http://academic.oup.com/cercor/article/15/10/1637/397009/Functional-Organization-of-Ferret-Auditory-Cortex},
  urldate = {2025-02-26},
  langid = {english},
  file = {/home/cristi/Zotero/storage/GR2AFFGZ/Bizley et al. - 2005 - Functional Organization of Ferret Auditory Cortex.pdf}
}

@article{bizleyFunctionalOrganizationFerret2005a,
  title = {Functional {{Organization}} of {{Ferret Auditory Cortex}}},
  author = {Bizley, Jennifer K. and Nodal, Fernando R. and Nelken, Israel and King, Andrew J.},
  date = {2005-10-01},
  journaltitle = {Cerebral Cortex},
  volume = {15},
  number = {10},
  pages = {1637--1653},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhi042},
  url = {http://academic.oup.com/cercor/article/15/10/1637/397009/Functional-Organization-of-Ferret-Auditory-Cortex},
  urldate = {2025-02-26},
  file = {/home/cristi/Zotero/storage/EEPC79JJ/Bizley et al. - 2005 - Functional Organization of Ferret Auditory Cortex.pdf}
}

@book{boddingtonCodeEthicsArtificial2017,
  title = {Towards a {{Code}} of {{Ethics}} for {{Artificial Intelligence}}},
  author = {Boddington, Paula},
  date = {2017},
  series = {Artificial {{Intelligence}}: {{Foundations}}, {{Theory}}, and {{Algorithms}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-60648-4},
  url = {http://link.springer.com/10.1007/978-3-319-60648-4},
  urldate = {2025-03-26},
  isbn = {978-3-319-60647-7 978-3-319-60648-4},
  file = {/home/cristi/Zotero/storage/DKRMQHBC/Boddington - 2017 - Towards a Code of Ethics for Artificial Intelligence.pdf}
}

@article{bonidiaNovelDecomposingModel2020,
  title = {A {{Novel Decomposing Model With Evolutionary Algorithms}} for {{Feature Selection}} in {{Long Non-Coding RNAs}}},
  author = {Bonidia, Robson P. and Machida, Jaqueline Sayuri and Negri, Tatianne C. and Alves, Wonder A. L. and Kashiwabara, Andre Y. and Domingues, Douglas S. and De Carvalho, Andre and Paschoal, Alexandre R. and Sanches, Danilo S.},
  date = {2020},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {8},
  pages = {181683--181697},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3028039},
  url = {https://ieeexplore.ieee.org/document/9210051/},
  urldate = {2025-01-31},
  file = {/home/cristi/Zotero/storage/2K5792CV/Bonidia et al. - 2020 - A Novel Decomposing Model With Evolutionary Algorithms for Feature Selection in Long Non-Coding RNAs.pdf}
}

@article{bonidiaNovelDecomposingModel2020a,
  title = {A {{Novel Decomposing Model With Evolutionary Algorithms}} for {{Feature Selection}} in {{Long Non-Coding RNAs}}},
  author = {Bonidia, Robson P. and Machida, Jaqueline Sayuri and Negri, Tatianne C. and Alves, Wonder A. L. and Kashiwabara, Andre Y. and Domingues, Douglas S. and De Carvalho, Andre and Paschoal, Alexandre R. and Sanches, Danilo S.},
  date = {2020},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {8},
  pages = {181683--181697},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3028039},
  url = {https://ieeexplore.ieee.org/document/9210051/},
  urldate = {2025-01-31},
  file = {/home/cristi/Zotero/storage/GH8BZ9Y2/Bonidia et al. - 2020 - A Novel Decomposing Model With Evolutionary Algorithms for Feature Selection in Long Non-Coding RNAs.pdf}
}

@incollection{borensteinRobotsEthicsIntimacy2019,
  title = {Robots, {{Ethics}}, and {{Intimacy}}: {{The Need}} for {{Scientific Research}}},
  shorttitle = {Robots, {{Ethics}}, and {{Intimacy}}},
  booktitle = {On the {{Cognitive}}, {{Ethical}}, and {{Scientific Dimensions}} of {{Artificial Intelligence}}},
  author = {Borenstein, Jason and Arkin, Ronald},
  editor = {Berkich, Don and family=Alfonso, given=Matteo Vincenzo, prefix=d', useprefix=true},
  date = {2019},
  volume = {134},
  pages = {299--309},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-01800-9_16},
  url = {http://link.springer.com/10.1007/978-3-030-01800-9_16},
  urldate = {2024-01-07},
  isbn = {978-3-030-01799-6 978-3-030-01800-9},
  langid = {english}
}

@incollection{borensteinRobotsEthicsIntimacy2019a,
  title = {Robots, {{Ethics}}, and {{Intimacy}}: {{The Need}} for {{Scientific Research}}},
  shorttitle = {Robots, {{Ethics}}, and {{Intimacy}}},
  booktitle = {On the {{Cognitive}}, {{Ethical}}, and {{Scientific Dimensions}} of {{Artificial Intelligence}}},
  author = {Borenstein, Jason and Arkin, Ronald},
  editor = {Berkich, Don and family=Alfonso, given=Matteo Vincenzo, prefix=d', useprefix=true},
  date = {2019},
  volume = {134},
  pages = {299--309},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-01800-9_16},
  url = {http://link.springer.com/10.1007/978-3-030-01800-9_16},
  urldate = {2024-01-07},
  isbn = {978-3-030-01799-6 978-3-030-01800-9},
  langid = {english},
  file = {/home/cristi/Zotero/storage/7MN9WJ3C/RobotsEthicsIntimacy-IACAP.pdf}
}

@incollection{borensteinRobotsEthicsIntimacy2019b,
  title = {Robots, {{Ethics}}, and {{Intimacy}}: {{The Need}} for {{Scientific Research}}},
  shorttitle = {Robots, {{Ethics}}, and {{Intimacy}}},
  booktitle = {On the {{Cognitive}}, {{Ethical}}, and {{Scientific Dimensions}} of {{Artificial Intelligence}}},
  author = {Borenstein, Jason and Arkin, Ronald},
  editor = {Berkich, Don and {family=Alfonso}, given=Matteo Vincenzo, prefix=d', useprefix=true},
  date = {2019},
  volume = {134},
  pages = {299--309},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-01800-9_16},
  url = {http://link.springer.com/10.1007/978-3-030-01800-9_16},
  urldate = {2024-01-07},
  isbn = {978-3-030-01799-6 978-3-030-01800-9},
  file = {/home/cristi/Zotero/storage/2ZCQCKGJ/RobotsEthicsIntimacy-IACAP.pdf}
}

@incollection{borensteinRobotsEthicsIntimacy2019c,
  title = {Robots, {{Ethics}}, and {{Intimacy}}: {{The Need}} for {{Scientific Research}}},
  shorttitle = {Robots, {{Ethics}}, and {{Intimacy}}},
  booktitle = {On the {{Cognitive}}, {{Ethical}}, and {{Scientific Dimensions}} of {{Artificial Intelligence}}},
  author = {Borenstein, Jason and Arkin, Ronald},
  editor = {Berkich, Don and {family=Alfonso}, given=Matteo Vincenzo, prefix=d', useprefix=true},
  date = {2019},
  volume = {134},
  pages = {299--309},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-01800-9_16},
  url = {http://link.springer.com/10.1007/978-3-030-01800-9_16},
  urldate = {2024-01-07},
  isbn = {978-3-030-01799-6 978-3-030-01800-9}
}

@incollection{bosmanVirtualAgentsProfessional2019,
  title = {Virtual {{Agents}} for {{Professional Social Skills Training}}: {{An Overview}} of the {{State-of-the-Art}}},
  shorttitle = {Virtual {{Agents}} for {{Professional Social Skills Training}}},
  booktitle = {Intelligent {{Technologies}} for {{Interactive Entertainment}}},
  author = {Bosman, Kim and Bosse, Tibor and Formolo, Daniel},
  editor = {Cortez, Paulo and Magalhães, Luís and Branco, Pedro and Portela, Carlos Filipe and Adão, Telmo},
  date = {2019},
  volume = {273},
  pages = {75--84},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-16447-8_8},
  url = {http://link.springer.com/10.1007/978-3-030-16447-8_8},
  urldate = {2022-10-06},
  isbn = {978-3-030-16446-1 978-3-030-16447-8},
  langid = {english}
}

@incollection{bosmanVirtualAgentsProfessional2019a,
  title = {Virtual {{Agents}} for {{Professional Social Skills Training}}: {{An Overview}} of the {{State-of-the-Art}}},
  shorttitle = {Virtual {{Agents}} for {{Professional Social Skills Training}}},
  booktitle = {Intelligent {{Technologies}} for {{Interactive Entertainment}}},
  author = {Bosman, Kim and Bosse, Tibor and Formolo, Daniel},
  editor = {Cortez, Paulo and Magalhães, Luís and Branco, Pedro and Portela, Carlos Filipe and Adão, Telmo},
  date = {2019},
  volume = {273},
  pages = {75--84},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-16447-8_8},
  url = {http://link.springer.com/10.1007/978-3-030-16447-8_8},
  urldate = {2022-10-06},
  isbn = {978-3-030-16446-1 978-3-030-16447-8}
}

@article{brassImagingVolitionWhat2013,
  title = {Imaging Volition: What the Brain Can Tell Us about the Will},
  shorttitle = {Imaging Volition},
  author = {Brass, Marcel and Lynn, Margaret T. and Demanet, Jelle and Rigoni, Davide},
  date = {2013-09},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {229},
  number = {3},
  pages = {301--312},
  issn = {0014-4819, 1432-1106},
  doi = {10.1007/s00221-013-3472-x},
  url = {http://link.springer.com/10.1007/s00221-013-3472-x},
  urldate = {2024-06-26},
  langid = {english},
  file = {/home/cristi/Zotero/storage/PSZFT9CE/Brass et al. - 2013 - Imaging volition what the brain can tell us about.pdf}
}

@article{brassImagingVolitionWhat2013a,
  title = {Imaging {{Volition}}: {{What}} the {{Brain Can Tell Us}} about the {{Will}}},
  shorttitle = {Imaging {{Volition}}},
  author = {Brass, Marcel and Lynn, Margaret T. and Demanet, Jelle and Rigoni, Davide},
  date = {2013-09},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {229},
  number = {3},
  pages = {301--312},
  issn = {0014-4819, 1432-1106},
  doi = {10.1007/s00221-013-3472-x},
  url = {http://link.springer.com/10.1007/s00221-013-3472-x},
  urldate = {2024-06-26},
  file = {/home/cristi/Zotero/storage/LY5CNVED/Brass et al. - 2013 - Imaging volition what the brain can tell us about.pdf}
}

@article{brassWhyNeuroscienceDoes2019,
  title = {Why Neuroscience Does Not Disprove Free Will},
  author = {Brass, Marcel and Furstenberg, Ariel and Mele, Alfred R.},
  date = {2019-07},
  journaltitle = {Neuroscience \& Biobehavioral Reviews},
  shortjournal = {Neuroscience \& Biobehavioral Reviews},
  volume = {102},
  pages = {251--263},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2019.04.024},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0149763419300739},
  urldate = {2024-06-26},
  langid = {english},
  file = {/home/cristi/Zotero/storage/PTNY6JP4/Brass et al. - 2019 - Why neuroscience does not disprove free will.pdf}
}

@article{brassWhyNeuroscienceDoes2019a,
  title = {Why {{Neuroscience Does Not Disprove Free Will}}},
  author = {Brass, Marcel and Furstenberg, Ariel and Mele, Alfred R.},
  date = {2019-07},
  journaltitle = {Neuroscience \& Biobehavioral Reviews},
  shortjournal = {Neuroscience \& Biobehavioral Reviews},
  volume = {102},
  pages = {251--263},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2019.04.024},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0149763419300739},
  urldate = {2024-06-26},
  file = {/home/cristi/Zotero/storage/C58844Q9/Brass et al. - 2019 - Why neuroscience does not disprove free will.pdf}
}

@article{brodbeckNeuralSourceDynamics2018,
  title = {Neural Source Dynamics of Brain Responses to Continuous Stimuli: {{Speech}} Processing from Acoustics to Comprehension},
  shorttitle = {Neural Source Dynamics of Brain Responses to Continuous Stimuli},
  author = {Brodbeck, Christian and Presacco, Alessandro and Simon, Jonathan Z.},
  date = {2018-05},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {172},
  pages = {162--174},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2018.01.042},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811918300429},
  urldate = {2024-05-17},
  langid = {english},
  file = {/home/cristi/Zotero/storage/A72HDQ75/Brodbeck et al. - 2018 - Neural source dynamics of brain responses to conti.pdf}
}

@article{brodbeckNeuralSourceDynamics2018a,
  title = {Neural {{Source Dynamics}} of {{Brain Responses}} to {{Continuous Stimuli}}: {{Speech Processing}} from {{Acoustics}} to {{Comprehension}}},
  shorttitle = {Neural {{Source Dynamics}} of {{Brain Responses}} to {{Continuous Stimuli}}},
  author = {Brodbeck, Christian and Presacco, Alessandro and Simon, Jonathan Z.},
  date = {2018-05},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {172},
  pages = {162--174},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2018.01.042},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811918300429},
  urldate = {2024-05-17},
  file = {/home/cristi/Zotero/storage/NVBSKLHC/Brodbeck et al. - 2018 - Neural source dynamics of brain responses to conti.pdf}
}

@inproceedings{buolamwiniGenderShadesIntersectional2018,
  title = {Gender {{Shades}}: {{Intersectional Accuracy Disparities}} in {{Commercial Gender Classification}}},
  shorttitle = {Gender {{Shades}}},
  booktitle = {Proceedings of the 1st {{Conference}} on {{Fairness}}, {{Accountability}} and {{Transparency}}},
  author = {Buolamwini, Joy and Gebru, Timnit},
  date = {2018-01-21},
  pages = {77--91},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
  urldate = {2025-03-26},
  abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
  eventtitle = {Conference on {{Fairness}}, {{Accountability}} and {{Transparency}}},
  langid = {english},
  file = {/home/cristi/Zotero/storage/3MU7LRMF/Buolamwini and Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities in Commercial Gender Classification.pdf;/home/cristi/Zotero/storage/BWDNY5FA/Buolamwini and Gebru - 2018 - Gender Shades Intersectional Accuracy Disparities in Commercial Gender Classification.pdf}
}

@book{bynonHistoricalLinguistics1977,
  title = {Historical Linguistics},
  author = {Bynon, Theodora},
  date = {1977},
  series = {Cambridge Textbooks in Linguistics},
  publisher = {Cambridge University Press},
  location = {Cambridge ; New York},
  isbn = {978-0-521-21582-4},
  pagetotal = {301},
  keywords = {Historical linguistics,Languages in contact}
}

@book{bynonHistoricalLinguistics1977a,
  title = {Historical {{Linguistics}}},
  author = {Bynon, Theodora},
  date = {1977},
  series = {Cambridge {{Textbooks}} in {{Linguistics}}},
  publisher = {Cambridge University Press},
  location = {Cambridge ; New York},
  isbn = {978-0-521-21582-4},
  pagetotal = {301},
  keywords = {Historical linguistics,Languages in contact}
}

@article{carandiniNormalizationCanonicalNeural2012,
  title = {Normalization as a Canonical Neural Computation},
  author = {Carandini, Matteo and Heeger, David J.},
  date = {2012-01},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {13},
  number = {1},
  pages = {51--62},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3136},
  url = {https://www.nature.com/articles/nrn3136},
  urldate = {2025-01-27},
  langid = {english},
  file = {/home/cristi/Zotero/storage/FZUUFPZU/Carandini and Heeger - 2012 - Normalization as a canonical neural computation.pdf;/home/cristi/Zotero/storage/S3BVCZRR/nrn3136.pdf}
}

@article{carandiniNormalizationCanonicalNeural2012a,
  title = {Normalization as a {{Canonical Neural Computation}}},
  author = {Carandini, Matteo and Heeger, David J.},
  date = {2012-01},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {13},
  number = {1},
  pages = {51--62},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3136},
  url = {https://www.nature.com/articles/nrn3136},
  urldate = {2025-01-27},
  file = {/home/cristi/Zotero/storage/CX4RWSXQ/nrn3136.pdf;/home/cristi/Zotero/storage/NAZTYIYU/Carandini and Heeger - 2012 - Normalization as a canonical neural computation.pdf}
}

@article{cauchoixNeuralDynamicsFace2014,
  title = {The {{Neural Dynamics}} of {{Face Detection}} in the {{Wild Revealed}} by {{MVPA}}},
  author = {Cauchoix, Maxime and Barragan-Jason, Gladys and Serre, Thomas and Barbeau, Emmanuel J.},
  date = {2014-01-15},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {34},
  number = {3},
  pages = {846--854},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3030-13.2014},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.3030-13.2014},
  urldate = {2024-05-17},
  abstract = {Previous magnetoencephalography/electroencephalography (M/EEG) studies have suggested that face processing is extremely rapid, indeed faster than any other object category. Most studies, however, have been performed using centered, cropped stimuli presented on a blank background resulting in artificially low interstimulus variability. In contrast, the aim of the present study was to assess the underlying temporal dynamics of face detection presented in complex natural scenes.             We recorded EEG activity while participants performed a rapid go/no-go categorization task in which they had to detect the presence of a human face. Subjects performed at ceiling (94.8\% accuracy), and traditional event-related potential analyses revealed only modest modulations of the two main components classically associated with face processing (P100 and N170). A multivariate pattern analysis conducted across all EEG channels revealed that face category could, however, be readout very early, under 100 ms poststimulus onset. Decoding was linked to reaction time as early as 125 ms. Decoding accuracy did not increase monotonically; we report an increase during an initial 95–140 ms period followed by a plateau ∼140–185 ms–perhaps reflecting a transitory stabilization of the face information available–and a strong increase afterward. Further analyses conducted on individual images confirmed these phases, further suggesting that decoding accuracy may be initially driven by low-level stimulus properties. Such latencies appear to be surprisingly short given the complexity of the natural scenes and the large intraclass variability of the face stimuli used, suggesting that the visual system is highly optimized for the processing of natural scenes.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/VFZPX64L/Cauchoix et al. - 2014 - The Neural Dynamics of Face Detection in the Wild .pdf}
}

@article{cauchoixNeuralDynamicsFace2014a,
  title = {The {{Neural Dynamics}} of {{Face Detection}} in the {{Wild Revealed}} by {{MVPA}}},
  author = {Cauchoix, Maxime and Barragan-Jason, Gladys and Serre, Thomas and Barbeau, Emmanuel J.},
  date = {2014-01-15},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {34},
  number = {3},
  pages = {846--854},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3030-13.2014},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.3030-13.2014},
  urldate = {2024-05-17},
  abstract = {Previous magnetoencephalography/electroencephalography (M/EEG) studies have suggested that face processing is extremely rapid, indeed faster than any other object category. Most studies, however, have been performed using centered, cropped stimuli presented on a blank background resulting in artificially low interstimulus variability. In contrast, the aim of the present study was to assess the underlying temporal dynamics of face detection presented in complex natural scenes. We recorded EEG activity while participants performed a rapid go/no-go categorization task in which they had to detect the presence of a human face. Subjects performed at ceiling (94.8\% accuracy), and traditional event-related potential analyses revealed only modest modulations of the two main components classically associated with face processing (P100 and N170). A multivariate pattern analysis conducted across all EEG channels revealed that face category could, however, be readout very early, under 100 ms poststimulus onset. Decoding was linked to reaction time as early as 125 ms. Decoding accuracy did not increase monotonically; we report an increase during an initial 95–140 ms period followed by a plateau ∼140–185 ms–perhaps reflecting a transitory stabilization of the face information available–and a strong increase afterward. Further analyses conducted on individual images confirmed these phases, further suggesting that decoding accuracy may be initially driven by low-level stimulus properties. Such latencies appear to be surprisingly short given the complexity of the natural scenes and the large intraclass variability of the face stimuli used, suggesting that the visual system is highly optimized for the processing of natural scenes.},
  file = {/home/cristi/Zotero/storage/SPA4QR55/Cauchoix et al. - 2014 - The Neural Dynamics of Face Detection in the Wild .pdf}
}

@article{celeghinConvolutionalNeuralNetworks2023,
  title = {Convolutional Neural Networks for Vision Neuroscience: Significance, Developments, and Outstanding Issues},
  shorttitle = {Convolutional Neural Networks for Vision Neuroscience},
  author = {Celeghin, Alessia and Borriero, Alessio and Orsenigo, Davide and Diano, Matteo and Méndez Guerrero, Carlos Andrés and Perotti, Alan and Petri, Giovanni and Tamietto, Marco},
  date = {2023-07-06},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  volume = {17},
  pages = {1153572},
  issn = {1662-5188},
  doi = {10.3389/fncom.2023.1153572},
  url = {https://www.frontiersin.org/articles/10.3389/fncom.2023.1153572/full},
  urldate = {2024-04-23},
  abstract = {Convolutional Neural Networks (CNN) are a class of machine learning models predominately used in computer vision tasks and can achieve human-like performance through learning from experience. Their striking similarities to the structural and functional principles of the primate visual system allow for comparisons between these artificial networks and their biological counterparts, enabling exploration of how visual functions and neural representations may emerge in the real brain from a limited set of computational principles. After considering the basic features of CNNs, we discuss the opportunities and challenges of endorsing CNNs as               in silico               models of the primate visual system. Specifically, we highlight several emerging notions about the anatomical and physiological properties of the visual system that still need to be systematically integrated into current CNN models. These tenets include the implementation of parallel processing pathways from the early stages of retinal input and the reconsideration of several assumptions concerning the serial progression of information flow. We suggest design choices and architectural constraints that could facilitate a closer alignment with biology provide causal evidence of the predictive link between the artificial and biological visual systems. Adopting this principled perspective could potentially lead to new research questions and applications of CNNs beyond modeling object recognition.},
  file = {/home/cristi/Zotero/storage/3I3WKH7I/Celeghin et al. - 2023 - Convolutional neural networks for vision neuroscie.pdf}
}

@article{celeghinConvolutionalNeuralNetworks2023a,
  title = {Convolutional {{Neural Networks}} for {{Vision Neuroscience}}: {{Significance}}, {{Developments}}, and {{Outstanding Issues}}},
  shorttitle = {Convolutional {{Neural Networks}} for {{Vision Neuroscience}}},
  author = {Celeghin, Alessia and Borriero, Alessio and Orsenigo, Davide and Diano, Matteo and Méndez Guerrero, Carlos Andrés and Perotti, Alan and Petri, Giovanni and Tamietto, Marco},
  date = {2023-07-06},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  volume = {17},
  pages = {1153572},
  issn = {1662-5188},
  doi = {10.3389/fncom.2023.1153572},
  url = {https://www.frontiersin.org/articles/10.3389/fncom.2023.1153572/full},
  urldate = {2024-04-23},
  abstract = {Convolutional Neural Networks (CNN) are a class of machine learning models predominately used in computer vision tasks and can achieve human-like performance through learning from experience. Their striking similarities to the structural and functional principles of the primate visual system allow for comparisons between these artificial networks and their biological counterparts, enabling exploration of how visual functions and neural representations may emerge in the real brain from a limited set of computational principles. After considering the basic features of CNNs, we discuss the opportunities and challenges of endorsing CNNs as in silico models of the primate visual system. Specifically, we highlight several emerging notions about the anatomical and physiological properties of the visual system that still need to be systematically integrated into current CNN models. These tenets include the implementation of parallel processing pathways from the early stages of retinal input and the reconsideration of several assumptions concerning the serial progression of information flow. We suggest design choices and architectural constraints that could facilitate a closer alignment with biology provide causal evidence of the predictive link between the artificial and biological visual systems. Adopting this principled perspective could potentially lead to new research questions and applications of CNNs beyond modeling object recognition.},
  file = {/home/cristi/Zotero/storage/K6J99ZAP/Celeghin et al. - 2023 - Convolutional neural networks for vision neuroscie.pdf}
}

@online{CellProteinsDuckDuckGo,
  title = {Cell Proteins at {{DuckDuckGo}}},
  url = {https://duckduckgo.com/?q=cell+proteins&t=newext&atb=v392-1&ia=web},
  urldate = {2025-02-11},
  file = {/home/cristi/Zotero/storage/N3F3VWTB/duckduckgo.com.html}
}

@online{CellProteinsDuckDuckGoa,
  title = {Cell {{Proteins}} at {{DuckDuckGo}}},
  url = {https://duckduckgo.com/?q=cell+proteins&t=newext&atb=v392-1&ia=web},
  urldate = {2025-02-11},
  file = {/home/cristi/Zotero/storage/725GQQL9/duckduckgo.com.html}
}

@incollection{chalmersHardProblemConsciousness2017,
  title = {The {{Hard Problem}} of {{Consciousness}}},
  booktitle = {The {{Blackwell Companion}} to {{Consciousness}}},
  author = {Chalmers, David},
  editor = {Schneider, Susan and Velmans, Max},
  date = {2017-04-12},
  edition = {1},
  pages = {32--42},
  publisher = {Wiley},
  doi = {10.1002/9781119132363.ch3},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/9781119132363.ch3},
  urldate = {2024-06-25},
  isbn = {978-0-470-67406-2 978-1-119-13236-3},
  langid = {english},
  file = {/home/cristi/Zotero/storage/GQZJ8E9Y/Chalmers - 2017 - The Hard Problem of Consciousness.pdf}
}

@incollection{chalmersHardProblemConsciousness2017a,
  title = {The {{Hard Problem}} of {{Consciousness}}},
  booktitle = {The {{Blackwell Companion}} to {{Consciousness}}},
  author = {Chalmers, David},
  editor = {Schneider, Susan and Velmans, Max},
  date = {2017-04-12},
  edition = {1},
  pages = {32--42},
  publisher = {Wiley},
  doi = {10.1002/9781119132363.ch3},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/9781119132363.ch3},
  urldate = {2024-06-25},
  isbn = {978-0-470-67406-2 978-1-119-13236-3},
  file = {/home/cristi/Zotero/storage/T4LH8KHJ/Chalmers - 2017 - The Hard Problem of Consciousness.pdf}
}

@article{chetverikovMotionDirectionRepresented2023,
  title = {Motion Direction Is Represented as a Bimodal Probability Distribution in the Human Visual Cortex},
  author = {Chetverikov, Andrey and Jehee, Janneke F. M.},
  date = {2023-11-22},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {14},
  number = {1},
  pages = {7634},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-43251-w},
  url = {https://www.nature.com/articles/s41467-023-43251-w},
  urldate = {2024-10-07},
  abstract = {Abstract             Humans infer motion direction from noisy sensory signals. We hypothesize that to make these inferences more precise, the visual system computes motion direction not only from velocity but also spatial orientation signals – a ‘streak’ created by moving objects. We implement this hypothesis in a Bayesian model, which quantifies knowledge with probability distributions, and test its predictions using psychophysics and fMRI. Using a probabilistic pattern-based analysis, we decode probability distributions of motion direction from trial-by-trial activity in the human visual cortex. Corroborating the predictions, the decoded distributions have a bimodal shape, with peaks that predict the direction and magnitude of behavioral errors. Interestingly, we observe similar bimodality in the distribution of the observers’ behavioral responses across trials. Together, these results suggest that observers use spatial orientation signals when estimating motion direction. More broadly, our findings indicate that the cortical representation of low-level visual features, such as motion direction, can reflect a combination of several qualitatively distinct signals.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/ZUSTUVFX/Chetverikov and Jehee - 2023 - Motion direction is represented as a bimodal proba.pdf}
}

@article{chetverikovMotionDirectionRepresented2023a,
  title = {Motion {{Direction Is Represented}} as a {{Bimodal Probability Distribution}} in the {{Human Visual Cortex}}},
  author = {Chetverikov, Andrey and Jehee, Janneke F. M.},
  date = {2023-11-22},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {14},
  number = {1},
  pages = {7634},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-43251-w},
  url = {https://www.nature.com/articles/s41467-023-43251-w},
  urldate = {2024-10-07},
  abstract = {Abstract Humans infer motion direction from noisy sensory signals. We hypothesize that to make these inferences more precise, the visual system computes motion direction not only from velocity but also spatial orientation signals – a ‘streak’ created by moving objects. We implement this hypothesis in a Bayesian model, which quantifies knowledge with probability distributions, and test its predictions using psychophysics and fMRI. Using a probabilistic pattern-based analysis, we decode probability distributions of motion direction from trial-by-trial activity in the human visual cortex. Corroborating the predictions, the decoded distributions have a bimodal shape, with peaks that predict the direction and magnitude of behavioral errors. Interestingly, we observe similar bimodality in the distribution of the observers’ behavioral responses across trials. Together, these results suggest that observers use spatial orientation signals when estimating motion direction. More broadly, our findings indicate that the cortical representation of low-level visual features, such as motion direction, can reflect a combination of several qualitatively distinct signals.},
  file = {/home/cristi/Zotero/storage/9SCAXU2R/Chetverikov and Jehee - 2023 - Motion direction is represented as a bimodal proba.pdf}
}

@inproceedings{chiuComparisonEndtoEndModels2019,
  title = {A {{Comparison}} of {{End-to-End Models}} for {{Long-Form Speech Recognition}}},
  booktitle = {2019 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Chiu, Chung-Cheng and Han, Wei and Zhang, Yu and Pang, Ruoming and Kishchenko, Sergey and Nguyen, Patrick and Narayanan, Arun and Liao, Hank and Zhang, Shuyuan and Kannan, Anjuli and Prabhavalkar, Rohit and Chen, Zhifeng and Sainath, Tara and Wu, Yonghui},
  date = {2019-12},
  pages = {889--896},
  publisher = {IEEE},
  location = {SG, Singapore},
  doi = {10.1109/ASRU46091.2019.9003854},
  url = {https://ieeexplore.ieee.org/document/9003854/},
  urldate = {2023-02-14},
  eventtitle = {2019 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  isbn = {978-1-7281-0306-8},
  file = {/home/cristi/Zotero/storage/RZQUW9K2/Chiu et al. - 2019 - A Comparison of End-to-End Models for Long-Form Sp.pdf}
}

@inproceedings{chiuComparisonEndtoEndModels2019a,
  title = {A {{Comparison}} of {{End-to-End Models}} for {{Long-Form Speech Recognition}}},
  booktitle = {2019 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Chiu, Chung-Cheng and Han, Wei and Zhang, Yu and Pang, Ruoming and Kishchenko, Sergey and Nguyen, Patrick and Narayanan, Arun and Liao, Hank and Zhang, Shuyuan and Kannan, Anjuli and Prabhavalkar, Rohit and Chen, Zhifeng and Sainath, Tara and Wu, Yonghui},
  date = {2019-12},
  pages = {889--896},
  publisher = {IEEE},
  location = {SG, Singapore},
  doi = {10.1109/ASRU46091.2019.9003854},
  url = {https://ieeexplore.ieee.org/document/9003854/},
  urldate = {2023-02-14},
  eventtitle = {2019 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  isbn = {978-1-7281-0306-8},
  file = {/home/cristi/Zotero/storage/AZ9PA4K4/Chiu et al. - 2019 - A Comparison of End-to-End Models for Long-Form Sp.pdf}
}

@article{cichyComparisonDeepNeural2016,
  title = {Comparison of Deep Neural Networks to Spatio-Temporal Cortical Dynamics of Human Visual Object Recognition Reveals Hierarchical Correspondence},
  author = {Cichy, Radoslaw Martin and Khosla, Aditya and Pantazis, Dimitrios and Torralba, Antonio and Oliva, Aude},
  date = {2016-06-10},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {6},
  number = {1},
  pages = {27755},
  issn = {2045-2322},
  doi = {10.1038/srep27755},
  url = {https://www.nature.com/articles/srep27755},
  urldate = {2024-04-25},
  abstract = {Abstract             The complex multi-stage architecture of cortical visual pathways provides the neural basis for efficient visual object recognition in humans. However, the stage-wise computations therein remain poorly understood. Here, we compared temporal (magnetoencephalography) and spatial (functional MRI) visual brain representations with representations in an artificial deep neural network (DNN) tuned to the statistics of real-world visual recognition. We showed that the DNN captured the stages of human visual processing in both time and space from early visual areas towards the dorsal and ventral streams. Further investigation of crucial DNN parameters revealed that while model architecture was important, training on real-world categorization was necessary to enforce spatio-temporal hierarchical relationships with the brain. Together our results provide an algorithmically informed view on the spatio-temporal dynamics of visual object recognition in the human visual brain.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/YU6PC83N/Cichy et al. - 2016 - Comparison of deep neural networks to spatio-tempo.pdf}
}

@article{cichyComparisonDeepNeural2016a,
  title = {Comparison of {{Deep Neural Networks}} to {{Spatio-Temporal Cortical Dynamics}} of {{Human Visual Object Recognition Reveals Hierarchical Correspondence}}},
  author = {Cichy, Radoslaw Martin and Khosla, Aditya and Pantazis, Dimitrios and Torralba, Antonio and Oliva, Aude},
  date = {2016-06-10},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {6},
  number = {1},
  pages = {27755},
  issn = {2045-2322},
  doi = {10.1038/srep27755},
  url = {https://www.nature.com/articles/srep27755},
  urldate = {2024-04-25},
  abstract = {Abstract The complex multi-stage architecture of cortical visual pathways provides the neural basis for efficient visual object recognition in humans. However, the stage-wise computations therein remain poorly understood. Here, we compared temporal (magnetoencephalography) and spatial (functional MRI) visual brain representations with representations in an artificial deep neural network (DNN) tuned to the statistics of real-world visual recognition. We showed that the DNN captured the stages of human visual processing in both time and space from early visual areas towards the dorsal and ventral streams. Further investigation of crucial DNN parameters revealed that while model architecture was important, training on real-world categorization was necessary to enforce spatio-temporal hierarchical relationships with the brain. Together our results provide an algorithmically informed view on the spatio-temporal dynamics of visual object recognition in the human visual brain.},
  file = {/home/cristi/Zotero/storage/423XUBV8/Cichy et al. - 2016 - Comparison of deep neural networks to spatio-tempo.pdf}
}

@book{claydenOrganicChemistry2012,
  title = {Organic Chemistry},
  author = {Clayden, Jonathan and Greeves, Nick and Warren, Stuart},
  date = {2012},
  edition = {2nd ed},
  publisher = {Oxford university press},
  location = {Oxford},
  isbn = {978-0-19-927029-3},
  langid = {english},
  file = {/home/cristi/Zotero/storage/2P5N2D3R/Clayden et al. - 2012 - Organic chemistry.pdf}
}

@book{claydenOrganicChemistry2012a,
  title = {Organic {{Chemistry}}},
  author = {Clayden, Jonathan and Greeves, Nick and Warren, Stuart},
  date = {2012},
  edition = {2nd ed},
  publisher = {Oxford university press},
  location = {Oxford},
  isbn = {978-0-19-927029-3},
  file = {/home/cristi/Zotero/storage/5SW7CAL6/Clayden et al. - 2012 - Organic chemistry.pdf}
}

@online{CognitiveComputingYear,
  title = {Cognitive {{Computing}} Year 1 | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-cognitive-computing/cognitive-computing-year-1},
  urldate = {2022-10-08},
  file = {/home/cristi/Zotero/storage/IBKNS6JP/cognitive-computing-year-1.html}
}

@online{CognitiveComputingYeara,
  title = {Cognitive {{Computing}} Year 2 | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-cognitive-computing/cognitive-computing-year-2},
  urldate = {2022-10-08},
  file = {/home/cristi/Zotero/storage/IX2BYEVQ/cognitive-computing-year-2.html}
}

@online{CognitiveComputingYearb,
  title = {Cognitive {{Computing}} Year 1 | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-cognitive-computing/cognitive-computing-year-1},
  urldate = {2022-10-22},
  file = {/home/cristi/Zotero/storage/8DJFG4EP/cognitive-computing-year-1.html}
}

@online{CognitiveComputingYearc,
  title = {Cognitive {{Computing Year}} 1 | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-cognitive-computing/cognitive-computing-year-1},
  urldate = {2022-10-22},
  file = {/home/cristi/Zotero/storage/9DKWL22D/cognitive-computing-year-1.html}
}

@online{CognitiveComputingYeard,
  title = {Cognitive {{Computing Year}} 2 | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-cognitive-computing/cognitive-computing-year-2},
  urldate = {2022-10-08},
  file = {/home/cristi/Zotero/storage/Q4Z28V3D/cognitive-computing-year-2.html}
}

@online{CognitiveComputingYeare,
  title = {Cognitive {{Computing Year}} 1 | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-cognitive-computing/cognitive-computing-year-1},
  urldate = {2022-10-08},
  file = {/home/cristi/Zotero/storage/5GFX6699/cognitive-computing-year-1.html}
}

@article{cohenTemporalDynamicsBrain1997,
  title = {Temporal Dynamics of Brain Activation during a Working Memory Task},
  author = {Cohen, Jonathan D. and Perlstein, William M. and Braver, Todd S. and Nystrom, Leigh E. and Noll, Douglas C. and Jonides, John and Smith, Edward E.},
  date = {1997-04},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {386},
  number = {6625},
  pages = {604--608},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/386604a0},
  url = {https://www.nature.com/articles/386604a0},
  urldate = {2024-05-17},
  langid = {english},
  file = {/home/cristi/Zotero/storage/ZJ6UJDXJ/Cohen et al. - 1997 - Temporal dynamics of brain activation during a wor.pdf}
}

@article{cohenTemporalDynamicsBrain1997a,
  title = {Temporal {{Dynamics}} of {{Brain Activation}} during a {{Working Memory Task}}},
  author = {Cohen, Jonathan D. and Perlstein, William M. and Braver, Todd S. and Nystrom, Leigh E. and Noll, Douglas C. and Jonides, John and Smith, Edward E.},
  date = {1997-04},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {386},
  number = {6625},
  pages = {604--608},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/386604a0},
  url = {https://www.nature.com/articles/386604a0},
  urldate = {2024-05-17},
  file = {/home/cristi/Zotero/storage/YC6PMHRA/Cohen et al. - 1997 - Temporal dynamics of brain activation during a wor.pdf}
}

@online{ContactUsLiveChat,
  title = {Contact Us via {{LiveChat}}!},
  url = {https://secure.livechatinc.com/},
  urldate = {2022-09-25},
  abstract = {Have any questions? Talk with us directly using LiveChat.},
  langid = {english}
}

@online{ContactUsLiveChata,
  title = {Contact Us via {{LiveChat}}!},
  url = {https://secure.livechatinc.com/},
  urldate = {2022-09-25},
  abstract = {Have any questions? Talk with us directly using LiveChat.},
  langid = {english}
}

@online{ContactUsLiveChatb,
  title = {Contact Us via {{LiveChat}}!},
  url = {https://secure.livechatinc.com/},
  urldate = {2022-10-08},
  abstract = {Have any questions? Talk with us directly using LiveChat.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/ETPU4TRG/open_chat.html}
}

@online{ContactUsLiveChatc,
  title = {Contact Us via {{LiveChat}}!},
  url = {https://secure.livechatinc.com/},
  urldate = {2022-10-08},
  abstract = {Have any questions? Talk with us directly using LiveChat.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/FRPZ5FZ8/open_chat.html}
}

@online{ContactUsLiveChatd,
  title = {Contact Us via {{LiveChat}}!},
  url = {https://secure.livechatinc.com/},
  urldate = {2022-10-22},
  abstract = {Have any questions? Talk with us directly using LiveChat.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/NWLAL3BQ/open_chat.html}
}

@online{ContactUsLiveChate,
  title = {Contact {{Us}} via {{LiveChat}}!},
  url = {https://secure.livechatinc.com/},
  urldate = {2022-10-22},
  abstract = {Have any questions? Talk with us directly using LiveChat.},
  file = {/home/cristi/Zotero/storage/RVVG4PZT/open_chat.html}
}

@online{ContactUsLiveChatf,
  title = {Contact {{Us}} via {{LiveChat}}!},
  url = {https://secure.livechatinc.com/},
  urldate = {2022-10-08},
  abstract = {Have any questions? Talk with us directly using LiveChat.},
  file = {/home/cristi/Zotero/storage/F894FXAA/open_chat.html}
}

@online{ContactUsLiveChatg,
  title = {Contact {{Us}} via {{LiveChat}}!},
  url = {https://secure.livechatinc.com/},
  urldate = {2022-09-25},
  abstract = {Have any questions? Talk with us directly using LiveChat.}
}

@online{ContactUsLiveChath,
  title = {Contact {{Us}} via {{LiveChat}}!},
  url = {https://secure.livechatinc.com/},
  urldate = {2022-09-25},
  abstract = {Have any questions? Talk with us directly using LiveChat.}
}

@online{ContactUsLiveChati,
  title = {Contact {{Us}} via {{LiveChat}}!},
  url = {https://secure.livechatinc.com/},
  urldate = {2022-10-08},
  abstract = {Have any questions? Talk with us directly using LiveChat.},
  file = {/home/cristi/Zotero/storage/IDCCW3WU/open_chat.html}
}

@article{cortes-martinNursingCarePlan2023,
  title = {Nursing {{Care Plan}} for {{Patients}} with {{Tay}}–{{Sachs}}—{{A Rare Paediatric Disease}}},
  author = {Cortés-Martín, Jonathan and Piqueras-Sola, Beatriz and Sánchez-García, Juan Carlos and Reinoso-Cobo, Andrés and Ramos-Petersen, Laura and Díaz-Rodríguez, Lourdes and Rodríguez-Blanque, Raquel},
  date = {2023-08-01},
  journaltitle = {Journal of Personalized Medicine},
  shortjournal = {JPM},
  volume = {13},
  number = {8},
  pages = {1222},
  issn = {2075-4426},
  doi = {10.3390/jpm13081222},
  url = {https://www.mdpi.com/2075-4426/13/8/1222},
  urldate = {2025-03-19},
  abstract = {Tay–Sachs disease is classified as a rare paediatric disease of metabolic origin. It is an autosomal recessive inherited disease. The gene responsible for the disease is known as HEXA, and it is located on chromosome 15(15q23). There is currently no effective treatment for Tay–Sachs disease; hence, it is an incurable disease in which patients do not live for more than five years, meaning that nursing care takes on greater importance to maintain quality of life. The main objective of this work is to develop a specific standard nursing care plan by applying an inductive research method supported by nursing methodology using the NANDA-NIC-NOC taxonomy and validated by the Delphi method. This care plan will improve the knowledge of health professionals on this topic and support future studies on the disease. Following its implementation, the care plan proposed in this study aims to increase the quality of life of patients diagnosed with this disease.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/X2IUFL6K/Cortés-Martín et al. - 2023 - Nursing Care Plan for Patients with Tay–Sachs—A Rare Paediatric Disease.pdf}
}

@incollection{coteFrenchLiaison2011,
  title = {French {{Liaison}}},
  booktitle = {The {{Blackwell Companion}} to {{Phonology}}},
  author = {Cǒté, Marie‐Hélène},
  editor = {Oostendorp, Marc and Ewen, Colin J. and Hume, Elizabeth and Rice, Keren},
  date = {2011-04-28},
  edition = {1},
  pages = {1--26},
  publisher = {Wiley},
  doi = {10.1002/9781444335262.wbctp0112},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/9781444335262.wbctp0112},
  urldate = {2024-02-26},
  abstract = {Liaison is a type of external sandhi, one of several occurring at word boundaries in French. It corresponds to the pronunciation of a consonant (liaison consonant = LC) between two words, the second being vowel‐initial, in certain liaison‐triggering contexts. Liaison may apply categorically or variably, depending on the context. The data in (1) illustrate this process. The words preceding and following the LC are referred to respectively as Word1 and Word2; their canonical pronunciation in non‐liaison contexts, including in isolation, is indicated in the relevant columns. In (1b), for example, the words               vous               and               allez               are pronounced in isolation [vu] and [ale]; when the two words are concatenated, an intervening [z] surfaces. LCs are underlined in all examples for ease of identification.               1},
  isbn = {978-1-4051-8423-6 978-1-4443-3526-2},
  langid = {english},
  file = {/home/cristi/Zotero/storage/LBHEJMTA/Cǒté - 2011 - French Liaison.pdf}
}

@incollection{coteFrenchLiaison2011a,
  title = {French {{Liaison}}},
  booktitle = {The {{Blackwell Companion}} to {{Phonology}}},
  author = {Cǒté, Marie‐Hélène},
  editor = {Oostendorp, Marc and Ewen, Colin J. and Hume, Elizabeth and Rice, Keren},
  date = {2011-04-28},
  edition = {1},
  pages = {1--26},
  publisher = {Wiley},
  doi = {10.1002/9781444335262.wbctp0112},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/9781444335262.wbctp0112},
  urldate = {2024-02-26},
  abstract = {Liaison is a type of external sandhi, one of several occurring at word boundaries in French. It corresponds to the pronunciation of a consonant (liaison consonant = LC) between two words, the second being vowel‐initial, in certain liaison‐triggering contexts. Liaison may apply categorically or variably, depending on the context. The data in (1) illustrate this process. The words preceding and following the LC are referred to respectively as Word1 and Word2; their canonical pronunciation in non‐liaison contexts, including in isolation, is indicated in the relevant columns. In (1b), for example, the words vous and allez are pronounced in isolation [vu] and [ale]; when the two words are concatenated, an intervening [z] surfaces. LCs are underlined in all examples for ease of identification. 1},
  isbn = {978-1-4051-8423-6 978-1-4443-3526-2},
  file = {/home/cristi/Zotero/storage/DUPJTW5M/Cǒté - 2011 - French Liaison.pdf}
}

@online{CourseOverviewB3,
  title = {Course Overview {{B3}} - {{Course}} Guides~2022 {{Faculty}} of {{Social Sciences}}},
  url = {https://www.ru.nl/courseguides/2022/socsci/bachelor/artificial-intelligence/third-year-b3/course-overview-b3/},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/IPDYXL3Q/course-overview-b3.html}
}

@online{CourseOverviewB3a,
  title = {Course {{Overview B3}} - {{Course Guides}} 2022 {{Faculty}} of {{Social Sciences}}},
  url = {https://www.ru.nl/courseguides/2022/socsci/bachelor/artificial-intelligence/third-year-b3/course-overview-b3/},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/RGL2ETI2/course-overview-b3.html}
}

@book{crystalCambridgeEncyclopediaEnglish2019,
  title = {The {{Cambridge}} Encyclopedia of the {{English}} Language},
  author = {Crystal, David},
  date = {2019},
  edition = {Third edition},
  publisher = {Cambridge University Press},
  location = {Cambridge New York, NY Port Melbourne New Delhi Singapore},
  isbn = {978-1-108-43773-8 978-1-108-42359-5},
  langid = {english},
  pagetotal = {573}
}

@book{crystalCambridgeEncyclopediaEnglish2019a,
  title = {The {{Cambridge Encyclopedia}} of the {{English Language}}},
  author = {Crystal, David},
  date = {2019},
  edition = {Third edition},
  publisher = {Cambridge University Press},
  location = {Cambridge New York, NY Port Melbourne New Delhi Singapore},
  isbn = {978-1-108-43773-8 978-1-108-42359-5},
  pagetotal = {573}
}

@online{dapelloSimulatingPrimaryVisual2020,
  title = {Simulating a {{Primary Visual Cortex}} at the {{Front}} of {{CNNs Improves Robustness}} to {{Image Perturbations}}},
  author = {Dapello, Joel and Marques, Tiago and Schrimpf, Martin and Geiger, Franziska and Cox, David D. and DiCarlo, James J.},
  date = {2020-06-17},
  doi = {10.1101/2020.06.16.154542},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.06.16.154542},
  urldate = {2024-06-16},
  abstract = {Abstract           Current state-of-the-art object recognition models are largely based on convolutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we first observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adversarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a fixed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientific model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor filter bank, simple and complex cell nonlinearities, and a V1 neuronal stochasticity generator. After training, VOneNets retain high ImageNet performance, but each is substantially more robust, outperforming the base CNNs and state-of-the-art methods by 18\% and 3\%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/cristi/Zotero/storage/NB6ZY4TS/Dapello et al. - 2020 - Simulating a Primary Visual Cortex at the Front of.pdf}
}

@online{dapelloSimulatingPrimaryVisual2020a,
  title = {Simulating a {{Primary Visual Cortex}} at the {{Front}} of {{CNNs Improves Robustness}} to {{Image Perturbations}}},
  author = {Dapello, Joel and Marques, Tiago and Schrimpf, Martin and Geiger, Franziska and Cox, David D. and DiCarlo, James J.},
  date = {2020-06-17},
  doi = {10.1101/2020.06.16.154542},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.06.16.154542},
  urldate = {2024-06-16},
  abstract = {Abstract           Current state-of-the-art object recognition models are largely based on convolutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we first observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adversarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a fixed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientific model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor filter bank, simple and complex cell nonlinearities, and a V1 neuronal stochasticity generator. After training, VOneNets retain high ImageNet performance, but each is substantially more robust, outperforming the base CNNs and state-of-the-art methods by 18\% and 3\%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/cristi/Zotero/storage/PHQGL7HZ/Dapello et al. - 2020 - Simulating a Primary Visual Cortex at the Front of.pdf}
}

@online{dapelloSimulatingPrimaryVisual2020b,
  title = {Simulating a {{Primary Visual Cortex}} at the {{Front}} of {{CNNs Improves Robustness}} to {{Image Perturbations}}},
  author = {Dapello, Joel and Marques, Tiago and Schrimpf, Martin and Geiger, Franziska and Cox, David D. and DiCarlo, James J.},
  date = {2020-06-17},
  doi = {10.1101/2020.06.16.154542},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.06.16.154542},
  urldate = {2024-06-16},
  abstract = {Abstract           Current state-of-the-art object recognition models are largely based on convolutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we first observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adversarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a fixed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientific model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor filter bank, simple and complex cell nonlinearities, and a V1 neuronal stochasticity generator. After training, VOneNets retain high ImageNet performance, but each is substantially more robust, outperforming the base CNNs and state-of-the-art methods by 18\% and 3\%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/cristi/Zotero/storage/4XJFEMS4/Dapello et al. - 2020 - Simulating a Primary Visual Cortex at the Front of.pdf}
}

@online{dapelloSimulatingPrimaryVisual2020c,
  title = {Simulating a {{Primary Visual Cortex}} at the {{Front}} of {{CNNs Improves Robustness}} to {{Image Perturbations}}},
  author = {Dapello, Joel and Marques, Tiago and Schrimpf, Martin and Geiger, Franziska and Cox, David D. and DiCarlo, James J.},
  date = {2020-06-17},
  doi = {10.1101/2020.06.16.154542},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.06.16.154542},
  urldate = {2024-06-17},
  abstract = {Current state-of-the-art object recognition models are largely based on convolutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we first observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adversarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a fixed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientific model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor filter bank, simple and complex cell nonlinearities, and a V1 neuronal stochasticity generator. After training, VOneNets retain high ImageNet performance, but each is substantially more robust, outperforming the base CNNs and state-of-the-art methods by 18\% and 3\%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/cristi/Zotero/storage/FN6SJ5LK/Dapello et al. - 2020 - Simulating a Primary Visual Cortex at the Front of.pdf}
}

@online{dapelloSimulatingPrimaryVisual2020d,
  title = {Simulating a {{Primary Visual Cortex}} at the {{Front}} of {{CNNs Improves Robustness}} to {{Image Perturbations}}},
  author = {Dapello, Joel and Marques, Tiago and Schrimpf, Martin and Geiger, Franziska and Cox, David D. and DiCarlo, James J.},
  date = {2020-06-17},
  doi = {10.1101/2020.06.16.154542},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.06.16.154542},
  urldate = {2024-06-17},
  abstract = {Current state-of-the-art object recognition models are largely based on convolutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we first observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adversarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a fixed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientific model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor filter bank, simple and complex cell nonlinearities, and a V1 neuronal stochasticity generator. After training, VOneNets retain high ImageNet performance, but each is substantially more robust, outperforming the base CNNs and state-of-the-art methods by 18\% and 3\%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications.},
  file = {/home/cristi/Zotero/storage/IT4N5DPV/Dapello et al. - 2020 - Simulating a Primary Visual Cortex at the Front of.pdf}
}

@online{dapelloSimulatingPrimaryVisual2020e,
  title = {Simulating a {{Primary Visual Cortex}} at the {{Front}} of {{CNNs Improves Robustness}} to {{Image Perturbations}}},
  author = {Dapello, Joel and Marques, Tiago and Schrimpf, Martin and Geiger, Franziska and Cox, David D. and DiCarlo, James J.},
  date = {2020-06-17},
  doi = {10.1101/2020.06.16.154542},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.06.16.154542},
  urldate = {2024-06-16},
  abstract = {Abstract Current state-of-the-art object recognition models are largely based on convolutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we first observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adversarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a fixed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientific model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor filter bank, simple and complex cell nonlinearities, and a V1 neuronal stochasticity generator. After training, VOneNets retain high ImageNet performance, but each is substantially more robust, outperforming the base CNNs and state-of-the-art methods by 18\% and 3\%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications.},
  file = {/home/cristi/Zotero/storage/9ZMIDKWS/Dapello et al. - 2020 - Simulating a Primary Visual Cortex at the Front of.pdf}
}

@online{dapelloSimulatingPrimaryVisual2020f,
  title = {Simulating a {{Primary Visual Cortex}} at the {{Front}} of {{CNNs Improves Robustness}} to {{Image Perturbations}}},
  author = {Dapello, Joel and Marques, Tiago and Schrimpf, Martin and Geiger, Franziska and Cox, David D. and DiCarlo, James J.},
  date = {2020-06-17},
  doi = {10.1101/2020.06.16.154542},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.06.16.154542},
  urldate = {2024-06-16},
  abstract = {Abstract Current state-of-the-art object recognition models are largely based on convolutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we first observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adversarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a fixed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientific model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor filter bank, simple and complex cell nonlinearities, and a V1 neuronal stochasticity generator. After training, VOneNets retain high ImageNet performance, but each is substantially more robust, outperforming the base CNNs and state-of-the-art methods by 18\% and 3\%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications.},
  file = {/home/cristi/Zotero/storage/XE34AMPU/Dapello et al. - 2020 - Simulating a Primary Visual Cortex at the Front of.pdf}
}

@online{dapelloSimulatingPrimaryVisual2020g,
  title = {Simulating a {{Primary Visual Cortex}} at the {{Front}} of {{CNNs Improves Robustness}} to {{Image Perturbations}}},
  author = {Dapello, Joel and Marques, Tiago and Schrimpf, Martin and Geiger, Franziska and Cox, David D. and DiCarlo, James J.},
  date = {2020-06-17},
  doi = {10.1101/2020.06.16.154542},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.06.16.154542},
  urldate = {2024-06-16},
  abstract = {Abstract Current state-of-the-art object recognition models are largely based on convolutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we first observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adversarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a fixed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientific model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor filter bank, simple and complex cell nonlinearities, and a V1 neuronal stochasticity generator. After training, VOneNets retain high ImageNet performance, but each is substantially more robust, outperforming the base CNNs and state-of-the-art methods by 18\% and 3\%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications.},
  file = {/home/cristi/Zotero/storage/TBFBUQ5N/Dapello et al. - 2020 - Simulating a Primary Visual Cortex at the Front of.pdf}
}

@article{davidPredictingNeuronalResponses2005,
  title = {Predicting Neuronal Responses during Natural Vision},
  author = {David, Stephen V. and Gallant, Jack L.},
  date = {2005-01},
  journaltitle = {Network: Computation in Neural Systems},
  shortjournal = {Network: Computation in Neural Systems},
  volume = {16},
  number = {2--3},
  pages = {239--260},
  issn = {0954-898X, 1361-6536},
  doi = {10.1080/09548980500464030},
  url = {https://www.tandfonline.com/doi/full/10.1080/09548980500464030},
  urldate = {2025-02-17},
  langid = {english},
  file = {/home/cristi/Zotero/storage/N27PMM55/David and Gallant - 2005 - Predicting neuronal responses during natural vision.pdf}
}

@article{davidPredictingNeuronalResponses2005a,
  title = {Predicting {{Neuronal Responses}} during {{Natural Vision}}},
  author = {David, Stephen V. and Gallant, Jack L.},
  date = {2005-01},
  journaltitle = {Network: Computation in Neural Systems},
  shortjournal = {Network: Computation in Neural Systems},
  volume = {16},
  number = {2--3},
  pages = {239--260},
  issn = {0954-898X, 1361-6536},
  doi = {10.1080/09548980500464030},
  url = {https://www.tandfonline.com/doi/full/10.1080/09548980500464030},
  urldate = {2025-02-17},
  file = {/home/cristi/Zotero/storage/D8H35AXW/David and Gallant - 2005 - Predicting neuronal responses during natural vision.pdf}
}

@article{davisCommonsenseReasoningCommonsense2015,
  title = {Commonsense Reasoning and Commonsense Knowledge in Artificial Intelligence},
  author = {Davis, Ernest and Marcus, Gary},
  date = {2015-08-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {58},
  number = {9},
  pages = {92--103},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2701413},
  url = {https://dl.acm.org/doi/10.1145/2701413},
  urldate = {2024-01-07},
  abstract = {AI has seen great advances of many kinds recently, but there is one critical area where progress has been extremely slow: ordinary commonsense.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/JWN7ZJZU/Davis and Marcus - 2015 - Commonsense reasoning and commonsense knowledge in.pdf}
}

@article{davisCommonsenseReasoningCommonsense2015a,
  title = {Commonsense {{Reasoning}} and {{Commonsense Knowledge}} in {{Artificial Intelligence}}},
  author = {Davis, Ernest and Marcus, Gary},
  date = {2015-08-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {58},
  number = {9},
  pages = {92--103},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2701413},
  url = {https://dl.acm.org/doi/10.1145/2701413},
  urldate = {2024-01-07},
  abstract = {AI has seen great advances of many kinds recently, but there is one critical area where progress has been extremely slow: ordinary commonsense.},
  file = {/home/cristi/Zotero/storage/KQ8Z4WTE/Davis and Marcus - 2015 - Commonsense reasoning and commonsense knowledge in.pdf}
}

@article{dehaene-lambertzInfancyHumanBrain2015,
  title = {The {{Infancy}} of the {{Human Brain}}},
  author = {Dehaene-Lambertz, G. and Spelke, E.S.},
  date = {2015-10},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {88},
  number = {1},
  pages = {93--109},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.09.026},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627315008156},
  urldate = {2024-05-17},
  langid = {english},
  file = {/home/cristi/Zotero/storage/Y8N5SKSR/Dehaene-Lambertz and Spelke - 2015 - The Infancy of the Human Brain.pdf}
}

@article{dehaene-lambertzInfancyHumanBrain2015a,
  title = {The {{Infancy}} of the {{Human Brain}}},
  author = {Dehaene-Lambertz, G. and Spelke, E.S.},
  date = {2015-10},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {88},
  number = {1},
  pages = {93--109},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.09.026},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627315008156},
  urldate = {2024-05-17},
  file = {/home/cristi/Zotero/storage/HW9ER4G2/Dehaene-Lambertz and Spelke - 2015 - The Infancy of the Human Brain.pdf}
}

@article{demertziHumanConsciousnessSupported2019,
  title = {Human Consciousness Is Supported by Dynamic Complex Patterns of Brain Signal Coordination},
  author = {Demertzi, A. and Tagliazucchi, E. and Dehaene, S. and Deco, G. and Barttfeld, P. and Raimondo, F. and Martial, C. and Fernández-Espejo, D. and Rohaut, B. and Voss, H. U. and Schiff, N. D. and Owen, A. M. and Laureys, S. and Naccache, L. and Sitt, J. D.},
  date = {2019-02},
  journaltitle = {Science Advances},
  shortjournal = {Sci. Adv.},
  volume = {5},
  number = {2},
  pages = {eaat7603},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aat7603},
  url = {https://www.science.org/doi/10.1126/sciadv.aat7603},
  urldate = {2024-05-17},
  abstract = {Dynamic patterns of brain activity at rest distinguish conscious and unconscious states in humans.           ,              Adopting the framework of brain dynamics as a cornerstone of human consciousness, we determined whether dynamic signal coordination provides specific and generalizable patterns pertaining to conscious and unconscious states after brain damage. A dynamic pattern of coordinated and anticoordinated functional magnetic resonance imaging signals characterized healthy individuals and minimally conscious patients. The brains of unresponsive patients showed primarily a pattern of low interareal phase coherence mainly mediated by structural connectivity, and had smaller chances to transition between patterns. The complex pattern was further corroborated in patients with covert cognition, who could perform neuroimaging mental imagery tasks, validating this pattern’s implication in consciousness. Anesthesia increased the probability of the less complex pattern to equal levels, validating its implication in unconsciousness. Our results establish that consciousness rests on the brain’s ability to sustain rich brain dynamics and pave the way for determining specific and generalizable fingerprints of conscious and unconscious states.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/KAZHP9TV/Demertzi et al. - 2019 - Human consciousness is supported by dynamic comple.pdf}
}

@article{demertziHumanConsciousnessSupported2019a,
  title = {Human {{Consciousness Is Supported}} by {{Dynamic Complex Patterns}} of {{Brain Signal Coordination}}},
  author = {Demertzi, A. and Tagliazucchi, E. and Dehaene, S. and Deco, G. and Barttfeld, P. and Raimondo, F. and Martial, C. and Fernández-Espejo, D. and Rohaut, B. and Voss, H. U. and Schiff, N. D. and Owen, A. M. and Laureys, S. and Naccache, L. and Sitt, J. D.},
  date = {2019-02},
  journaltitle = {Science Advances},
  shortjournal = {Sci. Adv.},
  volume = {5},
  number = {2},
  pages = {eaat7603},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aat7603},
  url = {https://www.science.org/doi/10.1126/sciadv.aat7603},
  urldate = {2024-05-17},
  abstract = {Dynamic patterns of brain activity at rest distinguish conscious and unconscious states in humans. , Adopting the framework of brain dynamics as a cornerstone of human consciousness, we determined whether dynamic signal coordination provides specific and generalizable patterns pertaining to conscious and unconscious states after brain damage. A dynamic pattern of coordinated and anticoordinated functional magnetic resonance imaging signals characterized healthy individuals and minimally conscious patients. The brains of unresponsive patients showed primarily a pattern of low interareal phase coherence mainly mediated by structural connectivity, and had smaller chances to transition between patterns. The complex pattern was further corroborated in patients with covert cognition, who could perform neuroimaging mental imagery tasks, validating this pattern’s implication in consciousness. Anesthesia increased the probability of the less complex pattern to equal levels, validating its implication in unconsciousness. Our results establish that consciousness rests on the brain’s ability to sustain rich brain dynamics and pave the way for determining specific and generalizable fingerprints of conscious and unconscious states.},
  file = {/home/cristi/Zotero/storage/DLXXE4L7/Demertzi et al. - 2019 - Human consciousness is supported by dynamic comple.pdf}
}

@article{desloovereMiniplateOsteosynthesisZygomatic1988,
  title = {[Mini-plate osteosynthesis in zygomatic fractures--restoration or alternative?]},
  shorttitle = {[Mini-plate osteosynthesis in zygomatic fractures--restoration or alternative?},
  author = {Desloovere, C. and Meyer-Breiting, E. and Häuser, H.},
  date = {1988-12},
  journaltitle = {Laryngologie, Rhinologie, Otologie},
  shortjournal = {Laryngol Rhinol Otol (Stuttg)},
  volume = {67},
  number = {12},
  eprint = {3210856},
  eprinttype = {pubmed},
  pages = {634--638},
  issn = {0340-1588},
  abstract = {At the University ENT Clinic Frankfurt 105 patients with zygomatic fractures were treated from 1980 until 1986: 45 were treated with a maxillary sinus stent, part of them in combination with wire osteosynthesis, miniplate osteosynthesis was performed in 30 patients, some fractures seemed stable after reposition without fixation. The zygomatic fractures are classified into 3 types requiring different surgical treatments. Comparing the long term results of these methods with the pre- and postoperative radiological and functional data, patients with type 2 and 3 fractures do better with miniplate osteosynthesis than with the other treatments. In combination with an orbital floor fracture, the degree of enophthalmus is more severe in those patients where fractures were only repositioned without fixation.},
  langid = {german},
  keywords = {Bone Plates,Follow-Up Studies,Fracture Fixation Internal,Humans,Ophthalmoplegia,Orbital Fractures,Postoperative Complications,Wound Healing,Zygomatic Fractures}
}

@article{desloovereMiniplateOsteosynthesisZygomatic1988a,
  title = {[{{Mini-plate}} Osteosynthesis in Zygomatic Fractures–Restoration or Alternative?]},
  shorttitle = {[{{Mini-plate}} Osteosynthesis in Zygomatic Fractures–Restoration or Alternative?},
  author = {Desloovere, C. and Meyer-Breiting, E. and Häuser, H.},
  date = {1988-12},
  journaltitle = {Laryngologie, Rhinologie, Otologie},
  shortjournal = {Laryngol Rhinol Otol (Stuttg)},
  volume = {67},
  number = {12},
  pages = {634--638},
  issn = {0340-1588},
  abstract = {At the University ENT Clinic Frankfurt 105 patients with zygomatic fractures were treated from 1980 until 1986: 45 were treated with a maxillary sinus stent, part of them in combination with wire osteosynthesis, miniplate osteosynthesis was performed in 30 patients, some fractures seemed stable after reposition without fixation. The zygomatic fractures are classified into 3 types requiring different surgical treatments. Comparing the long term results of these methods with the pre- and postoperative radiological and functional data, patients with type 2 and 3 fractures do better with miniplate osteosynthesis than with the other treatments. In combination with an orbital floor fracture, the degree of enophthalmus is more severe in those patients where fractures were only repositioned without fixation.},
  keywords = {Bone Plates,Follow-Up Studies,Fracture Fixation Internal,Humans,Ophthalmoplegia,Orbital Fractures,Postoperative Complications,Wound Healing,Zygomatic Fractures},
  annotation = {\_eprinttype: pmid}
}

@article{dijkstraDifferentialTemporalDynamics2018,
  title = {Differential Temporal Dynamics during Visual Imagery and Perception},
  author = {Dijkstra, Nadine and Mostert, Pim and Lange, Floris P De and Bosch, Sander and Van Gerven, Marcel Aj},
  date = {2018-05-29},
  journaltitle = {eLife},
  volume = {7},
  pages = {e33904},
  issn = {2050-084X},
  doi = {10.7554/eLife.33904},
  url = {https://elifesciences.org/articles/33904},
  urldate = {2024-05-17},
  abstract = {Visual perception and imagery rely on similar representations in the visual cortex. During perception, visual activity is characterized by distinct processing stages, but the temporal dynamics underlying imagery remain unclear. Here, we investigated the dynamics of visual imagery in human participants using magnetoencephalography. Firstly, we show that, compared to perception, imagery decoding becomes significant later and representations at the start of imagery already overlap with later time points. This suggests that during imagery, the entire visual representation is activated at once or that there are large differences in the timing of imagery between trials. Secondly, we found consistent overlap between imagery and perceptual processing around 160 ms and from 300 ms after stimulus onset. This indicates that the N170 gets reactivated during imagery and that imagery does not rely on early perceptual representations. Together, these results provide important insights for our understanding of the neural mechanisms of visual imagery.           ,              If someone stops you in the street to ask for directions, you might find yourself picturing a particular crossing in your mind’s eye as you explain the route. This ability to mentally visualize things that we cannot see is known as visual imagery. Neuroscientists have shown that imagining an object activates some of the same brain regions as looking at that object. But do these regions also become active in the same order when we imagine rather than perceive?             Our ability to see the world around us depends on light bouncing off objects and entering the eye, which converts it into electrical signals. These signals travel to an area at the back of the brain that processes basic visual features, such as lines and angles. The electrical activity then spreads forward through the brain toward other visual areas, which perform more complex processing. Within a few hundred milliseconds of light entering the eye, the brain generates a percept of the object in front of us.             So, does the brain perform these same steps when we mentally visualize an object? Dijkstra et al. measured brain activity in healthy volunteers while they either imagined faces and houses, or looked at pictures of them. Electrical activity spread from visual areas at the back of the brain to visual areas nearer the front as the volunteers looked at the pictures. But this did not happen when the volunteers imagined the faces and houses. Contrary to perception, the different brain areas did not seem to become activated in any apparent order. Instead, the brain areas active during imagining were those that only became active during perception after 130 milliseconds. This is the time at which brain areas responsible for complex visual processing become active when we look at objects.             These findings shed new light on how we see with our mind’s eye. They suggest that when we imagine an object, the brain activates the entire representation of that object at once rather than building it up in steps. Understanding how the brain forms a mental image in real time could help us develop new technologies, such as brain-computer interfaces. These devices aim to interpret patterns of brain activity and display the output on a computer. Such equipment could help people with paralysis to communicate.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/47CHFMUH/Dijkstra et al. - 2018 - Differential temporal dynamics during visual image.pdf}
}

@article{dijkstraDifferentialTemporalDynamics2018a,
  title = {Differential {{Temporal Dynamics}} during {{Visual Imagery}} and {{Perception}}},
  author = {Dijkstra, Nadine and Mostert, Pim and Lange, Floris P De and Bosch, Sander and Van Gerven, Marcel Aj},
  date = {2018-05-29},
  journaltitle = {eLife},
  volume = {7},
  pages = {e33904},
  issn = {2050-084X},
  doi = {10.7554/eLife.33904},
  url = {https://elifesciences.org/articles/33904},
  urldate = {2024-05-17},
  abstract = {Visual perception and imagery rely on similar representations in the visual cortex. During perception, visual activity is characterized by distinct processing stages, but the temporal dynamics underlying imagery remain unclear. Here, we investigated the dynamics of visual imagery in human participants using magnetoencephalography. Firstly, we show that, compared to perception, imagery decoding becomes significant later and representations at the start of imagery already overlap with later time points. This suggests that during imagery, the entire visual representation is activated at once or that there are large differences in the timing of imagery between trials. Secondly, we found consistent overlap between imagery and perceptual processing around 160 ms and from 300 ms after stimulus onset. This indicates that the N170 gets reactivated during imagery and that imagery does not rely on early perceptual representations. Together, these results provide important insights for our understanding of the neural mechanisms of visual imagery. , If someone stops you in the street to ask for directions, you might find yourself picturing a particular crossing in your mind’s eye as you explain the route. This ability to mentally visualize things that we cannot see is known as visual imagery. Neuroscientists have shown that imagining an object activates some of the same brain regions as looking at that object. But do these regions also become active in the same order when we imagine rather than perceive? Our ability to see the world around us depends on light bouncing off objects and entering the eye, which converts it into electrical signals. These signals travel to an area at the back of the brain that processes basic visual features, such as lines and angles. The electrical activity then spreads forward through the brain toward other visual areas, which perform more complex processing. Within a few hundred milliseconds of light entering the eye, the brain generates a percept of the object in front of us. So, does the brain perform these same steps when we mentally visualize an object? Dijkstra et al. measured brain activity in healthy volunteers while they either imagined faces and houses, or looked at pictures of them. Electrical activity spread from visual areas at the back of the brain to visual areas nearer the front as the volunteers looked at the pictures. But this did not happen when the volunteers imagined the faces and houses. Contrary to perception, the different brain areas did not seem to become activated in any apparent order. Instead, the brain areas active during imagining were those that only became active during perception after 130 milliseconds. This is the time at which brain areas responsible for complex visual processing become active when we look at objects. These findings shed new light on how we see with our mind’s eye. They suggest that when we imagine an object, the brain activates the entire representation of that object at once rather than building it up in steps. Understanding how the brain forms a mental image in real time could help us develop new technologies, such as brain-computer interfaces. These devices aim to interpret patterns of brain activity and display the output on a computer. Such equipment could help people with paralysis to communicate.},
  file = {/home/cristi/Zotero/storage/USFIFXA8/Dijkstra et al. - 2018 - Differential temporal dynamics during visual image.pdf}
}

@online{doshi-velezRigorousScienceInterpretable2017,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {Doshi-Velez, Finale and Kim, Been},
  date = {2017},
  doi = {10.48550/ARXIV.1702.08608},
  url = {https://arxiv.org/abs/1702.08608},
  urldate = {2025-03-27},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/home/cristi/Zotero/storage/MDYEL5L5/Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machine Learning.pdf}
}

@online{doshi-velezRigorousScienceInterpretable2017a,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {Doshi-Velez, Finale and Kim, Been},
  date = {2017},
  doi = {10.48550/ARXIV.1702.08608},
  url = {https://arxiv.org/abs/1702.08608},
  urldate = {2025-03-27},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@online{doshi-velezRigorousScienceInterpretable2017b,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {Doshi-Velez, Finale and Kim, Been},
  date = {2017-03-02},
  eprint = {1702.08608},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1702.08608},
  url = {http://arxiv.org/abs/1702.08608},
  urldate = {2025-03-27},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{DoubleMasterProgramme,
  title = {Double {{Master}}'s Programme Opportunities},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-cognitive-computing/double-masters-programme-opportunities},
  urldate = {2022-10-08},
  file = {/home/cristi/Zotero/storage/PUZV99RE/double-masters-programme-opportunities.html}
}

@online{DoubleMastersProgramme,
  title = {Double {{Master}}'s {{Programme Opportunities}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-cognitive-computing/double-masters-programme-opportunities},
  urldate = {2022-10-08},
  file = {/home/cristi/Zotero/storage/HMHL4BZV/double-masters-programme-opportunities.html}
}

@article{drakeCaffeineEffectsSleep2013,
  title = {Caffeine {{Effects}} on {{Sleep Taken}} 0, 3, or 6 {{Hours}} before {{Going}} to {{Bed}}},
  author = {Drake, Christopher and Roehrs, Timothy and Shambroom, John and Roth, Thomas},
  date = {2013-11-15},
  journaltitle = {Journal of Clinical Sleep Medicine},
  shortjournal = {Journal of Clinical Sleep Medicine},
  volume = {09},
  number = {11},
  pages = {1195--1200},
  issn = {1550-9389, 1550-9397},
  doi = {10.5664/jcsm.3170},
  url = {http://jcsm.aasm.org/doi/10.5664/jcsm.3170},
  urldate = {2023-02-08},
  langid = {english},
  file = {/home/cristi/Zotero/storage/UUE9QDJW/Drake et al. - 2013 - Caffeine Effects on Sleep Taken 0, 3, or 6 Hours b.pdf}
}

@article{drakeCaffeineEffectsSleep2013a,
  title = {Caffeine {{Effects}} on {{Sleep Taken}} 0, 3, or 6 {{Hours}} before {{Going}} to {{Bed}}},
  author = {Drake, Christopher and Roehrs, Timothy and Shambroom, John and Roth, Thomas},
  date = {2013-11-15},
  journaltitle = {Journal of Clinical Sleep Medicine},
  shortjournal = {Journal of Clinical Sleep Medicine},
  volume = {09},
  number = {11},
  pages = {1195--1200},
  issn = {1550-9389, 1550-9397},
  doi = {10.5664/jcsm.3170},
  url = {http://jcsm.aasm.org/doi/10.5664/jcsm.3170},
  urldate = {2023-02-08},
  file = {/home/cristi/Zotero/storage/IBBXUFPI/Drake et al. - 2013 - Caffeine Effects on Sleep Taken 0, 3, or 6 Hours b.pdf}
}

@article{dronkersWhatLanguageDisorders2017,
  title = {What {{Do Language Disorders Reveal}} about {{Brain}}–{{Language Relationships}}? {{From Classic Models}} to {{Network Approaches}}},
  shorttitle = {What {{Do Language Disorders Reveal}} about {{Brain}}–{{Language Relationships}}?},
  author = {Dronkers, Nina F. and Ivanova, Maria V. and Baldo, Juliana V.},
  date = {2017-10},
  journaltitle = {Journal of the International Neuropsychological Society},
  shortjournal = {J Int Neuropsychol Soc},
  volume = {23},
  number = {9--10},
  pages = {741--754},
  issn = {1355-6177, 1469-7661},
  doi = {10.1017/S1355617717001126},
  url = {https://www.cambridge.org/core/product/identifier/S1355617717001126/type/journal_article},
  urldate = {2023-02-08},
  abstract = {Abstract                            Studies of language disorders have shaped our understanding of brain–language relationships over the last two centuries. This article provides a review of this research and how our thinking has changed over the years regarding how the brain processes language. In the 19th century, a series of famous case studies linked distinct speech and language functions to specific portions of the left hemisphere of the brain, regions that later came to be known as Broca’s and Wernicke’s areas. One hundred years later, the emergence of new brain imaging tools allowed for the visualization of brain injuries               in vivo               that ushered in a new era of brain-behavior research and greatly expanded our understanding of the neural processes of language. Toward the end of the 20th century, sophisticated neuroimaging approaches allowed for the visualization of both structural and functional brain activity associated with language processing in both healthy individuals and in those with language disturbance. More recently, language is thought to be mediated by a much broader expanse of neural networks that covers a large number of cortical and subcortical regions and their interconnecting fiber pathways. Injury to both grey and white matter has been seen to affect the complexities of language in unique ways that have altered how we think about brain–language relationships. The findings that support this paradigm shift are described here along with the methodologies that helped to discover them, with some final thoughts on future directions, techniques, and treatment interventions for those with communication impairments. (               JINS               , 2017,               23               , 741–754)},
  langid = {english},
  file = {/home/cristi/Zotero/storage/TB988LZJ/Dronkers et al. - 2017 - What Do Language Disorders Reveal about Brain–Lang.pdf}
}

@article{dronkersWhatLanguageDisorders2017a,
  title = {What {{Do Language Disorders Reveal}} about {{Brain}}–{{Language Relationships}}? {{From Classic Models}} to {{Network Approaches}}},
  shorttitle = {What {{Do Language Disorders Reveal}} about {{Brain}}–{{Language Relationships}}?},
  author = {Dronkers, Nina F. and Ivanova, Maria V. and Baldo, Juliana V.},
  date = {2017-10},
  journaltitle = {Journal of the International Neuropsychological Society},
  shortjournal = {J Int Neuropsychol Soc},
  volume = {23},
  number = {9--10},
  pages = {741--754},
  issn = {1355-6177, 1469-7661},
  doi = {10.1017/S1355617717001126},
  url = {https://www.cambridge.org/core/product/identifier/S1355617717001126/type/journal_article},
  urldate = {2023-02-08},
  abstract = {Abstract                            Studies of language disorders have shaped our understanding of brain–language relationships over the last two centuries. This article provides a review of this research and how our thinking has changed over the years regarding how the brain processes language. In the 19th century, a series of famous case studies linked distinct speech and language functions to specific portions of the left hemisphere of the brain, regions that later came to be known as Broca’s and Wernicke’s areas. One hundred years later, the emergence of new brain imaging tools allowed for the visualization of brain injuries               in vivo               that ushered in a new era of brain-behavior research and greatly expanded our understanding of the neural processes of language. Toward the end of the 20th century, sophisticated neuroimaging approaches allowed for the visualization of both structural and functional brain activity associated with language processing in both healthy individuals and in those with language disturbance. More recently, language is thought to be mediated by a much broader expanse of neural networks that covers a large number of cortical and subcortical regions and their interconnecting fiber pathways. Injury to both grey and white matter has been seen to affect the complexities of language in unique ways that have altered how we think about brain–language relationships. The findings that support this paradigm shift are described here along with the methodologies that helped to discover them, with some final thoughts on future directions, techniques, and treatment interventions for those with communication impairments. (               JINS               , 2017,               23               , 741–754)},
  langid = {english},
  file = {/home/cristi/Zotero/storage/RLU86XNL/Dronkers et al. - 2017 - What Do Language Disorders Reveal about Brain–Lang.pdf}
}

@article{dronkersWhatLanguageDisorders2017b,
  title = {What {{Do Language Disorders Reveal}} about {{Brain}}–{{Language Relationships}}? {{From Classic Models}} to {{Network Approaches}}},
  shorttitle = {What {{Do Language Disorders Reveal}} about {{Brain}}–{{Language Relationships}}?},
  author = {Dronkers, Nina F. and Ivanova, Maria V. and Baldo, Juliana V.},
  date = {2017-10},
  journaltitle = {Journal of the International Neuropsychological Society},
  shortjournal = {J Int Neuropsychol Soc},
  volume = {23},
  number = {9--10},
  pages = {741--754},
  issn = {1355-6177, 1469-7661},
  doi = {10.1017/S1355617717001126},
  url = {https://www.cambridge.org/core/product/identifier/S1355617717001126/type/journal_article},
  urldate = {2023-02-08},
  abstract = {Abstract Studies of language disorders have shaped our understanding of brain–language relationships over the last two centuries. This article provides a review of this research and how our thinking has changed over the years regarding how the brain processes language. In the 19th century, a series of famous case studies linked distinct speech and language functions to specific portions of the left hemisphere of the brain, regions that later came to be known as Broca’s and Wernicke’s areas. One hundred years later, the emergence of new brain imaging tools allowed for the visualization of brain injuries in vivo that ushered in a new era of brain-behavior research and greatly expanded our understanding of the neural processes of language. Toward the end of the 20th century, sophisticated neuroimaging approaches allowed for the visualization of both structural and functional brain activity associated with language processing in both healthy individuals and in those with language disturbance. More recently, language is thought to be mediated by a much broader expanse of neural networks that covers a large number of cortical and subcortical regions and their interconnecting fiber pathways. Injury to both grey and white matter has been seen to affect the complexities of language in unique ways that have altered how we think about brain–language relationships. The findings that support this paradigm shift are described here along with the methodologies that helped to discover them, with some final thoughts on future directions, techniques, and treatment interventions for those with communication impairments. ( JINS , 2017, 23 , 741–754)},
  file = {/home/cristi/Zotero/storage/LBUXVRJ4/Dronkers et al. - 2017 - What Do Language Disorders Reveal about Brain–Lang.pdf}
}

@article{dronkersWhatLanguageDisorders2017c,
  title = {What {{Do Language Disorders Reveal}} about {{Brain}}–{{Language Relationships}}? {{From Classic Models}} to {{Network Approaches}}},
  shorttitle = {What {{Do Language Disorders Reveal}} about {{Brain}}–{{Language Relationships}}?},
  author = {Dronkers, Nina F. and Ivanova, Maria V. and Baldo, Juliana V.},
  date = {2017-10},
  journaltitle = {Journal of the International Neuropsychological Society},
  shortjournal = {J Int Neuropsychol Soc},
  volume = {23},
  number = {9--10},
  pages = {741--754},
  issn = {1355-6177, 1469-7661},
  doi = {10.1017/S1355617717001126},
  url = {https://www.cambridge.org/core/product/identifier/S1355617717001126/type/journal_article},
  urldate = {2023-02-08},
  abstract = {Abstract Studies of language disorders have shaped our understanding of brain–language relationships over the last two centuries. This article provides a review of this research and how our thinking has changed over the years regarding how the brain processes language. In the 19th century, a series of famous case studies linked distinct speech and language functions to specific portions of the left hemisphere of the brain, regions that later came to be known as Broca’s and Wernicke’s areas. One hundred years later, the emergence of new brain imaging tools allowed for the visualization of brain injuries in vivo that ushered in a new era of brain-behavior research and greatly expanded our understanding of the neural processes of language. Toward the end of the 20th century, sophisticated neuroimaging approaches allowed for the visualization of both structural and functional brain activity associated with language processing in both healthy individuals and in those with language disturbance. More recently, language is thought to be mediated by a much broader expanse of neural networks that covers a large number of cortical and subcortical regions and their interconnecting fiber pathways. Injury to both grey and white matter has been seen to affect the complexities of language in unique ways that have altered how we think about brain–language relationships. The findings that support this paradigm shift are described here along with the methodologies that helped to discover them, with some final thoughts on future directions, techniques, and treatment interventions for those with communication impairments. ( JINS , 2017, 23 , 741–754)},
  file = {/home/cristi/Zotero/storage/DN6EG8ZV/Dronkers et al. - 2017 - What Do Language Disorders Reveal about Brain–Lang.pdf}
}

@article{eaglemanVisualIllusionsNeurobiology2001,
  title = {Visual Illusions and Neurobiology},
  author = {Eagleman, David M.},
  date = {2001-12},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {2},
  number = {12},
  pages = {920--926},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/35104092},
  url = {https://www.nature.com/articles/35104092},
  urldate = {2024-05-27},
  langid = {english},
  file = {/home/cristi/Zotero/storage/ATXYBE6W/Eagleman - 2001 - Visual illusions and neurobiology.pdf}
}

@article{eaglemanVisualIllusionsNeurobiology2001a,
  title = {Visual {{Illusions}} and {{Neurobiology}}},
  author = {Eagleman, David M.},
  date = {2001-12},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {2},
  number = {12},
  pages = {920--926},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/35104092},
  url = {https://www.nature.com/articles/35104092},
  urldate = {2024-05-27},
  file = {/home/cristi/Zotero/storage/3CQXNLZ7/Eagleman - 2001 - Visual illusions and neurobiology.pdf}
}

@online{elsayedAdversarialExamplesThat2018,
  title = {Adversarial {{Examples}} That {{Fool}} Both {{Computer Vision}} and {{Time-Limited Humans}}},
  author = {Elsayed, Gamaleldin F. and Shankar, Shreya and Cheung, Brian and Papernot, Nicolas and Kurakin, Alex and Goodfellow, Ian and Sohl-Dickstein, Jascha},
  date = {2018-05-21},
  eprint = {1802.08195},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio, stat},
  url = {http://arxiv.org/abs/1802.08195},
  urldate = {2024-06-17},
  abstract = {Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/home/cristi/Zotero/storage/GX7DMQB9/Elsayed et al. - 2018 - Adversarial Examples that Fool both Computer Visio.pdf;/home/cristi/Zotero/storage/7QZ5K3CW/1802.html}
}

@online{elsayedAdversarialExamplesThat2018a,
  title = {Adversarial {{Examples That Fool Both Computer Vision}} and {{Time-Limited Humans}}},
  author = {Elsayed, Gamaleldin F. and Shankar, Shreya and Cheung, Brian and Papernot, Nicolas and Kurakin, Alex and Goodfellow, Ian and Sohl-Dickstein, Jascha},
  date = {2018-05-21},
  eprint = {1802.08195},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1802.08195},
  urldate = {2024-06-17},
  abstract = {Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/home/cristi/Zotero/storage/B9C9RA8C/Elsayed et al. - 2018 - Adversarial Examples that Fool both Computer Visio.pdf;/home/cristi/Zotero/storage/ZUN8SV7K/1802.html}
}

@article{familyheijdenBrightSideInfluence2021,
  title = {On the {{Bright Side}}: {{The Influence}} of {{Brightness}} on {{Overall Taste Intensity Perception}}},
  shorttitle = {On the {{Bright Side}}},
  author = {{family=Heijden}, given=Kimberley, prefix=van der, useprefix=true and Festjens, Anouk and Goukens, Caroline},
  date = {2021-03},
  journaltitle = {Food Quality and Preference},
  shortjournal = {Food Quality and Preference},
  volume = {88},
  pages = {104099},
  issn = {09503293},
  doi = {10.1016/j.foodqual.2020.104099},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0950329320303682},
  urldate = {2023-02-08},
  file = {/home/cristi/Zotero/storage/BDDWSHBI/van der Heijden et al. - 2021 - On the bright side The influence of brightness on.pdf}
}

@article{felzmannTransparencyYouCan2019,
  title = {Transparency You Can Trust: {{Transparency}} Requirements for Artificial Intelligence between Legal Norms and Contextual Concerns},
  shorttitle = {Transparency You Can Trust},
  author = {Felzmann, Heike and Villaronga, Eduard Fosch and Lutz, Christoph and Tamò-Larrieux, Aurelia},
  date = {2019-01},
  journaltitle = {Big Data \& Society},
  shortjournal = {Big Data \& Society},
  volume = {6},
  number = {1},
  pages = {2053951719860542},
  issn = {2053-9517, 2053-9517},
  doi = {10.1177/2053951719860542},
  url = {https://journals.sagepub.com/doi/10.1177/2053951719860542},
  urldate = {2025-03-28},
  abstract = {Transparency is now a fundamental principle for data processing under the General Data Protection Regulation. We explore what this requirement entails for artificial intelligence and automated decision-making systems. We address the topic of transparency in artificial intelligence by integrating legal, social, and ethical aspects. We first investigate the ratio legis of the transparency requirement in the General Data Protection Regulation and its ethical underpinnings, showing its focus on the provision of information and explanation. We then discuss the pitfalls with respect to this requirement by focusing on the significance of contextual and performative factors in the implementation of transparency. We show that human–computer interaction and human-robot interaction literature do not provide clear results with respect to the benefits of transparency for users of artificial intelligence technologies due to the impact of a wide range of contextual factors, including performative aspects. We conclude by integrating the information- and explanation-based approach to transparency with the critical contextual approach, proposing that transparency as required by the General Data Protection Regulation in itself may be insufficient to achieve the positive goals associated with transparency. Instead, we propose to understand transparency relationally, where information provision is conceptualized as communication between technology providers and users, and where assessments of trustworthiness based on contextual factors mediate the value of transparency communications. This relational concept of transparency points to future research directions for the study of transparency in artificial intelligence systems and should be taken into account in policymaking.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/UFFXK933/Felzmann et al. - 2019 - Transparency you can trust Transparency requirements for artificial intelligence between legal norm.pdf}
}

@article{fernyhoughAlienVoicesInner2004,
  title = {Alien Voices and Inner Dialogue: Towards a Developmental Account of Auditory Verbal Hallucinations},
  shorttitle = {Alien Voices and Inner Dialogue},
  author = {Fernyhough, Charles},
  date = {2004-04},
  journaltitle = {New Ideas in Psychology},
  shortjournal = {New Ideas in Psychology},
  volume = {22},
  number = {1},
  pages = {49--68},
  issn = {0732118X},
  doi = {10.1016/j.newideapsych.2004.09.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0732118X04000054},
  urldate = {2024-11-29},
  langid = {english}
}

@article{fernyhoughAlienVoicesInner2004a,
  title = {Alien {{Voices}} and {{Inner Dialogue}}: {{Towards}} a {{Developmental Account}} of {{Auditory Verbal Hallucinations}}},
  shorttitle = {Alien {{Voices}} and {{Inner Dialogue}}},
  author = {Fernyhough, Charles},
  date = {2004-04},
  journaltitle = {New Ideas in Psychology},
  shortjournal = {New Ideas in Psychology},
  volume = {22},
  number = {1},
  pages = {49--68},
  issn = {0732118X},
  doi = {10.1016/j.newideapsych.2004.09.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0732118X04000054},
  urldate = {2024-11-29}
}

@online{filaAnswerWhyComplexity2016,
  title = {Answer to "{{Why}} Is the Complexity of {{Arc-Consistency Algorithm O}}(Cd\textasciicircum 3)?"},
  shorttitle = {Answer to "{{Why}} Is the Complexity of {{Arc-Consistency Algorithm O}}(Cd\textasciicircum 3)?},
  author = {Fila},
  date = {2016-04-21},
  url = {https://stackoverflow.com/a/36771818},
  urldate = {2023-11-22},
  organization = {Stack Overflow},
  file = {/home/cristi/Zotero/storage/TC9EIYM8/why-is-the-complexity-of-arc-consistency-algorithm-ocd3.html}
}

@online{filaAnswerWhyComplexity2016a,
  title = {Answer to "{{Why Is}} the {{Complexity}} of {{Arc-Consistency Algorithm O}}({{Cdˆ}} 3)?"},
  shorttitle = {Answer to "{{Why Is}} the {{Complexity}} of {{Arc-Consistency Algorithm O}}({{Cdˆ}} 3)?},
  author = {{Fila}},
  date = {2016-04-21},
  publisher = {Stack Overflow},
  url = {https://stackoverflow.com/a/36771818},
  urldate = {2023-11-22},
  file = {/home/cristi/Zotero/storage/DZQI2ILX/why-is-the-complexity-of-arc-consistency-algorithm-ocd3.html}
}

@article{fiskeYourRobotTherapist2019,
  title = {Your {{Robot Therapist Will See You Now}}: {{Ethical Implications}} of {{Embodied Artificial Intelligence}} in {{Psychiatry}}, {{Psychology}}, and {{Psychotherapy}}},
  shorttitle = {Your {{Robot Therapist Will See You Now}}},
  author = {Fiske, Amelia and Henningsen, Peter and Buyx, Alena},
  date = {2019-05-09},
  journaltitle = {Journal of Medical Internet Research},
  shortjournal = {J Med Internet Res},
  volume = {21},
  number = {5},
  pages = {e13216},
  issn = {1438-8871},
  doi = {10.2196/13216},
  url = {https://www.jmir.org/2019/5/e13216/},
  urldate = {2022-10-06},
  langid = {english},
  file = {/home/cristi/Zotero/storage/N4Q4KWW3/Fiske et al. - 2019 - Your Robot Therapist Will See You Now Ethical Imp.pdf}
}

@article{fiskeYourRobotTherapist2019a,
  title = {Your {{Robot Therapist Will See You Now}}: {{Ethical Implications}} of {{Embodied Artificial Intelligence}} in {{Psychiatry}}, {{Psychology}}, and {{Psychotherapy}}},
  shorttitle = {Your {{Robot Therapist Will See You Now}}},
  author = {Fiske, Amelia and Henningsen, Peter and Buyx, Alena},
  date = {2019-05-09},
  journaltitle = {Journal of Medical Internet Research},
  shortjournal = {J Med Internet Res},
  volume = {21},
  number = {5},
  pages = {e13216},
  issn = {1438-8871},
  doi = {10.2196/13216},
  url = {https://www.jmir.org/2019/5/e13216/},
  urldate = {2022-10-06},
  file = {/home/cristi/Zotero/storage/CLJIVKXL/Fiske et al. - 2019 - Your Robot Therapist Will See You Now Ethical Imp.pdf}
}

@book{foucaultDisciplinePunishBirth2020,
  title = {Discipline and Punish: The Birth of the Prison},
  shorttitle = {Discipline and Punish},
  author = {Foucault, Michel},
  translator = {Sheridan, Alan},
  date = {2020},
  publisher = {Penguin Classics},
  location = {London},
  isbn = {978-0-241-38601-9},
  langid = {english},
  pagetotal = {333}
}

@article{freemanEthicalTheoryMedical2006a,
  title = {Ethical {{Theory}} and {{Medical Ethics}}: {{A Personal Perspective}}},
  shorttitle = {Ethical {{Theory}} and {{Medical Ethics}}},
  author = {Freeman, J M},
  date = {2006-10},
  journaltitle = {Journal of Medical Ethics},
  shortjournal = {J Med Ethics},
  volume = {32},
  number = {10},
  pages = {617--618},
  issn = {0306-6800, 1473-4257},
  doi = {10.1136/jme.2005.014837},
  url = {https://jme.bmj.com/lookup/doi/10.1136/jme.2005.014837},
  urldate = {2025-02-20},
  file = {/home/cristi/Zotero/storage/5HKIZQ5X/Freeman - 2006 - Ethical theory and medical ethics a personal perspective.pdf}
}

@book{gazzanigaCognitiveNeuroscienceBiology2019,
  title = {Cognitive Neuroscience: The Biology of the Mind},
  shorttitle = {Cognitive Neuroscience},
  author = {Gazzaniga, Michael S. and Ivry, Richard B. and Mangun, G. R.},
  date = {2019},
  edition = {Fifth edition},
  publisher = {W.W. Norton \& Company},
  location = {New York},
  isbn = {978-0-393-60317-0},
  pagetotal = {1},
  keywords = {Cognitive neuroscience},
  file = {/home/cristi/Zotero/storage/4CKUMPW9/Gazzaniga et al. - 2019 - Cognitive neuroscience the biology of the mind.pdf}
}

@book{gazzanigaCognitiveNeuroscienceBiology2019a,
  title = {Cognitive {{Neuroscience}}: {{The Biology}} of the {{Mind}}},
  shorttitle = {Cognitive {{Neuroscience}}},
  author = {Gazzaniga, Michael S. and Ivry, Richard B. and Mangun, G. R.},
  date = {2019},
  edition = {Fifth edition},
  publisher = {W.W. Norton \& Company},
  location = {New York},
  isbn = {978-0-393-60317-0},
  pagetotal = {1},
  keywords = {Cognitive neuroscience},
  file = {/home/cristi/Zotero/storage/AVWKCSJY/Gazzaniga et al. - 2019 - Cognitive neuroscience the biology of the mind.pdf}
}

@online{GenerateQrCode,
  title = {Generate Qr Code - {{Google Search}}},
  url = {https://www.google.com/search?q=generate+qr+code&client=firefox-b-d&sxsrf=AJOqlzVLXSAVAIQ1g45OXfv8eeFFUa9vzw:1676901589628&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjtssqcoaT9AhXRtqQKHYiPCsMQ_AUoAXoECAEQAw&biw=1440&bih=756&dpr=2#imgrc=XTCSAKH6op7NVM},
  urldate = {2023-02-20},
  file = {/home/cristi/Zotero/storage/ZGPNYGC2/search.html}
}

@online{GenerateQrCodea,
  title = {Generate {{Qr Code}} - {{Google Search}}},
  url = {https://www.google.com/search?q=generate+qr+code&client=firefox-b-d&sxsrf=AJOqlzVLXSAVAIQ1g45OXfv8eeFFUa9vzw:1676901589628&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjtssqcoaT9AhXRtqQKHYiPCsMQ_AUoAXoECAEQAw&biw=1440&bih=756&dpr=2#imgrc=XTCSAKH6op7NVM},
  urldate = {2023-02-20},
  file = {/home/cristi/Zotero/storage/DCVLP935/search.html}
}

@inproceedings{georgescuKaldibasedDNNArchitectures2019,
  title = {Kaldi-Based {{DNN Architectures}} for {{Speech Recognition}} in {{Romanian}}},
  booktitle = {2019 {{International Conference}} on {{Speech Technology}} and {{Human-Computer Dialogue}} ({{SpeD}})},
  author = {Georgescu, Alexandru-Lucian and Cucu, Horia and Burileanu, Corneliu},
  date = {2019-10},
  pages = {1--6},
  publisher = {IEEE},
  location = {Timisoara, Romania},
  doi = {10.1109/SPED.2019.8906555},
  url = {https://ieeexplore.ieee.org/document/8906555/},
  urldate = {2023-03-17},
  eventtitle = {2019 {{International Conference}} on {{Speech Technology}} and {{Human-Computer Dialogue}} ({{SpeD}})},
  isbn = {978-1-7281-0984-8}
}

@inproceedings{georgescuKaldiBasedDNNArchitectures2019a,
  title = {Kaldi-{{Based DNN Architectures}} for {{Speech Recognition}} in {{Romanian}}},
  booktitle = {2019 {{International Conference}} on {{Speech Technology}} and {{Human-Computer Dialogue}} ({{SpeD}})},
  author = {Georgescu, Alexandru-Lucian and Cucu, Horia and Burileanu, Corneliu},
  date = {2019-10},
  pages = {1--6},
  publisher = {IEEE},
  location = {Timisoara, Romania},
  doi = {10.1109/SPED.2019.8906555},
  url = {https://ieeexplore.ieee.org/document/8906555/},
  urldate = {2023-03-17},
  eventtitle = {2019 {{International Conference}} on {{Speech Technology}} and {{Human-Computer Dialogue}} ({{SpeD}})},
  isbn = {978-1-7281-0984-8}
}

@article{geurtsPublisherCorrectionSubjective2022,
  title = {Publisher {{Correction}}: {{Subjective}} Confidence Reflects Representation of {{Bayesian}} Probability in Cortex},
  shorttitle = {Publisher {{Correction}}},
  author = {Geurts, Laura S. and Cooke, James R. H. and Van Bergen, Ruben S. and Jehee, Janneke F. M.},
  date = {2022-03-09},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {6},
  number = {3},
  pages = {470--470},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01326-6},
  url = {https://www.nature.com/articles/s41562-022-01326-6},
  urldate = {2024-10-06},
  langid = {english},
  file = {/home/cristi/Zotero/storage/NZAPUBXZ/Geurts et al. - 2022 - Publisher Correction Subjective confidence reflec.pdf}
}

@article{geurtsPublisherCorrectionSubjective2022a,
  title = {Publisher {{Correction}}: {{Subjective Confidence Reflects Representation}} of {{Bayesian Probability}} in {{Cortex}}},
  shorttitle = {Publisher {{Correction}}},
  author = {Geurts, Laura S. and Cooke, James R. H. and Van Bergen, Ruben S. and Jehee, Janneke F. M.},
  date = {2022-03-09},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {6},
  number = {3},
  pages = {470--470},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01326-6},
  url = {https://www.nature.com/articles/s41562-022-01326-6},
  urldate = {2024-10-06},
  file = {/home/cristi/Zotero/storage/RRSXC9QH/Geurts et al. - 2022 - Publisher Correction Subjective confidence reflec.pdf}
}

@article{geurtsSubjectiveConfidenceReflects2022,
  title = {Subjective Confidence Reflects Representation of {{Bayesian}} Probability in Cortex},
  author = {Geurts, Laura S. and Cooke, James R. H. and Van Bergen, Ruben S. and Jehee, Janneke F. M.},
  date = {2022-01-20},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {6},
  number = {2},
  pages = {294--305},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01247-w},
  url = {https://www.nature.com/articles/s41562-021-01247-w},
  urldate = {2024-10-06},
  langid = {english},
  file = {/home/cristi/Zotero/storage/432K42C9/Geurts et al. - 2022 - Subjective confidence reflects representation of B.pdf}
}

@article{geurtsSubjectiveConfidenceReflects2022a,
  title = {Subjective {{Confidence Reflects Representation}} of {{Bayesian Probability}} in {{Cortex}}},
  author = {Geurts, Laura S. and Cooke, James R. H. and Van Bergen, Ruben S. and Jehee, Janneke F. M.},
  date = {2022-01-20},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {6},
  number = {2},
  pages = {294--305},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01247-w},
  url = {https://www.nature.com/articles/s41562-021-01247-w},
  urldate = {2024-10-06},
  file = {/home/cristi/Zotero/storage/5JM3GNVI/Geurts et al. - 2022 - Subjective confidence reflects representation of B.pdf}
}

@article{giovannucciCaImAnOpenSource2019,
  title = {{{CaImAn}} an Open Source Tool for Scalable Calcium Imaging Data Analysis},
  author = {Giovannucci, Andrea and Friedrich, Johannes and Gunn, Pat and Kalfon, Jérémie and Brown, Brandon L and Koay, Sue Ann and Taxidis, Jiannis and Najafi, Farzaneh and Gauthier, Jeffrey L and Zhou, Pengcheng and Khakh, Baljit S and Tank, David W and Chklovskii, Dmitri B and Pnevmatikakis, Eftychios A},
  date = {2019-01-17},
  journaltitle = {eLife},
  volume = {8},
  pages = {e38173},
  issn = {2050-084X},
  doi = {10.7554/eLife.38173},
  url = {https://elifesciences.org/articles/38173},
  urldate = {2025-03-17},
  abstract = {Advances in fluorescence microscopy enable monitoring larger brain areas in-vivo with finer time resolution. The resulting data rates require reproducible analysis pipelines that are reliable, fully automated, and scalable to datasets generated over the course of months. We present CaImAn, an open-source library for calcium imaging data analysis. CaImAn provides automatic and scalable methods to address problems common to pre-processing, including motion correction, neural activity identification, and registration across different sessions of data collection. It does this while requiring minimal user intervention, with good scalability on computers ranging from laptops to high-performance computing clusters. CaImAn is suitable for two-photon and one-photon imaging, and also enables real-time analysis on streaming data. To benchmark the performance of CaImAn we collected and combined a corpus of manual annotations from multiple labelers on nine mouse two-photon datasets. We demonstrate that CaImAn achieves near-human performance in detecting locations of active neurons.           ,              The human brain contains billions of cells called neurons that rapidly carry information from one part of the brain to another. Progress in medical research and healthcare is hindered by the difficulty in understanding precisely which neurons are active at any given time. New brain imaging techniques and genetic tools allow researchers to track the activity of thousands of neurons in living animals over many months. However, these experiments produce large volumes of data that researchers currently have to analyze manually, which can take a long time and generate irreproducible results.             There is a need to develop new computational tools to analyze such data. The new tools should be able to operate on standard computers rather than just specialist equipment as this would limit the use of the solutions to particularly well-funded research teams. Ideally, the tools should also be able to operate in real-time as several experimental and therapeutic scenarios, like the control of robotic limbs, require this. To address this need, Giovannucci et al. developed a new software package called CaImAn to analyze brain images on a large scale.             Firstly, the team developed algorithms that are suitable to analyze large sets of data on laptops and other standard computing equipment. These algorithms were then adapted to operate online in real-time. To test how well the new software performs against manual analysis by human researchers, Giovannucci et al. asked several trained human annotators to identify active neurons that were round or donut-shaped in several sets of imaging data from mouse brains. Each set of data was independently analyzed by three or four researchers who then discussed any neurons they disagreed on to generate a ‘consensus annotation’. Giovannucci et al. then used CaImAn to analyze the same sets of data and compared the results to the consensus annotations. This demonstrated that CaImAn is nearly as good as human researchers at identifying active neurons in brain images.             CaImAn provides a quicker method to analyze large sets of brain imaging data and is currently used by over a hundred laboratories across the world. The software is open source, meaning that it is freely-available and that users are encouraged to customize it and collaborate with other users to develop it further.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/K9866SNT/Giovannucci et al. - 2019 - CaImAn an open source tool for scalable calcium imaging data analysis.pdf}
}

@inproceedings{godfreySWITCHBOARDTelephoneSpeech1992,
  title = {{{SWITCHBOARD}}: Telephone Speech Corpus for Research and Development},
  shorttitle = {{{SWITCHBOARD}}},
  booktitle = {[{{Proceedings}}] {{ICASSP-92}}: 1992 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Godfrey, J.J. and Holliman, E.C. and McDaniel, J.},
  date = {1992},
  pages = {517-520 vol.1},
  publisher = {IEEE},
  location = {San Francisco, CA, USA},
  doi = {10.1109/ICASSP.1992.225858},
  url = {http://ieeexplore.ieee.org/document/225858/},
  urldate = {2023-02-14},
  eventtitle = {[{{Proceedings}}] {{ICASSP-92}}: 1992 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  isbn = {978-0-7803-0532-8},
  file = {/home/cristi/Zotero/storage/F76A9LY3/Godfrey et al. - 1992 - SWITCHBOARD telephone speech corpus for research .pdf}
}

@inproceedings{godfreySWITCHBOARDTelephoneSpeech1992a,
  title = {{{SWITCHBOARD}}: {{Telephone Speech Corpus}} for {{Research}} and {{Development}}},
  shorttitle = {{{SWITCHBOARD}}},
  booktitle = {[{{Proceedings}}] {{ICASSP-92}}: 1992 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Godfrey, J.J. and Holliman, E.C. and McDaniel, J.},
  date = {1992},
  pages = {517-520 vol.1},
  publisher = {IEEE},
  location = {San Francisco, CA, USA},
  doi = {10.1109/ICASSP.1992.225858},
  url = {http://ieeexplore.ieee.org/document/225858/},
  urldate = {2023-02-14},
  eventtitle = {[{{Proceedings}}] {{ICASSP-92}}: 1992 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  isbn = {978-0-7803-0532-8},
  file = {/home/cristi/Zotero/storage/LWSRJDRL/Godfrey et al. - 1992 - SWITCHBOARD telephone speech corpus for research .pdf}
}

@article{goeringRecommendationsResponsibleDevelopment2021,
  title = {Recommendations for {{Responsible Development}} and {{Application}} of {{Neurotechnologies}}},
  author = {Goering, Sara and Klein, Eran and Specker Sullivan, Laura and Wexler, Anna and Agüera Y Arcas, Blaise and Bi, Guoqiang and Carmena, Jose M. and Fins, Joseph J. and Friesen, Phoebe and Gallant, Jack and Huggins, Jane E. and Kellmeyer, Philipp and Marblestone, Adam and Mitchell, Christine and Parens, Erik and Pham, Michelle and Rubel, Alan and Sadato, Norihiro and Teicher, Mina and Wasserman, David and Whittaker, Meredith and Wolpaw, Jonathan and Yuste, Rafael},
  date = {2021-12},
  journaltitle = {Neuroethics},
  shortjournal = {Neuroethics},
  volume = {14},
  number = {3},
  pages = {365--386},
  issn = {1874-5490, 1874-5504},
  doi = {10.1007/s12152-021-09468-6},
  url = {https://link.springer.com/10.1007/s12152-021-09468-6},
  urldate = {2024-01-07},
  langid = {english},
  file = {/home/cristi/Zotero/storage/CFGRHFLT/Goering et al. - 2021 - Recommendations for Responsible Development and Ap.pdf}
}

@article{goeringRecommendationsResponsibleDevelopment2021a,
  title = {Recommendations for {{Responsible Development}} and {{Application}} of {{Neurotechnologies}}},
  author = {Goering, Sara and Klein, Eran and Specker Sullivan, Laura and Wexler, Anna and Agüera Y Arcas, Blaise and Bi, Guoqiang and Carmena, Jose M. and Fins, Joseph J. and Friesen, Phoebe and Gallant, Jack and Huggins, Jane E. and Kellmeyer, Philipp and Marblestone, Adam and Mitchell, Christine and Parens, Erik and Pham, Michelle and Rubel, Alan and Sadato, Norihiro and Teicher, Mina and Wasserman, David and Whittaker, Meredith and Wolpaw, Jonathan and Yuste, Rafael},
  date = {2021-12},
  journaltitle = {Neuroethics},
  shortjournal = {Neuroethics},
  volume = {14},
  number = {3},
  pages = {365--386},
  issn = {1874-5490, 1874-5504},
  doi = {10.1007/s12152-021-09468-6},
  url = {https://link.springer.com/10.1007/s12152-021-09468-6},
  urldate = {2024-01-07},
  file = {/home/cristi/Zotero/storage/KD26HXTR/Goering et al. - 2021 - Recommendations for Responsible Development and Ap.pdf}
}

@article{grienbergerImagingCalciumNeurons2012,
  title = {Imaging {{Calcium}} in {{Neurons}}},
  author = {Grienberger, Christine and Konnerth, Arthur},
  date = {2012-03},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {73},
  number = {5},
  pages = {862--885},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.02.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627312001729},
  urldate = {2025-02-17},
  langid = {english},
  file = {/home/cristi/Zotero/storage/9U59NCXZ/Grienberger and Konnerth - 2012 - Imaging Calcium in Neurons.pdf}
}

@article{grienbergerImagingCalciumNeurons2012a,
  title = {Imaging {{Calcium}} in {{Neurons}}},
  author = {Grienberger, Christine and Konnerth, Arthur},
  date = {2012-03},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {73},
  number = {5},
  pages = {862--885},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.02.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627312001729},
  urldate = {2025-02-17},
  file = {/home/cristi/Zotero/storage/GY4UZ4WV/Grienberger and Konnerth - 2012 - Imaging Calcium in Neurons.pdf}
}

@article{gucluDeepNeuralNetworks2015,
  title = {Deep {{Neural Networks Reveal}} a {{Gradient}} in the {{Complexity}} of {{Neural Representations}} across the {{Ventral Stream}}},
  author = {Guclu, U. and Van Gerven, M. A. J.},
  date = {2015-07-08},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {Journal of Neuroscience},
  volume = {35},
  number = {27},
  pages = {10005--10014},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5023-14.2015},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.5023-14.2015},
  urldate = {2024-04-25},
  langid = {english},
  file = {/home/cristi/Zotero/storage/67YI52K8/Guclu and Van Gerven - 2015 - Deep Neural Networks Reveal a Gradient in the Comp.pdf}
}

@article{gucluDeepNeuralNetworks2015a,
  title = {Deep {{Neural Networks Reveal}} a {{Gradient}} in the {{Complexity}} of {{Neural Representations}} across the {{Ventral Stream}}},
  author = {Guclu, U. and Van Gerven, M. A. J.},
  date = {2015-07-08},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {Journal of Neuroscience},
  volume = {35},
  number = {27},
  pages = {10005--10014},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5023-14.2015},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.5023-14.2015},
  urldate = {2024-06-16},
  langid = {english},
  file = {/home/cristi/Zotero/storage/GZXCQMP6/Guclu and Van Gerven - 2015 - Deep Neural Networks Reveal a Gradient in the Comp.pdf}
}

@article{gucluDeepNeuralNetworks2015b,
  title = {Deep {{Neural Networks Reveal}} a {{Gradient}} in the {{Complexity}} of {{Neural Representations}} across the {{Ventral Stream}}},
  author = {Guclu, U. and Van Gerven, M. A. J.},
  date = {2015-07-08},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {Journal of Neuroscience},
  volume = {35},
  number = {27},
  pages = {10005--10014},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5023-14.2015},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.5023-14.2015},
  urldate = {2024-06-16},
  file = {/home/cristi/Zotero/storage/KYAFLSFY/Guclu and Van Gerven - 2015 - Deep Neural Networks Reveal a Gradient in the Comp.pdf}
}

@article{gucluDeepNeuralNetworks2015c,
  title = {Deep {{Neural Networks Reveal}} a {{Gradient}} in the {{Complexity}} of {{Neural Representations}} across the {{Ventral Stream}}},
  author = {Guclu, U. and Van Gerven, M. A. J.},
  date = {2015-07-08},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {Journal of Neuroscience},
  volume = {35},
  number = {27},
  pages = {10005--10014},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5023-14.2015},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.5023-14.2015},
  urldate = {2024-04-25},
  file = {/home/cristi/Zotero/storage/TLTIUYN7/Guclu and Van Gerven - 2015 - Deep Neural Networks Reveal a Gradient in the Comp.pdf}
}

@article{gursesEngineeringPrivacyDesign,
  title = {Engineering {{Privacy}} by {{Design}}},
  author = {Gurses, Seda and Troncoso, Carmela and Diaz, Claudia},
  abstract = {The design and implementation of privacy requirements in systems is a difficult problem and requires the translation of complex social, legal and ethical concerns into systems requirements. The concept of “privacy by design” has been proposed to serve as a guideline on how to address these concerns.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/8SDE9GUN/Gurses et al. - Engineering Privacy by Design.pdf}
}

@article{gursesEngineeringPrivacyDesign2011,
  title = {Engineering Privacy by Design},
  author = {Gürses, Seda and Troncoso, Carmela and Diaz, Claudia},
  date = {2011},
  journaltitle = {Computers, Privacy \& Data Protection},
  volume = {14},
  number = {3},
  pages = {25},
  url = {https://software.imdea.org/~carmela.troncoso/papers/Gurses-CPDP11.pdf},
  urldate = {2025-03-28},
  file = {/home/cristi/Zotero/storage/BMFBUT77/Gürses et al. - 2011 - Engineering privacy by design.pdf}
}

@article{gursesEngineeringPrivacyDesigna,
  title = {Engineering {{Privacy}} by {{Design}}},
  author = {Gurses, Seda and Troncoso, Carmela and Diaz, Claudia},
  abstract = {The design and implementation of privacy requirements in systems is a difficult problem and requires the translation of complex social, legal and ethical concerns into systems requirements. The concept of “privacy by design” has been proposed to serve as a guideline on how to address these concerns.},
  file = {/home/cristi/Zotero/storage/ZD46VIEF/Gurses et al. - Engineering Privacy by Design.pdf}
}

@article{haggardHumanVolitionNeuroscience2008,
  title = {Human Volition: Towards a Neuroscience of Will},
  shorttitle = {Human Volition},
  author = {Haggard, Patrick},
  date = {2008-12},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {9},
  number = {12},
  pages = {934--946},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn2497},
  url = {https://www.nature.com/articles/nrn2497},
  urldate = {2024-06-22},
  langid = {english}
}

@article{haggardHumanVolitionNeuroscience2008a,
  title = {Human Volition: Towards a Neuroscience of Will},
  shorttitle = {Human Volition},
  author = {Haggard, Patrick},
  date = {2008-12},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {9},
  number = {12},
  pages = {934--946},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn2497},
  url = {https://www.nature.com/articles/nrn2497},
  urldate = {2024-06-22},
  langid = {english},
  file = {/home/cristi/Zotero/storage/C6RRIYTX/Haggard - 2008 - Human volition towards a neuroscience of will.pdf}
}

@article{haggardHumanVolitionNeuroscience2008b,
  title = {Human {{Volition}}: {{Towards}} a {{Neuroscience}} of {{Will}}},
  shorttitle = {Human {{Volition}}},
  author = {Haggard, Patrick},
  date = {2008-12},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {9},
  number = {12},
  pages = {934--946},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn2497},
  url = {https://www.nature.com/articles/nrn2497},
  urldate = {2024-06-22},
  file = {/home/cristi/Zotero/storage/PBTWPRPQ/Haggard - 2008 - Human volition towards a neuroscience of will.pdf}
}

@article{haggardHumanVolitionNeuroscience2008c,
  title = {Human {{Volition}}: {{Towards}} a {{Neuroscience}} of {{Will}}},
  shorttitle = {Human {{Volition}}},
  author = {Haggard, Patrick},
  date = {2008-12},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {9},
  number = {12},
  pages = {934--946},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn2497},
  url = {https://www.nature.com/articles/nrn2497},
  urldate = {2024-06-22}
}

@online{haghiriComparisonBasedFrameworkPsychophysics2019,
  title = {Comparison-{{Based Framework}} for {{Psychophysics}}: {{Lab}} versus {{Crowdsourcing}}},
  shorttitle = {Comparison-{{Based Framework}} for {{Psychophysics}}},
  author = {Haghiri, Siavash and Rubisch, Patricia and Geirhos, Robert and Wichmann, Felix and family=Luxburg, given=Ulrike, prefix=von, useprefix=true},
  date = {2019-07-26},
  eprint = {1905.07234},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.07234},
  urldate = {2024-04-25},
  abstract = {Traditionally, psychophysical experiments are conducted by repeated measurements on a few well-trained participants under well-controlled conditions, often resulting in, if done properly, high quality data. In recent years, however, crowdsourcing platforms are becoming increasingly popular means of data collection, measuring many participants at the potential cost of obtaining data of worse quality. In this paper we study whether the use of comparison-based (ordinal) data, combined with machine learning algorithms, can boost the reliability of crowdsourcing studies for psychophysics, such that they can achieve performance close to a lab experiment. To this end, we compare three setups: simulations, a psychophysics lab experiment, and the same experiment on Amazon Mechanical Turk. All these experiments are conducted in a comparison-based setting where participants have to answer triplet questions of the form "is object x closer to y or to z?". We then use machine learning to solve the triplet prediction problem: given a subset of triplet questions with corresponding answers, we predict the answer to the remaining questions. Considering the limitations and noise on MTurk, we find that the accuracy of triplet prediction is surprisingly close---but not equal---to our lab study.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/cristi/Zotero/storage/3I3HCW4J/Haghiri et al. - 2019 - Comparison-Based Framework for Psychophysics Lab .pdf;/home/cristi/Zotero/storage/5QJDGVUC/1905.html}
}

@online{haghiriComparisonBasedFrameworkPsychophysics2019a,
  title = {Comparison-{{Based Framework}} for {{Psychophysics}}: {{Lab}} versus {{Crowdsourcing}}},
  shorttitle = {Comparison-{{Based Framework}} for {{Psychophysics}}},
  author = {Haghiri, Siavash and Rubisch, Patricia and Geirhos, Robert and Wichmann, Felix and {family=Luxburg}, given=Ulrike, prefix=von, useprefix=true},
  date = {2019-07-26},
  eprint = {1905.07234},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1905.07234},
  urldate = {2024-04-25},
  abstract = {Traditionally, psychophysical experiments are conducted by repeated measurements on a few well-trained participants under well-controlled conditions, often resulting in, if done properly, high quality data. In recent years, however, crowdsourcing platforms are becoming increasingly popular means of data collection, measuring many participants at the potential cost of obtaining data of worse quality. In this paper we study whether the use of comparison-based (ordinal) data, combined with machine learning algorithms, can boost the reliability of crowdsourcing studies for psychophysics, such that they can achieve performance close to a lab experiment. To this end, we compare three setups: simulations, a psychophysics lab experiment, and the same experiment on Amazon Mechanical Turk. All these experiments are conducted in a comparison-based setting where participants have to answer triplet questions of the form "is object x closer to y or to z?". We then use machine learning to solve the triplet prediction problem: given a subset of triplet questions with corresponding answers, we predict the answer to the remaining questions. Considering the limitations and noise on MTurk, we find that the accuracy of triplet prediction is surprisingly close—but not equal—to our lab study.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/cristi/Zotero/storage/DEXPPTN9/Haghiri et al. - 2019 - Comparison-Based Framework for Psychophysics Lab .pdf;/home/cristi/Zotero/storage/ZDXD72QN/1905.html}
}

@dataset{henryHerculaneumPapyri2013,
  title = {Herculaneum {{Papyri}}},
  author = {Henry, W. Benjamin},
  date = {2013-07-24},
  pages = {9780195389661--0170},
  publisher = {Oxford University Press},
  doi = {10.1093/obo/9780195389661-0170},
  url = {https://oxfordbibliographies.com/view/document/obo-9780195389661/obo-9780195389661-0170.xml},
  urldate = {2023-10-30},
  langid = {english}
}

@article{hintonDeepNeuralNetworks2012,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}: {{The Shared Views}} of {{Four Research Groups}}},
  shorttitle = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
  date = {2012-11},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {29},
  number = {6},
  pages = {82--97},
  issn = {1053-5888},
  doi = {10.1109/MSP.2012.2205597},
  url = {http://ieeexplore.ieee.org/document/6296526/},
  urldate = {2023-02-15},
  file = {/home/cristi/Zotero/storage/4MYF9NHY/hinton2012.pdf}
}

@article{hintonDeepNeuralNetworks2012a,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}: {{The Shared Views}} of {{Four Research Groups}}},
  shorttitle = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
  date = {2012-11},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {29},
  number = {6},
  pages = {82--97},
  issn = {1053-5888},
  doi = {10.1109/MSP.2012.2205597},
  url = {http://ieeexplore.ieee.org/document/6296526/},
  urldate = {2023-02-15},
  file = {/home/cristi/Zotero/storage/BNI9SZN5/hinton2012.pdf}
}

@book{hoggCambridgeHistoryEnglish1992,
  title = {The {{Cambridge}} History of the {{English}} Language},
  editor = {Hogg, Richard M. and Blake, N. F. and Lass, Roger and Romaine, Suzanne and Burchfield, R. W. and Algeo, John},
  date = {1992},
  publisher = {Cambridge University Press},
  location = {Cambridge ; New York, NY, USA},
  isbn = {978-0-521-26474-7 978-0-521-26475-4 978-0-521-26476-1 978-0-521-26477-8 978-0-521-26478-5 978-0-521-26479-2},
  pagetotal = {6},
  keywords = {English language,History}
}

@book{hoggCambridgeHistoryEnglish1992a,
  title = {The {{Cambridge History}} of the {{English Language}}},
  editor = {Hogg, Richard M. and Blake, N. F. and Lass, Roger and Romaine, Suzanne and Burchfield, R. W. and Algeo, John},
  date = {1992},
  publisher = {Cambridge University Press},
  location = {Cambridge ; New York, NY, USA},
  isbn = {978-0-521-26474-7 978-0-521-26475-4 978-0-521-26476-1 978-0-521-26477-8 978-0-521-26478-5 978-0-521-26479-2},
  pagetotal = {6},
  keywords = {English language,History}
}

@article{hongStudyPerceptionUse2022,
  title = {A {{Study}} on the {{Perception}} and {{Use}} of {{Korean Language Learners}} '-아/어 주다' {{Expression}}},
  author = {Hong, Seung-ah},
  date = {2022-12-31},
  journaltitle = {Hanminjok Emunhak},
  shortjournal = {HEM},
  volume = {98},
  pages = {69--102},
  issn = {12290742, 27339513},
  doi = {10.31821/HEM.98.3},
  url = {https://kiss.kstudy.com/thesis/thesis-view.asp?key=3995103},
  urldate = {2023-03-03}
}

@article{hongStudyPerceptionUse2022a,
  title = {A {{Study}} on the {{Perception}} and {{Use}} of {{Korean Language Learners}} '-아/어 주다' {{Expression}}},
  author = {Hong, Seung-ah},
  date = {2022-12-31},
  journaltitle = {Hanminjok Emunhak},
  shortjournal = {HEM},
  volume = {98},
  pages = {69--102},
  issn = {12290742, 27339513},
  doi = {10.31821/HEM.98.3},
  url = {https://kiss.kstudy.com/thesis/thesis-view.asp?key=3995103},
  urldate = {2023-03-03}
}

@online{huangCanLargeLanguage2023,
  title = {Can {{Large Language Models Explain Themselves}}? {{A Study}} of {{LLM-Generated Self-Explanations}}},
  shorttitle = {Can {{Large Language Models Explain Themselves}}?},
  author = {Huang, Shiyuan and Mamidanna, Siddarth and Jangam, Shreedhar and Zhou, Yilun and Gilpin, Leilani H.},
  date = {2023},
  doi = {10.48550/ARXIV.2310.11207},
  url = {https://arxiv.org/abs/2310.11207},
  urldate = {2025-04-06},
  abstract = {Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce "helpful" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as "fantastic" and "memorable" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/home/cristi/Zotero/storage/DWVBD4R2/Huang et al. - 2023 - Can Large Language Models Explain Themselves A Study of LLM-Generated Self-Explanations.pdf}
}

@inproceedings{huangComparativeAnalyticStudy2014,
  title = {A Comparative Analytic Study on the {{Gaussian}} Mixture and Context Dependent Deep Neural Network Hidden {{Markov}} Models},
  booktitle = {Interspeech 2014},
  author = {Huang, Yan and Yu, Dong and Liu, Chaojun and Gong, Yifan},
  date = {2014-09-14},
  pages = {1895--1899},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2014-429},
  url = {https://www.isca-speech.org/archive/interspeech_2014/huang14d_interspeech.html},
  urldate = {2023-02-15},
  eventtitle = {Interspeech 2014},
  langid = {english}
}

@inproceedings{huangComparativeAnalyticStudy2014a,
  title = {A {{Comparative Analytic Study}} on the {{Gaussian Mixture}} and {{Context Dependent Deep Neural Network Hidden Markov Models}}},
  booktitle = {Interspeech 2014},
  author = {Huang, Yan and Yu, Dong and Liu, Chaojun and Gong, Yifan},
  date = {2014-09-14},
  pages = {1895--1899},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2014-429},
  url = {https://www.isca-speech.org/archive/interspeech_2014/huang14d_interspeech.html},
  urldate = {2023-02-15},
  eventtitle = {Interspeech 2014}
}

@article{hulImpactMusicConsumers1997,
  title = {The Impact of Music on Consumers' Reactions to Waiting for Services},
  author = {Hul, Michael K. and Dube, Laurette and Chebat, Jean-Charles},
  date = {1997-03},
  journaltitle = {Journal of Retailing},
  shortjournal = {Journal of Retailing},
  volume = {73},
  number = {1},
  pages = {87--104},
  issn = {00224359},
  doi = {10.1016/S0022-4359(97)90016-6},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0022435997900166},
  urldate = {2023-02-08},
  langid = {english},
  file = {/home/cristi/Zotero/storage/QGA9TCNN/Hul et al. - 1997 - The impact of music on consumers' reactions to wai.pdf}
}

@article{hulImpactMusicConsumers1997a,
  title = {The {{Impact}} of {{Music}} on {{Consumers}}' {{Reactions}} to {{Waiting}} for {{Services}}},
  author = {Hul, Michael K. and Dube, Laurette and Chebat, Jean-Charles},
  date = {1997-03},
  journaltitle = {Journal of Retailing},
  shortjournal = {Journal of Retailing},
  volume = {73},
  number = {1},
  pages = {87--104},
  issn = {00224359},
  doi = {10.1016/S0022-4359(97)90016-6},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0022435997900166},
  urldate = {2023-02-08},
  file = {/home/cristi/Zotero/storage/TAC4GBRE/Hul et al. - 1997 - The impact of music on consumers' reactions to wai.pdf}
}

@online{InteractiveAgentsRadboud,
  title = {Interactive {{Agents}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-intelligent-technology/interactive-agents},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/BF7PCSM5/interactive-agents.html}
}

@online{InteractiveAgentsRadbouda,
  title = {Interactive {{Agents}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-intelligent-technology/interactive-agents},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/FHXW3KDT/interactive-agents.html}
}

@article{islamSystematicReviewExplainable2022,
  title = {A {{Systematic Review}} of {{Explainable Artificial Intelligence}} in {{Terms}} of {{Different Application Domains}} and {{Tasks}}},
  author = {Islam, Mir Riyanul and Ahmed, Mobyen Uddin and Barua, Shaibal and Begum, Shahina},
  date = {2022-01-27},
  journaltitle = {Applied Sciences},
  shortjournal = {Applied Sciences},
  volume = {12},
  number = {3},
  pages = {1353},
  issn = {2076-3417},
  doi = {10.3390/app12031353},
  url = {https://www.mdpi.com/2076-3417/12/3/1353},
  urldate = {2025-01-31},
  abstract = {Artificial intelligence (AI) and machine learning (ML) have recently been radically improved and are now being employed in almost every application domain to develop automated or semi-automated systems. To facilitate greater human acceptability of these systems, explainable artificial intelligence (XAI) has experienced significant growth over the last couple of years with the development of highly accurate models but with a paucity of explainability and interpretability. The literature shows evidence from numerous studies on the philosophy and methodologies of XAI. Nonetheless, there is an evident scarcity of secondary studies in connection with the application domains and tasks, let alone review studies following prescribed guidelines, that can enable researchers’ understanding of the current trends in XAI, which could lead to future research for domain- and application-specific method development. Therefore, this paper presents a systematic literature review (SLR) on the recent developments of XAI methods and evaluation metrics concerning different application domains and tasks. This study considers 137 articles published in recent years and identified through the prominent bibliographic databases. This systematic synthesis of research articles resulted in several analytical findings: XAI methods are mostly developed for safety-critical domains worldwide, deep learning and ensemble models are being exploited more than other types of AI/ML models, visual explanations are more acceptable to end-users and robust evaluation metrics are being developed to assess the quality of explanations. Research studies have been performed on the addition of explanations to widely used AI/ML models for expert users. However, more attention is required to generate explanations for general users from sensitive domains such as finance and the judicial system.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/5YFJM336/Islam et al. - 2022 - A Systematic Review of Explainable Artificial Intelligence in Terms of Different Application Domains.pdf}
}

@article{islamSystematicReviewExplainable2022a,
  title = {A {{Systematic Review}} of {{Explainable Artificial Intelligence}} in {{Terms}} of {{Different Application Domains}} and {{Tasks}}},
  author = {Islam, Mir Riyanul and Ahmed, Mobyen Uddin and Barua, Shaibal and Begum, Shahina},
  date = {2022-01-27},
  journaltitle = {Applied Sciences},
  shortjournal = {Applied Sciences},
  volume = {12},
  number = {3},
  pages = {1353},
  issn = {2076-3417},
  doi = {10.3390/app12031353},
  url = {https://www.mdpi.com/2076-3417/12/3/1353},
  urldate = {2025-01-31},
  abstract = {Artificial intelligence (AI) and machine learning (ML) have recently been radically improved and are now being employed in almost every application domain to develop automated or semi-automated systems. To facilitate greater human acceptability of these systems, explainable artificial intelligence (XAI) has experienced significant growth over the last couple of years with the development of highly accurate models but with a paucity of explainability and interpretability. The literature shows evidence from numerous studies on the philosophy and methodologies of XAI. Nonetheless, there is an evident scarcity of secondary studies in connection with the application domains and tasks, let alone review studies following prescribed guidelines, that can enable researchers’ understanding of the current trends in XAI, which could lead to future research for domain- and application-specific method development. Therefore, this paper presents a systematic literature review (SLR) on the recent developments of XAI methods and evaluation metrics concerning different application domains and tasks. This study considers 137 articles published in recent years and identified through the prominent bibliographic databases. This systematic synthesis of research articles resulted in several analytical findings: XAI methods are mostly developed for safety-critical domains worldwide, deep learning and ensemble models are being exploited more than other types of AI/ML models, visual explanations are more acceptable to end-users and robust evaluation metrics are being developed to assess the quality of explanations. Research studies have been performed on the addition of explanations to widely used AI/ML models for expert users. However, more attention is required to generate explanations for general users from sensitive domains such as finance and the judicial system.},
  file = {/home/cristi/Zotero/storage/HW5BQIAP/Islam et al. - 2022 - A Systematic Review of Explainable Artificial Intelligence in Terms of Different Application Domains.pdf}
}

@article{issaMultiscaleOpticalCa22014,
  title = {Multiscale {{Optical Ca2}}+ {{Imaging}} of {{Tonal Organization}} in {{Mouse Auditory Cortex}}},
  author = {Issa, John~B. and Haeffele, Benjamin~D. and Agarwal, Amit and Bergles, Dwight~E. and Young, Eric~D. and Yue, David~T.},
  date = {2014-08},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {83},
  number = {4},
  pages = {944--959},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.07.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S089662731400590X},
  urldate = {2025-02-28},
  langid = {english},
  file = {/home/cristi/Zotero/storage/4ASTQFG8/Issa et al. - 2014 - Multiscale Optical Ca2+ Imaging of Tonal Organization in Mouse Auditory Cortex.pdf}
}

@article{issaMultiscaleOpticalCa22014a,
  title = {Multiscale {{Optical Ca2}}+ {{Imaging}} of {{Tonal Organization}} in {{Mouse Auditory Cortex}}},
  author = {Issa, John B. and Haeffele, Benjamin D. and Agarwal, Amit and Bergles, Dwight E. and Young, Eric D. and Yue, David T.},
  date = {2014-08},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {83},
  number = {4},
  pages = {944--959},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.07.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S089662731400590X},
  urldate = {2025-02-28},
  file = {/home/cristi/Zotero/storage/BQRIKUVG/Issa et al. - 2014 - Multiscale Optical Ca2+ Imaging of Tonal Organization in Mouse Auditory Cortex.pdf}
}

@article{izhikevichSimpleModelSpiking2003,
  title = {Simple Model of Spiking Neurons},
  author = {Izhikevich, E.M.},
  date = {2003-11},
  journaltitle = {IEEE Transactions on Neural Networks},
  shortjournal = {IEEE Trans. Neural Netw.},
  volume = {14},
  number = {6},
  pages = {1569--1572},
  issn = {1045-9227},
  doi = {10.1109/TNN.2003.820440},
  url = {http://ieeexplore.ieee.org/document/1257420/},
  urldate = {2024-04-23},
  langid = {english},
  file = {/home/cristi/Zotero/storage/SFZBZDT6/Izhikevich - 2003 - Simple model of spiking neurons.pdf}
}

@article{izhikevichSimpleModelSpiking2003a,
  title = {Simple {{Model}} of {{Spiking Neurons}}},
  author = {Izhikevich, E.M.},
  date = {2003-11},
  journaltitle = {IEEE Transactions on Neural Networks},
  shortjournal = {IEEE Trans. Neural Netw.},
  volume = {14},
  number = {6},
  pages = {1569--1572},
  issn = {1045-9227},
  doi = {10.1109/TNN.2003.820440},
  url = {http://ieeexplore.ieee.org/document/1257420/},
  urldate = {2024-04-23},
  file = {/home/cristi/Zotero/storage/QAFZKNAI/Izhikevich - 2003 - Simple model of spiking neurons.pdf}
}

@article{jangImprovedModelingHuman2024,
  title = {Improved Modeling of Human Vision by Incorporating Robustness to Blur in Convolutional Neural Networks},
  author = {Jang, Hojin and Tong, Frank},
  date = {2024-03-05},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {15},
  number = {1},
  pages = {1989},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-45679-0},
  url = {https://www.nature.com/articles/s41467-024-45679-0},
  urldate = {2024-04-21},
  abstract = {Abstract             Whenever a visual scene is cast onto the retina, much of it will appear degraded due to poor resolution in the periphery; moreover, optical defocus can cause blur in central vision. However, the pervasiveness of blurry or degraded input is typically overlooked in the training of convolutional neural networks (CNNs). We hypothesized that the absence of blurry training inputs may cause CNNs to rely excessively on high spatial frequency information for object recognition, thereby causing systematic deviations from biological vision. We evaluated this hypothesis by comparing standard CNNs with CNNs trained on a combination of clear and blurry images. We show that blur-trained CNNs outperform standard CNNs at predicting neural responses to objects across a variety of viewing conditions. Moreover, blur-trained CNNs acquire increased sensitivity to shape information and greater robustness to multiple forms of visual noise, leading to improved correspondence with human perception. Our results provide multi-faceted neurocomputational evidence that blurry visual experiences may be critical for conferring robustness to biological visual systems.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/HXZQXE5Y/Jang and Tong - 2024 - Improved modeling of human vision by incorporating.pdf}
}

@article{jangImprovedModelingHuman2024a,
  title = {Improved {{Modeling}} of {{Human Vision}} by {{Incorporating Robustness}} to {{Blur}} in {{Convolutional Neural Networks}}},
  author = {Jang, Hojin and Tong, Frank},
  date = {2024-03-05},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {15},
  number = {1},
  pages = {1989},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-45679-0},
  url = {https://www.nature.com/articles/s41467-024-45679-0},
  urldate = {2024-04-21},
  abstract = {Abstract Whenever a visual scene is cast onto the retina, much of it will appear degraded due to poor resolution in the periphery; moreover, optical defocus can cause blur in central vision. However, the pervasiveness of blurry or degraded input is typically overlooked in the training of convolutional neural networks (CNNs). We hypothesized that the absence of blurry training inputs may cause CNNs to rely excessively on high spatial frequency information for object recognition, thereby causing systematic deviations from biological vision. We evaluated this hypothesis by comparing standard CNNs with CNNs trained on a combination of clear and blurry images. We show that blur-trained CNNs outperform standard CNNs at predicting neural responses to objects across a variety of viewing conditions. Moreover, blur-trained CNNs acquire increased sensitivity to shape information and greater robustness to multiple forms of visual noise, leading to improved correspondence with human perception. Our results provide multi-faceted neurocomputational evidence that blurry visual experiences may be critical for conferring robustness to biological visual systems.},
  file = {/home/cristi/Zotero/storage/AE9QLTXF/Jang and Tong - 2024 - Improved modeling of human vision by incorporating.pdf}
}

@article{jazayeriNewPerceptualIllusion2007,
  title = {A New Perceptual Illusion Reveals Mechanisms of Sensory Decoding},
  author = {Jazayeri, Mehrdad and Movshon, J. Anthony},
  date = {2007-04},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {446},
  number = {7138},
  pages = {912--915},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature05739},
  url = {https://www.nature.com/articles/nature05739},
  urldate = {2024-05-26},
  langid = {english},
  file = {/home/cristi/Zotero/storage/CQSFQ6AD/Jazayeri and Movshon - 2007 - A new perceptual illusion reveals mechanisms of se.pdf}
}

@article{jazayeriNewPerceptualIllusion2007a,
  title = {A {{New Perceptual Illusion Reveals Mechanisms}} of {{Sensory Decoding}}},
  author = {Jazayeri, Mehrdad and Movshon, J. Anthony},
  date = {2007-04},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {446},
  number = {7138},
  pages = {912--915},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature05739},
  url = {https://www.nature.com/articles/nature05739},
  urldate = {2024-05-26},
  file = {/home/cristi/Zotero/storage/86RVMDRW/Jazayeri and Movshon - 2007 - A new perceptual illusion reveals mechanisms of se.pdf}
}

@article{jobinGlobalLandscapeAI2019,
  title = {The Global Landscape of {{AI}} Ethics Guidelines},
  author = {Jobin, Anna and Ienca, Marcello and Vayena, Effy},
  date = {2019-09-02},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {1},
  number = {9},
  pages = {389--399},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0088-2},
  url = {https://www.nature.com/articles/s42256-019-0088-2},
  urldate = {2025-03-27},
  langid = {english},
  file = {/home/cristi/Zotero/storage/ZU7BIBPZ/Jobin et al. - 2019 - The global landscape of AI ethics guidelines.pdf}
}

@article{jobinGlobalLandscapeAI2019a,
  title = {The Global Landscape of {{AI}} Ethics Guidelines},
  author = {Jobin, Anna and Ienca, Marcello and Vayena, Effy},
  date = {2019-09-02},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {1},
  number = {9},
  pages = {389--399},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0088-2},
  url = {https://www.nature.com/articles/s42256-019-0088-2},
  urldate = {2025-03-27},
  langid = {english}
}

@article{johnsonStoryPapyriVilla2006,
  title = {The Story of the Papyri of the {{Villa}} Dei {{Papiri}} - {{DAVID SIDER}}, {{THE LIBRARY OF THE VILLA DEI PAPIRI AT HERCULANEUM}} ({{The J}}. {{Paul Getty Museum}}, {{Los Angeles2005}}). {{Pp}}. Viii + 123, Figs. 80 Including 42 in Colour. {{ISBN}} 0-89236- 799-7. \$40.},
  author = {Johnson, William A.},
  date = {2006},
  journaltitle = {Journal of Roman Archaeology},
  shortjournal = {J. Roman archaeol.},
  volume = {19},
  pages = {493--496},
  issn = {1047-7594, 2331-5709},
  doi = {10.1017/S1047759400006693},
  url = {https://www.cambridge.org/core/product/identifier/S1047759400006693/type/journal_article},
  urldate = {2023-10-30},
  langid = {english},
  file = {/home/cristi/Zotero/storage/XP6H4ZTB/Johnson - 2006 - The story of the papyri of the Villa dei Papiri - .pdf}
}

@article{johnsonStoryPapyriVilla2006a,
  title = {The {{Story}} of the {{Papyri}} of the {{Villa Dei Papiri}} - {{DAVID SIDER}}, {{THE LIBRARY OF THE VILLA DEI PAPIRI AT HERCULANEUM}} ({{The J}}. {{Paul Getty Museum}}, {{Los Angeles2005}}). {{Pp}}. {{Viii}} + 123, {{Figs}}. 80 {{Including}} 42 in {{Colour}}. {{ISBN}} 0-89236- 799-7. \$40.},
  author = {Johnson, William A.},
  date = {2006},
  journaltitle = {Journal of Roman Archaeology},
  shortjournal = {J. Roman archaeol.},
  volume = {19},
  pages = {493--496},
  issn = {1047-7594, 2331-5709},
  doi = {10.1017/S1047759400006693},
  url = {https://www.cambridge.org/core/product/identifier/S1047759400006693/type/journal_article},
  urldate = {2023-10-30},
  file = {/home/cristi/Zotero/storage/RFF42YEM/Johnson - 2006 - The story of the papyri of the Villa dei Papiri - .pdf}
}

@inproceedings{joshiStateFateLinguistic2020,
  title = {The {{State}} and {{Fate}} of {{Linguistic Diversity}} and {{Inclusion}} in the {{NLP World}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Joshi, Pratik and Santy, Sebastin and Budhiraja, Amar and Bali, Kalika and Choudhury, Monojit},
  date = {2020},
  pages = {6282--6293},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.acl-main.560},
  url = {https://www.aclweb.org/anthology/2020.acl-main.560},
  urldate = {2023-02-12},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  file = {/home/cristi/Zotero/storage/LJSZTDYM/Joshi et al. - 2020 - The State and Fate of Linguistic Diversity and Inc.pdf}
}

@inproceedings{joshiStateFateLinguistic2020a,
  title = {The {{State}} and {{Fate}} of {{Linguistic Diversity}} and {{Inclusion}} in the {{NLP World}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Joshi, Pratik and Santy, Sebastin and Budhiraja, Amar and Bali, Kalika and Choudhury, Monojit},
  date = {2020},
  pages = {6282--6293},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.acl-main.560},
  url = {https://www.aclweb.org/anthology/2020.acl-main.560},
  urldate = {2023-02-12},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  file = {/home/cristi/Zotero/storage/6YFH4M2H/Joshi et al. - 2020 - The State and Fate of Linguistic Diversity and Inc.pdf}
}

@article{jumperHighlyAccurateProtein2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  date = {2021-08-26},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  url = {https://www.nature.com/articles/s41586-021-03819-2},
  urldate = {2025-02-22},
  abstract = {Abstract                            Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort               1–4               , the structures of around 100,000 unique proteins have been determined               5               , but this represents a small fraction of the billions of known protein sequences               6,7               . Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’               8               —has been an important open research problem for more than 50~years               9               . Despite recent progress               10–14               , existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)               15               , demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/LR6FK3XK/Jumper et al. - 2021 - Highly accurate protein structure prediction with AlphaFold.pdf}
}

@article{jumperHighlyAccurateProtein2021a,
  title = {Highly {{Accurate Protein Structure Prediction}} with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  date = {2021-08-26},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  url = {https://www.nature.com/articles/s41586-021-03819-2},
  urldate = {2025-02-22},
  abstract = {Abstract Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort 1–4 , the structures of around 100,000 unique proteins have been determined 5 , but this represents a small fraction of the billions of known protein sequences 6,7 . Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’ 8 —has been an important open research problem for more than 50 years 9 . Despite recent progress 10–14 , existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14) 15 , demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  file = {/home/cristi/Zotero/storage/JFL3Z383/Jumper et al. - 2021 - Highly accurate protein structure prediction with AlphaFold.pdf}
}

@book{kandelPrinciplesNeuralScience2021,
  title = {Principles of Neural Science},
  editor = {Kandel, Eric R. and Koester, John and Mack, Sarah and Siegelbaum, Steven},
  date = {2021},
  edition = {Sixth edition},
  publisher = {McGraw Hill},
  location = {New York},
  abstract = {"As in previous editions, the goal of this sixth edition of Principles of Neural Science is to provide readers with insight into how genes, molecules, neurons and the circuits they form give rise to mind"-- Provided by publisher},
  isbn = {978-1-259-64223-4},
  langid = {english},
  annotation = {OCLC: 1199587061},
  file = {/home/cristi/Zotero/storage/83N9DPP4/Kandel et al. - 2021 - Principles of neural science.pdf}
}

@book{kandelPrinciplesNeuralScience2021a,
  title = {Principles of {{Neural Science}}},
  editor = {Kandel, Eric R. and Koester, John and Mack, Sarah and Siegelbaum, Steven},
  date = {2021},
  edition = {Sixth edition},
  publisher = {McGraw Hill},
  location = {New York},
  abstract = {"As in previous editions, the goal of this sixth edition of Principles of Neural Science is to provide readers with insight into how genes, molecules, neurons and the circuits they form give rise to mind"– Provided by publisher},
  isbn = {978-1-259-64223-4},
  file = {/home/cristi/Zotero/storage/U8DTMAL8/Kandel et al. - 2021 - Principles of neural science.pdf}
}

@article{kangTonogenesisEarlyContemporary2013,
  title = {Tonogenesis in Early {{Contemporary Seoul Korean}}: {{A}} Longitudinal Case Study},
  shorttitle = {Tonogenesis in Early {{Contemporary Seoul Korean}}},
  author = {Kang, Yoonjung and Han, Sungwoo},
  date = {2013-09},
  journaltitle = {Lingua},
  shortjournal = {Lingua},
  volume = {134},
  pages = {62--74},
  issn = {00243841},
  doi = {10.1016/j.lingua.2013.06.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0024384113001344},
  urldate = {2023-05-28},
  langid = {english},
  file = {/home/cristi/Zotero/storage/KDC6MVQ7/Kang and Han - 2013 - Tonogenesis in early Contemporary Seoul Korean A .pdf}
}

@article{kangTonogenesisEarlyContemporary2013a,
  title = {Tonogenesis in {{Early Contemporary Seoul Korean}}: {{A Longitudinal Case Study}}},
  shorttitle = {Tonogenesis in {{Early Contemporary Seoul Korean}}},
  author = {Kang, Yoonjung and Han, Sungwoo},
  date = {2013-09},
  journaltitle = {Lingua},
  shortjournal = {Lingua},
  volume = {134},
  pages = {62--74},
  issn = {00243841},
  doi = {10.1016/j.lingua.2013.06.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0024384113001344},
  urldate = {2023-05-28},
  file = {/home/cristi/Zotero/storage/H77ZFP6Z/Kang and Han - 2013 - Tonogenesis in early Contemporary Seoul Korean A .pdf}
}

@article{keshishianEstimatingInterpretingNonlinear2020,
  title = {Estimating and Interpreting Nonlinear Receptive Field of Sensory Neural Responses with Deep Neural Network Models},
  author = {Keshishian, Menoua and Akbari, Hassan and Khalighinejad, Bahar and Herrero, Jose L and Mehta, Ashesh D and Mesgarani, Nima},
  date = {2020-06-26},
  journaltitle = {eLife},
  volume = {9},
  pages = {e53445},
  issn = {2050-084X},
  doi = {10.7554/eLife.53445},
  url = {https://elifesciences.org/articles/53445},
  urldate = {2024-12-19},
  abstract = {Our understanding of nonlinear stimulus transformations by neural circuits is hindered by the lack of comprehensive yet interpretable computational modeling frameworks. Here, we propose a data-driven approach based on deep neural networks to directly model arbitrarily nonlinear stimulus-response mappings. Reformulating the exact function of a trained neural network as a collection of stimulus-dependent linear functions enables a locally linear receptive field interpretation of the neural network. Predicting the neural responses recorded invasively from the auditory cortex of neurosurgical patients as they listened to speech, this approach significantly improves the prediction accuracy of auditory cortical responses, particularly in nonprimary areas. Moreover, interpreting the functions learned by neural networks uncovered three distinct types of nonlinear transformations of speech that varied considerably from primary to nonprimary auditory regions. The ability of this framework to capture arbitrary stimulus-response mappings while maintaining model interpretability leads to a better understanding of cortical processing of sensory signals.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/KTBBSF6W/Keshishian et al. - 2020 - Estimating and interpreting nonlinear receptive fi.pdf}
}

@article{keshishianEstimatingInterpretingNonlinear2020a,
  title = {Estimating and {{Interpreting Nonlinear Receptive Field}} of {{Sensory Neural Responses}} with {{Deep Neural Network Models}}},
  author = {Keshishian, Menoua and Akbari, Hassan and Khalighinejad, Bahar and Herrero, Jose L and Mehta, Ashesh D and Mesgarani, Nima},
  date = {2020-06-26},
  journaltitle = {eLife},
  volume = {9},
  pages = {e53445},
  issn = {2050-084X},
  doi = {10.7554/eLife.53445},
  url = {https://elifesciences.org/articles/53445},
  urldate = {2024-12-19},
  abstract = {Our understanding of nonlinear stimulus transformations by neural circuits is hindered by the lack of comprehensive yet interpretable computational modeling frameworks. Here, we propose a data-driven approach based on deep neural networks to directly model arbitrarily nonlinear stimulus-response mappings. Reformulating the exact function of a trained neural network as a collection of stimulus-dependent linear functions enables a locally linear receptive field interpretation of the neural network. Predicting the neural responses recorded invasively from the auditory cortex of neurosurgical patients as they listened to speech, this approach significantly improves the prediction accuracy of auditory cortical responses, particularly in nonprimary areas. Moreover, interpreting the functions learned by neural networks uncovered three distinct types of nonlinear transformations of speech that varied considerably from primary to nonprimary auditory regions. The ability of this framework to capture arbitrary stimulus-response mappings while maintaining model interpretability leads to a better understanding of cortical processing of sensory signals.},
  file = {/home/cristi/Zotero/storage/L76Q4XUN/Keshishian et al. - 2020 - Estimating and interpreting nonlinear receptive fi.pdf}
}

@article{khaligh-razaviDeepSupervisedNot2014,
  title = {Deep {{Supervised}}, but {{Not Unsupervised}}, {{Models May Explain IT Cortical Representation}}},
  author = {Khaligh-Razavi, Seyed-Mahdi and Kriegeskorte, Nikolaus},
  editor = {Diedrichsen, Jörn},
  date = {2014-11-06},
  journaltitle = {PLoS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {10},
  number = {11},
  pages = {e1003915},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003915},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1003915},
  urldate = {2024-04-25},
  langid = {english},
  file = {/home/cristi/Zotero/storage/GVQGAYZD/Khaligh-Razavi and Kriegeskorte - 2014 - Deep Supervised, but Not Unsupervised, Models May .pdf}
}

@article{khaligh-razaviDeepSupervisedNot2014a,
  title = {Deep {{Supervised}}, but {{Not Unsupervised}}, {{Models May Explain IT Cortical Representation}}},
  author = {Khaligh-Razavi, Seyed-Mahdi and Kriegeskorte, Nikolaus},
  editor = {Diedrichsen, Jörn},
  date = {2014-11-06},
  journaltitle = {PLoS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {10},
  number = {11},
  pages = {e1003915},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003915},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1003915},
  urldate = {2024-04-25},
  file = {/home/cristi/Zotero/storage/J46S967C/Khaligh-Razavi and Kriegeskorte - 2014 - Deep Supervised, but Not Unsupervised, Models May .pdf}
}

@article{kleinStimulusinvariantProcessingSpectrotemporal2006,
  title = {Stimulus-Invariant Processing and Spectrotemporal Reverse Correlation in Primary Auditory Cortex},
  author = {Klein, David J. and Simon, Jonathan Z. and Depireux, Didier A. and Shamma, Shihab A.},
  date = {2006-04},
  journaltitle = {Journal of Computational Neuroscience},
  shortjournal = {J Comput Neurosci},
  volume = {20},
  number = {2},
  pages = {111--136},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-005-3589-4},
  url = {http://link.springer.com/10.1007/s10827-005-3589-4},
  urldate = {2025-01-13},
  langid = {english},
  file = {/home/cristi/Zotero/storage/TM3PBEQB/Klein et al. - 2006 - Stimulus-invariant processing and spectrotemporal reverse correlation in primary auditory cortex.pdf}
}

@article{kleinStimulusInvariantProcessingSpectrotemporal2006a,
  title = {Stimulus-{{Invariant Processing}} and {{Spectrotemporal Reverse Correlation}} in {{Primary Auditory Cortex}}},
  author = {Klein, David J. and Simon, Jonathan Z. and Depireux, Didier A. and Shamma, Shihab A.},
  date = {2006-04},
  journaltitle = {Journal of Computational Neuroscience},
  shortjournal = {J Comput Neurosci},
  volume = {20},
  number = {2},
  pages = {111--136},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-005-3589-4},
  url = {http://link.springer.com/10.1007/s10827-005-3589-4},
  urldate = {2025-01-13},
  file = {/home/cristi/Zotero/storage/23NDEKEI/Klein et al. - 2006 - Stimulus-invariant processing and spectrotemporal reverse correlation in primary auditory cortex.pdf}
}

@article{kriegeskorteCognitiveComputationalNeuroscience2018,
  title = {Cognitive Computational Neuroscience},
  author = {Kriegeskorte, Nikolaus and Douglas, Pamela K.},
  date = {2018-09},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {21},
  number = {9},
  pages = {1148--1160},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-018-0210-5},
  url = {https://www.nature.com/articles/s41593-018-0210-5},
  urldate = {2025-02-22},
  langid = {english},
  file = {/home/cristi/Zotero/storage/SIQIAQQ5/Kriegeskorte and Douglas - 2018 - Cognitive computational neuroscience.pdf}
}

@article{kriegeskorteCognitiveComputationalNeuroscience2018a,
  title = {Cognitive {{Computational Neuroscience}}},
  author = {Kriegeskorte, Nikolaus and Douglas, Pamela K.},
  date = {2018-09},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {21},
  number = {9},
  pages = {1148--1160},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-018-0210-5},
  url = {https://www.nature.com/articles/s41593-018-0210-5},
  urldate = {2025-02-22},
  file = {/home/cristi/Zotero/storage/CINRL46U/Kriegeskorte and Douglas - 2018 - Cognitive computational neuroscience.pdf}
}

@article{kriegeskorteDeepNeuralNetworks2015,
  title = {Deep {{Neural Networks}}: {{A New Framework}} for {{Modeling Biological Vision}} and {{Brain Information Processing}}},
  shorttitle = {Deep {{Neural Networks}}},
  author = {Kriegeskorte, Nikolaus},
  date = {2015-11-24},
  journaltitle = {Annual Review of Vision Science},
  shortjournal = {Annu. Rev. Vis. Sci.},
  volume = {1},
  number = {1},
  pages = {417--446},
  issn = {2374-4642, 2374-4650},
  doi = {10.1146/annurev-vision-082114-035447},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-vision-082114-035447},
  urldate = {2024-04-21},
  abstract = {Recent advances in neural network modeling have enabled major strides in computer vision and other artificial intelligence applications. Human-level visual recognition abilities are coming within reach of artificial systems. Artificial neural networks are inspired by the brain, and their computations could be implemented in biological neurons. Convolutional feedforward networks, which now dominate computer vision, take further inspiration from the architecture of the primate visual hierarchy. However, the current models are designed with engineering goals, not to model brain computations. Nevertheless, initial studies comparing internal representations between these models and primate brains find surprisingly similar representational spaces. With human-level performance no longer out of reach, we are entering an exciting new era, in which we will be able to build biologically faithful feedforward and recurrent computational models of how biological brains perform high-level feats of intelligence, including vision.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/ZF5IK3U8/Kriegeskorte - 2015 - Deep Neural Networks A New Framework for Modeling.pdf}
}

@article{kriegeskorteDeepNeuralNetworks2015a,
  title = {Deep {{Neural Networks}}: {{A New Framework}} for {{Modeling Biological Vision}} and {{Brain Information Processing}}},
  shorttitle = {Deep {{Neural Networks}}},
  author = {Kriegeskorte, Nikolaus},
  date = {2015-11-24},
  journaltitle = {Annual Review of Vision Science},
  shortjournal = {Annu. Rev. Vis. Sci.},
  volume = {1},
  number = {1},
  pages = {417--446},
  issn = {2374-4642, 2374-4650},
  doi = {10.1146/annurev-vision-082114-035447},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-vision-082114-035447},
  urldate = {2024-04-21},
  abstract = {Recent advances in neural network modeling have enabled major strides in computer vision and other artificial intelligence applications. Human-level visual recognition abilities are coming within reach of artificial systems. Artificial neural networks are inspired by the brain, and their computations could be implemented in biological neurons. Convolutional feedforward networks, which now dominate computer vision, take further inspiration from the architecture of the primate visual hierarchy. However, the current models are designed with engineering goals, not to model brain computations. Nevertheless, initial studies comparing internal representations between these models and primate brains find surprisingly similar representational spaces. With human-level performance no longer out of reach, we are entering an exciting new era, in which we will be able to build biologically faithful feedforward and recurrent computational models of how biological brains perform high-level feats of intelligence, including vision.},
  file = {/home/cristi/Zotero/storage/EHTW2QRW/Kriegeskorte - 2015 - Deep Neural Networks A New Framework for Modeling.pdf}
}

@article{kriegeskorteNeuralNetworkModels2019,
  title = {Neural Network Models and Deep Learning},
  author = {Kriegeskorte, Nikolaus and Golan, Tal},
  date = {2019-04},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {29},
  number = {7},
  pages = {R231-R236},
  issn = {09609822},
  doi = {10.1016/j.cub.2019.02.034},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982219302040},
  urldate = {2024-04-23},
  langid = {english},
  file = {/home/cristi/Zotero/storage/HDF35ZXZ/Kriegeskorte and Golan - 2019 - Neural network models and deep learning.pdf}
}

@article{kriegeskorteNeuralNetworkModels2019a,
  title = {Neural {{Network Models}} and {{Deep Learning}}},
  author = {Kriegeskorte, Nikolaus and Golan, Tal},
  date = {2019-04},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {29},
  number = {7},
  pages = {R231-R236},
  issn = {09609822},
  doi = {10.1016/j.cub.2019.02.034},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982219302040},
  urldate = {2024-04-23},
  file = {/home/cristi/Zotero/storage/8REHEK4U/Kriegeskorte and Golan - 2019 - Neural network models and deep learning.pdf}
}

@unpublished{kwisthoutBKI212aArtificialIntelligence,
  title = {{{BKI212a}}: {{Artificial Intelligence}}: {{Search}}, {{Planning}}, and {{Machine Learning}}  - {{The}} Variable Elimination Algorithm},
  author = {Kwisthout, Johan},
  url = {https://brightspace.ru.nl/d2l/le/content/431290/viewContent/2259255/View},
  urldate = {2023-12-21},
  venue = {Nijmegen}
}

@unpublished{kwisthoutBKI212aArtificialIntelligencea,
  title = {{{BKI212a}}: {{Artificial Intelligence}}: {{Search}}, {{Planning}}, and {{Machine Learning}} - {{The Variable Elimination Algorithm}}},
  author = {Kwisthout, Johan},
  url = {https://brightspace.ru.nl/d2l/le/content/431290/viewContent/2259255/View},
  urldate = {2023-12-21}
}

@unpublished{kwisthoutConstraintSatisfactionProblems2023,
  type = {PowerPoint slides},
  title = {Constraint Satisfaction Problems},
  author = {Kwisthout, Johan},
  date = {2023-10-02},
  url = {https://brightspace.ru.nl/d2l/le/content/431290/viewContent/2371537/View},
  urldate = {2023-11-19},
  venue = {Nijmegen}
}

@unpublished{kwisthoutConstraintSatisfactionProblems2023a,
  title = {Constraint {{Satisfaction Problems}}},
  author = {Kwisthout, Johan},
  date = {2023-10-02},
  url = {https://brightspace.ru.nl/d2l/le/content/431290/viewContent/2371537/View},
  urldate = {2023-11-19},
  howpublished = {PowerPoint slides}
}

@article{kyrimiIncrementalExplanationInference2020,
  title = {An Incremental Explanation of Inference in {{Bayesian}} Networks for Increasing Model Trustworthiness and Supporting Clinical Decision Making},
  author = {Kyrimi, Evangelia and Mossadegh, Somayyeh and Tai, Nigel and Marsh, William},
  date = {2020-03},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artificial Intelligence in Medicine},
  volume = {103},
  pages = {101812},
  issn = {09333657},
  doi = {10.1016/j.artmed.2020.101812},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365719307560},
  urldate = {2025-02-18},
  langid = {english},
  file = {/home/cristi/Zotero/storage/M7GL44D3/Kyrimi et al. - 2020 - An incremental explanation of inference in Bayesian networks for increasing model trustworthiness an.pdf}
}

@article{kyrimiIncrementalExplanationInference2020a,
  title = {An {{Incremental Explanation}} of {{Inference}} in {{Bayesian Networks}} for {{Increasing Model Trustworthiness}} and {{Supporting Clinical Decision Making}}},
  author = {Kyrimi, Evangelia and Mossadegh, Somayyeh and Tai, Nigel and Marsh, William},
  date = {2020-03},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artificial Intelligence in Medicine},
  volume = {103},
  pages = {101812},
  issn = {09333657},
  doi = {10.1016/j.artmed.2020.101812},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365719307560},
  urldate = {2025-02-18},
  file = {/home/cristi/Zotero/storage/YFR2J9KC/Kyrimi et al. - 2020 - An incremental explanation of inference in Bayesian networks for increasing model trustworthiness an.pdf}
}

@article{lavazzaFreeWillNeuroscience2016,
  title = {Free {{Will}} and {{Neuroscience}}: {{From Explaining Freedom Away}} to {{New Ways}} of {{Operationalizing}} and {{Measuring It}}},
  shorttitle = {Free {{Will}} and {{Neuroscience}}},
  author = {Lavazza, Andrea},
  date = {2016-06-01},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {10},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2016.00262},
  url = {http://journal.frontiersin.org/article/10.3389/fnhum.2016.00262},
  urldate = {2024-06-21},
  file = {/home/cristi/Zotero/storage/ZB4MAWUX/Lavazza - 2016 - Free Will and Neuroscience From Explaining Freedo.pdf}
}

@article{lavazzaFreeWillNeuroscience2016a,
  title = {Free {{Will}} and {{Neuroscience}}: {{From Explaining Freedom Away}} to {{New Ways}} of {{Operationalizing}} and {{Measuring It}}},
  shorttitle = {Free {{Will}} and {{Neuroscience}}},
  author = {Lavazza, Andrea},
  date = {2016-06-01},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {10},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2016.00262},
  url = {http://journal.frontiersin.org/article/10.3389/fnhum.2016.00262},
  urldate = {2024-06-21},
  file = {/home/cristi/Zotero/storage/C6QSZ5IV/Lavazza - 2016 - Free Will and Neuroscience From Explaining Freedo.pdf}
}

@article{lavazzaOperationalizingMeasuringKind2015,
  title = {Operationalizing and {{Measuring}} (a {{Kind}} of) {{Free Will}} (and {{Responsibility}}). {{Towards}} a {{New Framework}} for {{Psychology}}, {{Ethics}}, and {{Law}}},
  author = {Lavazza, Andrea and Inglese, Silvia},
  date = {2015-04-30},
  journaltitle = {Rivista Internazionale di Filosofia e Psicologia},
  pages = {37--55},
  issn = {2239-2629},
  doi = {10.4453/rifp.2015.0004},
  url = {https://doi.org/10.4453/rifp.2015.0004},
  urldate = {2024-06-26},
  issue = {Vol. VI, n. 1, 2015},
  langid = {english}
}

@article{lavazzaOperationalizingMeasuringKind2015a,
  title = {Operationalizing and {{Measuring}} (a {{Kind}} of) {{Free Will}} (and {{Responsibility}}). {{Towards}} a {{New Framework}} for {{Psychology}}, {{Ethics}}, and {{Law}}},
  author = {Lavazza, Andrea and Inglese, Silvia},
  date = {2015-04-30},
  journaltitle = {Rivista Internazionale di Filosofia e Psicologia},
  pages = {37--55},
  issn = {2239-2629},
  doi = {10.4453/rifp.2015.0004},
  url = {https://doi.org/10.4453/rifp.2015.0004},
  urldate = {2024-06-26},
  issue = {Vol. VI, n. 1, 2015}
}

@article{leffCorticalDynamicsIntelligible2008,
  title = {The {{Cortical Dynamics}} of {{Intelligible Speech}}},
  author = {Leff, Alexander P. and Schofield, Thomas M. and Stephan, Klass E. and Crinion, Jennifer T. and Friston, Karl J. and Price, Cathy J.},
  date = {2008-12-03},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {28},
  number = {49},
  pages = {13209--13215},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2903-08.2008},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.2903-08.2008},
  urldate = {2024-05-17},
  abstract = {An important and unresolved question is how the human brain processes speech for meaning after initial analyses in early auditory cortical regions. A variety of left-hemispheric areas have been identified that clearly support semantic processing, although a systematic analysis of directed interactions among these areas is lacking. We applied dynamic causal modeling of functional magnetic resonance imaging responses and Bayesian model selection to investigate, for the first time, experimentally induced changes in coupling among three key multimodal regions that were activated by intelligible speech: the posterior and anterior superior temporal sulcus (pSTS and aSTS, respectively) and pars orbitalis (POrb) of the inferior frontal gyrus. We tested 216 different dynamic causal models and found that the best model was a “forward” system that was driven by auditory inputs into the pSTS, with forward connections from the pSTS to both the aSTS and the POrb that increased considerably in strength (by 76 and 150\%, respectively) when subjects listened to intelligible speech. Task-related, directional effects can now be incorporated into models of speech comprehension.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/ERPQJMMZ/Leff et al. - 2008 - The Cortical Dynamics of Intelligible Speech.pdf}
}

@article{leffCorticalDynamicsIntelligible2008a,
  title = {The {{Cortical Dynamics}} of {{Intelligible Speech}}},
  author = {Leff, Alexander P. and Schofield, Thomas M. and Stephan, Klass E. and Crinion, Jennifer T. and Friston, Karl J. and Price, Cathy J.},
  date = {2008-12-03},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {28},
  number = {49},
  pages = {13209--13215},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2903-08.2008},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.2903-08.2008},
  urldate = {2024-05-17},
  abstract = {An important and unresolved question is how the human brain processes speech for meaning after initial analyses in early auditory cortical regions. A variety of left-hemispheric areas have been identified that clearly support semantic processing, although a systematic analysis of directed interactions among these areas is lacking. We applied dynamic causal modeling of functional magnetic resonance imaging responses and Bayesian model selection to investigate, for the first time, experimentally induced changes in coupling among three key multimodal regions that were activated by intelligible speech: the posterior and anterior superior temporal sulcus (pSTS and aSTS, respectively) and pars orbitalis (POrb) of the inferior frontal gyrus. We tested 216 different dynamic causal models and found that the best model was a “forward” system that was driven by auditory inputs into the pSTS, with forward connections from the pSTS to both the aSTS and the POrb that increased considerably in strength (by 76 and 150\%, respectively) when subjects listened to intelligible speech. Task-related, directional effects can now be incorporated into models of speech comprehension.},
  file = {/home/cristi/Zotero/storage/ZU3LS2UG/Leff et al. - 2008 - The Cortical Dynamics of Intelligible Speech.pdf}
}

@article{leroyEarlyMaturationLinguistic2011,
  title = {Early {{Maturation}} of the {{Linguistic Dorsal Pathway}} in {{Human Infants}}},
  author = {Leroy, François and Glasel, Hervé and Dubois, Jessica and Hertz-Pannier, Lucie and Thirion, Bertrand and Mangin, Jean-François and Dehaene-Lambertz, Ghislaine},
  date = {2011-01-26},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {31},
  number = {4},
  pages = {1500--1506},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4141-10.2011},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.4141-10.2011},
  urldate = {2024-05-17},
  abstract = {Human infants, unlike even closely related primates, exhibit a remarkable capacity for language learning. Yet how the underlying anatomical network matures remains largely unknown. The classical view is that of a largely immature brain comprising only a few islands of maturity in primary cortices. This view has favored a description of learning based on bottom-up algorithms and has tended to discard the role of frontal regions, which were assumed to be barely functional early on. Here, using an index based on the normalized T2-weighted magnetic resonance signal, we have quantified maturation within the linguistic network in fourteen 1- to 4-month-old infants. Our results show first that the ventral superior temporal sulcus (STS), and not the inferior frontal area, is the less mature perisylvian region. A significant difference of maturation in the STS favoring the right side is an early testimony of the distinctive left–right development of this structure observed during the whole life. Second, asymmetries of maturation in Broca's area were correlated with asymmetries in the posterior STS and in the parietal segment of the arcuate fasciculus, suggesting that an efficient frontotemporal dorsal pathway might provide infants with a phonological loop circuitry much earlier than expected.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/WDYL6CZ5/Leroy et al. - 2011 - Early Maturation of the Linguistic Dorsal Pathway .pdf}
}

@article{leroyEarlyMaturationLinguistic2011a,
  title = {Early {{Maturation}} of the {{Linguistic Dorsal Pathway}} in {{Human Infants}}},
  author = {Leroy, François and Glasel, Hervé and Dubois, Jessica and Hertz-Pannier, Lucie and Thirion, Bertrand and Mangin, Jean-François and Dehaene-Lambertz, Ghislaine},
  date = {2011-01-26},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {31},
  number = {4},
  pages = {1500--1506},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4141-10.2011},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.4141-10.2011},
  urldate = {2024-05-17},
  abstract = {Human infants, unlike even closely related primates, exhibit a remarkable capacity for language learning. Yet how the underlying anatomical network matures remains largely unknown. The classical view is that of a largely immature brain comprising only a few islands of maturity in primary cortices. This view has favored a description of learning based on bottom-up algorithms and has tended to discard the role of frontal regions, which were assumed to be barely functional early on. Here, using an index based on the normalized T2-weighted magnetic resonance signal, we have quantified maturation within the linguistic network in fourteen 1- to 4-month-old infants. Our results show first that the ventral superior temporal sulcus (STS), and not the inferior frontal area, is the less mature perisylvian region. A significant difference of maturation in the STS favoring the right side is an early testimony of the distinctive left–right development of this structure observed during the whole life. Second, asymmetries of maturation in Broca's area were correlated with asymmetries in the posterior STS and in the parietal segment of the arcuate fasciculus, suggesting that an efficient frontotemporal dorsal pathway might provide infants with a phonological loop circuitry much earlier than expected.},
  file = {/home/cristi/Zotero/storage/AK8ECG4U/Leroy et al. - 2011 - Early Maturation of the Linguistic Dorsal Pathway .pdf}
}

@article{leroyNewHumanspecificBrain2015,
  title = {New Human-Specific Brain Landmark: {{The}} Depth Asymmetry of Superior Temporal Sulcus},
  shorttitle = {New Human-Specific Brain Landmark},
  author = {Leroy, François and Cai, Qing and Bogart, Stephanie L. and Dubois, Jessica and Coulon, Olivier and Monzalvo, Karla and Fischer, Clara and Glasel, Hervé and Van Der Haegen, Lise and Bénézit, Audrey and Lin, Ching-Po and Kennedy, David N. and Ihara, Aya S. and Hertz-Pannier, Lucie and Moutard, Marie-Laure and Poupon, Cyril and Brysbaert, Marc and Roberts, Neil and Hopkins, William D. and Mangin, Jean-François and Dehaene-Lambertz, Ghislaine},
  date = {2015-01-27},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {112},
  number = {4},
  pages = {1208--1213},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1412389112},
  url = {https://pnas.org/doi/full/10.1073/pnas.1412389112},
  urldate = {2024-05-17},
  abstract = {Significance             In the human brain, from early in development through to adulthood, the superior temporal sulcus is deeper in the right than the left cerebral hemisphere in the area ventral of Heschl’s gyrus. Irrespective of gender, handedness, and language lateralization, and present in several pathologies, this asymmetry is widely shared among the human population. Its appearance early in life suggests strong genetic control over this part of the brain. In contrast, the asymmetry is barely visible in chimpanzees. Thus this asymmetry probably is a key locus to look for variations in gene expression among the primate lineage that have favored the evolution of crucial cognitive abilities sustained by this sulcus in our species, namely communication and social cognition.           ,              Identifying potentially unique features of the human cerebral cortex is a first step to understanding how evolution has shaped the brain in our species. By analyzing MR images obtained from 177 humans and 73 chimpanzees, we observed a human-specific asymmetry in the superior temporal sulcus at the heart of the communication regions and which we have named the “superior temporal asymmetrical pit” (STAP). This 45-mm-long segment ventral to Heschl’s gyrus is deeper in the right hemisphere than in the left in 95\% of typical human subjects, from infanthood till adulthood, and is present, irrespective of handedness, language lateralization, and sex although it is greater in males than in females. The STAP also is seen in several groups of atypical subjects including persons with situs inversus, autistic spectrum disorder, Turner syndrome, and corpus callosum agenesis. It is explained in part by the larger number of sulcal interruptions in the left than in the right hemisphere. Its early presence in the infants of this study as well as in fetuses and premature infants suggests a strong genetic influence. Because this asymmetry is barely visible in chimpanzees, we recommend the STAP region during midgestation as an important phenotype to investigate asymmetrical variations of gene expression among the primate lineage. This genetic target may provide important insights regarding the evolution of the crucial cognitive abilities sustained by this sulcus in our species, namely communication and social cognition.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/EL4MZX9K/Leroy et al. - 2015 - New human-specific brain landmark The depth asymm.pdf}
}

@article{leroyNewHumanSpecificBrain2015a,
  title = {New {{Human-Specific Brain Landmark}}: {{The Depth Asymmetry}} of {{Superior Temporal Sulcus}}},
  shorttitle = {New {{Human-Specific Brain Landmark}}},
  author = {Leroy, François and Cai, Qing and Bogart, Stephanie L. and Dubois, Jessica and Coulon, Olivier and Monzalvo, Karla and Fischer, Clara and Glasel, Hervé and Van Der Haegen, Lise and Bénézit, Audrey and Lin, Ching-Po and Kennedy, David N. and Ihara, Aya S. and Hertz-Pannier, Lucie and Moutard, Marie-Laure and Poupon, Cyril and Brysbaert, Marc and Roberts, Neil and Hopkins, William D. and Mangin, Jean-François and Dehaene-Lambertz, Ghislaine},
  date = {2015-01-27},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {112},
  number = {4},
  pages = {1208--1213},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1412389112},
  url = {https://pnas.org/doi/full/10.1073/pnas.1412389112},
  urldate = {2024-05-17},
  abstract = {Significance In the human brain, from early in development through to adulthood, the superior temporal sulcus is deeper in the right than the left cerebral hemisphere in the area ventral of Heschl’s gyrus. Irrespective of gender, handedness, and language lateralization, and present in several pathologies, this asymmetry is widely shared among the human population. Its appearance early in life suggests strong genetic control over this part of the brain. In contrast, the asymmetry is barely visible in chimpanzees. Thus this asymmetry probably is a key locus to look for variations in gene expression among the primate lineage that have favored the evolution of crucial cognitive abilities sustained by this sulcus in our species, namely communication and social cognition. , Identifying potentially unique features of the human cerebral cortex is a first step to understanding how evolution has shaped the brain in our species. By analyzing MR images obtained from 177 humans and 73 chimpanzees, we observed a human-specific asymmetry in the superior temporal sulcus at the heart of the communication regions and which we have named the “superior temporal asymmetrical pit” (STAP). This 45-mm-long segment ventral to Heschl’s gyrus is deeper in the right hemisphere than in the left in 95\% of typical human subjects, from infanthood till adulthood, and is present, irrespective of handedness, language lateralization, and sex although it is greater in males than in females. The STAP also is seen in several groups of atypical subjects including persons with situs inversus, autistic spectrum disorder, Turner syndrome, and corpus callosum agenesis. It is explained in part by the larger number of sulcal interruptions in the left than in the right hemisphere. Its early presence in the infants of this study as well as in fetuses and premature infants suggests a strong genetic influence. Because this asymmetry is barely visible in chimpanzees, we recommend the STAP region during midgestation as an important phenotype to investigate asymmetrical variations of gene expression among the primate lineage. This genetic target may provide important insights regarding the evolution of the crucial cognitive abilities sustained by this sulcus in our species, namely communication and social cognition.},
  file = {/home/cristi/Zotero/storage/GN9WNF6D/Leroy et al. - 2015 - New human-specific brain landmark The depth asymm.pdf}
}

@article{levyComputersPopulismArtificial2018,
  title = {Computers and Populism: Artificial Intelligence, Jobs, and Politics in the near Term},
  shorttitle = {Computers and Populism},
  author = {Levy, Frank},
  date = {2018-07-02},
  journaltitle = {Oxford Review of Economic Policy},
  volume = {34},
  number = {3},
  pages = {393--417},
  issn = {0266-903X, 1460-2121},
  doi = {10.1093/oxrep/gry004},
  url = {https://academic.oup.com/oxrep/article/34/3/393/5047375},
  urldate = {2023-11-12},
  langid = {english},
  file = {/home/cristi/Zotero/storage/DFY2YBSE/Levy - 2018 - Computers and populism artificial intelligence, j.pdf}
}

@article{levyComputersPopulismArtificial2018a,
  title = {Computers and Populism: Artificial Intelligence, Jobs, and Politics in the near Term},
  shorttitle = {Computers and Populism},
  author = {Levy, Frank},
  date = {2018-07-02},
  journaltitle = {Oxford Review of Economic Policy},
  volume = {34},
  number = {3},
  pages = {393--417},
  issn = {0266-903X, 1460-2121},
  doi = {10.1093/oxrep/gry004},
  url = {https://academic.oup.com/oxrep/article/34/3/393/5047375},
  urldate = {2024-01-07},
  langid = {english}
}

@article{levyComputersPopulismArtificial2018b,
  title = {Computers and {{Populism}}: {{Artificial Intelligence}}, {{Jobs}}, and {{Politics}} in the near {{Term}}},
  shorttitle = {Computers and {{Populism}}},
  author = {Levy, Frank},
  date = {2018-07-02},
  journaltitle = {Oxford Review of Economic Policy},
  volume = {34},
  number = {3},
  pages = {393--417},
  issn = {0266-903X, 1460-2121},
  doi = {10.1093/oxrep/gry004},
  url = {https://academic.oup.com/oxrep/article/34/3/393/5047375},
  urldate = {2023-11-12},
  file = {/home/cristi/Zotero/storage/MB5A74EN/Levy - 2018 - Computers and populism artificial intelligence, j.pdf}
}

@article{levyComputersPopulismArtificial2018c,
  title = {Computers and {{Populism}}: {{Artificial Intelligence}}, {{Jobs}}, and {{Politics}} in the near {{Term}}},
  shorttitle = {Computers and {{Populism}}},
  author = {Levy, Frank},
  date = {2018-07-02},
  journaltitle = {Oxford Review of Economic Policy},
  volume = {34},
  number = {3},
  pages = {393--417},
  issn = {0266-903X, 1460-2121},
  doi = {10.1093/oxrep/gry004},
  url = {https://academic.oup.com/oxrep/article/34/3/393/5047375},
  urldate = {2024-01-07}
}

@online{liaoHumanCenteredExplainableAI2021,
  title = {Human-{{Centered Explainable AI}} ({{XAI}}): {{From Algorithms}} to {{User Experiences}}},
  shorttitle = {Human-{{Centered Explainable AI}} ({{XAI}})},
  author = {Liao, Q. Vera and Varshney, Kush R.},
  date = {2021},
  doi = {10.48550/ARXIV.2110.10790},
  url = {https://arxiv.org/abs/2110.10790},
  urldate = {2025-03-04},
  abstract = {In recent years, the field of explainable AI (XAI) has produced a vast collection of algorithms, providing a useful toolbox for researchers and practitioners to build XAI applications. With the rich application opportunities, explainability is believed to have moved beyond a demand by data scientists or researchers to comprehend the models they develop, to an essential requirement for people to trust and adopt AI deployed in numerous domains. However, explainability is an inherently human-centric property and the field is starting to embrace human-centered approaches. Human-computer interaction (HCI) research and user experience (UX) design in this area are becoming increasingly important. In this chapter, we begin with a high-level overview of the technical landscape of XAI algorithms, then selectively survey our own and other recent HCI works that take human-centered approaches to design, evaluate, and provide conceptual and methodological tools for XAI. We ask the question "what are human-centered approaches doing for XAI" and highlight three roles that they play in shaping XAI technologies by helping navigate, assess and expand the XAI toolbox: to drive technical choices by users' explainability needs, to uncover pitfalls of existing XAI methods and inform new methods, and to provide conceptual frameworks for human-compatible XAI.},
  pubstate = {prepublished},
  version = {5},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Human-Computer Interaction (cs.HC)},
  file = {/home/cristi/Zotero/storage/ZQNGTG86/Liao and Varshney - 2021 - Human-Centered Explainable AI (XAI) From Algorithms to User Experiences.pdf}
}

@online{liaoHumanCenteredExplainableAI2021a,
  title = {Human-{{Centered Explainable AI}} ({{XAI}}): {{From Algorithms}} to {{User Experiences}}},
  shorttitle = {Human-{{Centered Explainable AI}} ({{XAI}})},
  author = {Liao, Q. Vera and Varshney, Kush R.},
  date = {2021},
  doi = {10.48550/ARXIV.2110.10790},
  url = {https://arxiv.org/abs/2110.10790},
  urldate = {2025-03-04},
  abstract = {In recent years, the field of explainable AI (XAI) has produced a vast collection of algorithms, providing a useful toolbox for researchers and practitioners to build XAI applications. With the rich application opportunities, explainability is believed to have moved beyond a demand by data scientists or researchers to comprehend the models they develop, to an essential requirement for people to trust and adopt AI deployed in numerous domains. However, explainability is an inherently human-centric property and the field is starting to embrace human-centered approaches. Human-computer interaction (HCI) research and user experience (UX) design in this area are becoming increasingly important. In this chapter, we begin with a high-level overview of the technical landscape of XAI algorithms, then selectively survey our own and other recent HCI works that take human-centered approaches to design, evaluate, and provide conceptual and methodological tools for XAI. We ask the question "what are human-centered approaches doing for XAI" and highlight three roles that they play in shaping XAI technologies by helping navigate, assess and expand the XAI toolbox: to drive technical choices by users' explainability needs, to uncover pitfalls of existing XAI methods and inform new methods, and to provide conceptual frameworks for human-compatible XAI.},
  version = {5},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Human-Computer Interaction (cs.HC)},
  file = {/home/cristi/Zotero/storage/ZKQPWVPT/Liao and Varshney - 2021 - Human-Centered Explainable AI (XAI) From Algorithms to User Experiences.pdf}
}

@inbook{libetTimeConsciousIntention1993,
  title = {Time of {{Conscious Intention}} to {{Act}} in {{Relation}} to {{Onset}} of {{Cerebral Activity}} ({{Readiness-Potential}})},
  booktitle = {Neurophysiology of {{Consciousness}}},
  author = {Libet, Benjamin and Gleason, Curtis A. and Wright, Elwood W. and Pearl, Dennis K.},
  date = {1993},
  pages = {249--268},
  publisher = {Birkhäuser Boston},
  location = {Boston, MA},
  doi = {10.1007/978-1-4612-0355-1_15},
  url = {http://link.springer.com/10.1007/978-1-4612-0355-1_15},
  urldate = {2024-06-23},
  bookauthor = {Libet, Benjamin},
  isbn = {978-1-4612-6722-5 978-1-4612-0355-1},
  langid = {english},
  file = {/home/cristi/Zotero/storage/C3M2CNJ3/Libet et al. - 1993 - Time of Conscious Intention to Act in Relation to .pdf}
}

@incollection{libetTimeConsciousIntention1993a,
  title = {Time of {{Conscious Intention}} to {{Act}} in {{Relation}} to {{Onset}} of {{Cerebral Activity}} ({{Readiness-Potential}})},
  booktitle = {Neurophysiology of {{Consciousness}}},
  author = {Libet, Benjamin and Gleason, Curtis A. and Wright, Elwood W. and Pearl, Dennis K.},
  date = {1993},
  pages = {249--268},
  publisher = {Birkhäuser Boston},
  location = {Boston, MA},
  doi = {10.1007/978-1-4612-0355-1_15},
  url = {http://link.springer.com/10.1007/978-1-4612-0355-1_15},
  urldate = {2024-06-23},
  isbn = {978-1-4612-6722-5 978-1-4612-0355-1},
  file = {/home/cristi/Zotero/storage/B9XCARBJ/Libet et al. - 1993 - Time of Conscious Intention to Act in Relation to .pdf}
}

@article{libetUnconsciousCerebralInitiative1985,
  title = {Unconscious Cerebral Initiative and the Role of Conscious Will in Voluntary Action},
  author = {Libet, Benjamin},
  date = {1985-12},
  journaltitle = {Behavioral and Brain Sciences},
  shortjournal = {Behav Brain Sci},
  volume = {8},
  number = {4},
  pages = {529--539},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X00044903},
  url = {https://www.cambridge.org/core/product/identifier/S0140525X00044903/type/journal_article},
  urldate = {2024-06-23},
  abstract = {Abstract             Voluntary acts are preceded by electrophysiological “readiness potentials” (RPs). With spontaneous acts involving no preplanning, the main negative RP shift begins at about—550 ms. Such RPs were used to indicate the minimum onset times for the cerebral activity that precedes a fully endogenous voluntary act. The time of conscious intention to act was obtained from the subject's recall of the spatial clock position of a revolving spot at the time of his initial awareness of intending or wanting to move (W). W occurred at about—200 ms. Control experiments, in which a skin stimulus was timed (S), helped evaluate each subject's error in reporting the clock times for awareness of any perceived event.             For spontaneous voluntary acts, RP onset preceded the uncorrected Ws by about 350 ms and the Ws corrected for S by about 400 ms. The direction of this difference was consistent and significant throughout, regardless of which of several measures of RP onset or W were used. It was concluded that cerebral initiation of a spontaneous voluntary act begins unconsciously. However, it was found that the final decision to act could still be consciously controlled during the 150 ms or so remaining after the specific conscious intention appears. Subjects can in fact “veto” motor performance during a 100–200-ms period before a prearranged time to act.             The role of conscious will would be not to initiate a specific voluntary act but rather to select and control volitional outcome. It is proposed that conscious will can function in a permissive fashion, either to permit or to prevent the motor implementation of the intention to act that arises unconsciously. Alternatively, there may be the need for a conscious activation or triggering, without which the final motor output would not follow the unconscious cerebral initiating and preparatory processes.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/FCPF3VRY/Libet - 1985 - Unconscious cerebral initiative and the role of co.pdf}
}

@article{libetUnconsciousCerebralInitiative1985a,
  title = {Unconscious {{Cerebral Initiative}} and the {{Role}} of {{Conscious Will}} in {{Voluntary Action}}},
  author = {Libet, Benjamin},
  date = {1985-12},
  journaltitle = {Behavioral and Brain Sciences},
  shortjournal = {Behav Brain Sci},
  volume = {8},
  number = {4},
  pages = {529--539},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X00044903},
  url = {https://www.cambridge.org/core/product/identifier/S0140525X00044903/type/journal_article},
  urldate = {2024-06-23},
  abstract = {Abstract Voluntary acts are preceded by electrophysiological “readiness potentials” (RPs). With spontaneous acts involving no preplanning, the main negative RP shift begins at about—550 ms. Such RPs were used to indicate the minimum onset times for the cerebral activity that precedes a fully endogenous voluntary act. The time of conscious intention to act was obtained from the subject's recall of the spatial clock position of a revolving spot at the time of his initial awareness of intending or wanting to move (W). W occurred at about—200 ms. Control experiments, in which a skin stimulus was timed (S), helped evaluate each subject's error in reporting the clock times for awareness of any perceived event. For spontaneous voluntary acts, RP onset preceded the uncorrected Ws by about 350 ms and the Ws corrected for S by about 400 ms. The direction of this difference was consistent and significant throughout, regardless of which of several measures of RP onset or W were used. It was concluded that cerebral initiation of a spontaneous voluntary act begins unconsciously. However, it was found that the final decision to act could still be consciously controlled during the 150 ms or so remaining after the specific conscious intention appears. Subjects can in fact “veto” motor performance during a 100–200-ms period before a prearranged time to act. The role of conscious will would be not to initiate a specific voluntary act but rather to select and control volitional outcome. It is proposed that conscious will can function in a permissive fashion, either to permit or to prevent the motor implementation of the intention to act that arises unconsciously. Alternatively, there may be the need for a conscious activation or triggering, without which the final motor output would not follow the unconscious cerebral initiating and preparatory processes.},
  file = {/home/cristi/Zotero/storage/QZPMA28Z/Libet - 1985 - Unconscious cerebral initiative and the role of co.pdf}
}

@article{lindenSpectrotemporalStructureReceptive2003,
  title = {Spectrotemporal {{Structure}} of {{Receptive Fields}} in {{Areas AI}} and {{AAF}} of {{Mouse Auditory Cortex}}},
  author = {Linden, Jennifer F. and Liu, Robert C. and Sahani, Maneesh and Schreiner, Christoph E. and Merzenich, Michael M.},
  date = {2003-10},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {Journal of Neurophysiology},
  volume = {90},
  number = {4},
  pages = {2660--2675},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00751.2002},
  url = {https://www.physiology.org/doi/10.1152/jn.00751.2002},
  urldate = {2025-02-28},
  abstract = {The mouse is a promising model system for auditory cortex research because of the powerful genetic tools available for manipulating its neural circuitry. Previous studies have identified two tonotopic auditory areas in the mouse—primary auditory cortex (AI) and anterior auditory field (AAF)— but auditory receptive fields in these areas have not yet been described. To establish a foundation for investigating auditory cortical circuitry and plasticity in the mouse, we characterized receptive-field structure in AI and AAF of anesthetized mice using spectrally complex and temporally dynamic stimuli as well as simple tonal stimuli. Spectrotemporal receptive fields (STRFs) were derived from extracellularly recorded responses to complex stimuli, and frequency-intensity tuning curves were constructed from responses to simple tonal stimuli. Both analyses revealed temporal differences between AI and AAF responses: peak latencies and receptive-field durations for STRFs and first-spike latencies for responses to tone bursts were significantly longer in AI than in AAF. Spectral properties of AI and AAF receptive fields were more similar, although STRF bandwidths were slightly broader in AI than in AAF. Finally, in both AI and AAF, a substantial minority of STRFs were spectrotemporally inseparable. The spectrotemporal interaction typically appeared in the form of clearly disjoint excitatory and inhibitory subfields or an obvious spectrotemporal slant in the STRF. These data provide the first detailed description of auditory receptive fields in the mouse and suggest that although neurons in areas AI and AAF share many response characteristics, area AAF may be specialized for faster temporal processing.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/RVN8P4VU/Linden et al. - 2003 - Spectrotemporal Structure of Receptive Fields in Areas AI and AAF of Mouse Auditory Cortex.pdf}
}

@article{lindenSpectrotemporalStructureReceptive2003a,
  title = {Spectrotemporal {{Structure}} of {{Receptive Fields}} in {{Areas AI}} and {{AAF}} of {{Mouse Auditory Cortex}}},
  author = {Linden, Jennifer F. and Liu, Robert C. and Sahani, Maneesh and Schreiner, Christoph E. and Merzenich, Michael M.},
  date = {2003-10},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {Journal of Neurophysiology},
  volume = {90},
  number = {4},
  pages = {2660--2675},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00751.2002},
  url = {https://www.physiology.org/doi/10.1152/jn.00751.2002},
  urldate = {2025-02-28},
  abstract = {The mouse is a promising model system for auditory cortex research because of the powerful genetic tools available for manipulating its neural circuitry. Previous studies have identified two tonotopic auditory areas in the mouse—primary auditory cortex (AI) and anterior auditory field (AAF)— but auditory receptive fields in these areas have not yet been described. To establish a foundation for investigating auditory cortical circuitry and plasticity in the mouse, we characterized receptive-field structure in AI and AAF of anesthetized mice using spectrally complex and temporally dynamic stimuli as well as simple tonal stimuli. Spectrotemporal receptive fields (STRFs) were derived from extracellularly recorded responses to complex stimuli, and frequency-intensity tuning curves were constructed from responses to simple tonal stimuli. Both analyses revealed temporal differences between AI and AAF responses: peak latencies and receptive-field durations for STRFs and first-spike latencies for responses to tone bursts were significantly longer in AI than in AAF. Spectral properties of AI and AAF receptive fields were more similar, although STRF bandwidths were slightly broader in AI than in AAF. Finally, in both AI and AAF, a substantial minority of STRFs were spectrotemporally inseparable. The spectrotemporal interaction typically appeared in the form of clearly disjoint excitatory and inhibitory subfields or an obvious spectrotemporal slant in the STRF. These data provide the first detailed description of auditory receptive fields in the mouse and suggest that although neurons in areas AI and AAF share many response characteristics, area AAF may be specialized for faster temporal processing.},
  file = {/home/cristi/Zotero/storage/QPK9W4PN/Linden et al. - 2003 - Spectrotemporal Structure of Receptive Fields in Areas AI and AAF of Mouse Auditory Cortex.pdf}
}

@article{liOverviewNoiseRobustAutomatic2014,
  title = {An {{Overview}} of {{Noise-Robust Automatic Speech Recognition}}},
  author = {Li, Jinyu and Deng, Li and Gong, Yifan and Haeb-Umbach, Reinhold},
  date = {2014-04},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {22},
  number = {4},
  pages = {745--777},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2014.2304637},
  url = {http://ieeexplore.ieee.org/document/6732927/},
  urldate = {2023-03-13},
  file = {/home/cristi/Zotero/storage/FW36XN2Q/Li et al. - 2014 - An Overview of Noise-Robust Automatic Speech Recog.pdf}
}

@article{liOverviewNoiseRobustAutomatic2014a,
  title = {An {{Overview}} of {{Noise-Robust Automatic Speech Recognition}}},
  author = {Li, Jinyu and Deng, Li and Gong, Yifan and Haeb-Umbach, Reinhold},
  date = {2014-04},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {22},
  number = {4},
  pages = {745--777},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2014.2304637},
  url = {http://ieeexplore.ieee.org/document/6732927/},
  urldate = {2023-03-13},
  file = {/home/cristi/Zotero/storage/YDWQ7ED2/Li et al. - 2014 - An Overview of Noise-Robust Automatic Speech Recog.pdf}
}

@article{liptonMythosModelInterpretability2018,
  title = {The {{Mythos}} of {{Model Interpretability}}: {{In}} Machine Learning, the Concept of Interpretability Is Both Important and Slippery.},
  shorttitle = {The {{Mythos}} of {{Model Interpretability}}},
  author = {Lipton, Zachary C.},
  date = {2018-06},
  journaltitle = {Queue},
  shortjournal = {Queue},
  volume = {16},
  number = {3},
  pages = {31--57},
  issn = {1542-7730, 1542-7749},
  doi = {10.1145/3236386.3241340},
  url = {https://dl.acm.org/doi/10.1145/3236386.3241340},
  urldate = {2025-03-28},
  abstract = {Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?},
  langid = {english},
  file = {/home/cristi/Zotero/storage/RATWTDBQ/Lipton - 2018 - The Mythos of Model Interpretability In machine learning, the concept of interpretability is both i.pdf}
}

@article{liRobustNeuronalDynamics2016,
  title = {Robust Neuronal Dynamics in Premotor Cortex during Motor Planning},
  author = {Li, Nuo and Daie, Kayvon and Svoboda, Karel and Druckmann, Shaul},
  date = {2016-04-28},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {532},
  number = {7600},
  pages = {459--464},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature17643},
  url = {https://www.nature.com/articles/nature17643},
  urldate = {2024-05-17},
  langid = {english},
  file = {/home/cristi/Zotero/storage/D364YZXD/Li et al. - 2016 - Robust neuronal dynamics in premotor cortex during.pdf}
}

@article{liRobustNeuronalDynamics2016a,
  title = {Robust {{Neuronal Dynamics}} in {{Premotor Cortex}} during {{Motor Planning}}},
  author = {Li, Nuo and Daie, Kayvon and Svoboda, Karel and Druckmann, Shaul},
  date = {2016-04-28},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {532},
  number = {7600},
  pages = {459--464},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature17643},
  url = {https://www.nature.com/articles/nature17643},
  urldate = {2024-05-17},
  file = {/home/cristi/Zotero/storage/K2EAZRJM/Li et al. - 2016 - Robust neuronal dynamics in premotor cortex during.pdf}
}

@article{liSpokenLanguageRecognition2013,
  title = {Spoken {{Language Recognition}}: {{From Fundamentals}} to {{Practice}}},
  shorttitle = {Spoken {{Language Recognition}}},
  author = {Li, Haizhou and Ma, Bin and Lee, Kong Aik},
  date = {2013-05},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {101},
  number = {5},
  pages = {1136--1159},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2012.2237151},
  url = {http://ieeexplore.ieee.org/document/6451097/},
  urldate = {2023-03-13},
  file = {/home/cristi/Zotero/storage/XCYPMZ8S/Li et al. - 2013 - Spoken Language Recognition From Fundamentals to .pdf}
}

@article{liSpokenLanguageRecognition2013a,
  title = {Spoken {{Language Recognition}}: {{From Fundamentals}} to {{Practice}}},
  shorttitle = {Spoken {{Language Recognition}}},
  author = {Li, Haizhou and Ma, Bin and Lee, Kong Aik},
  date = {2013-05},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {101},
  number = {5},
  pages = {1136--1159},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2012.2237151},
  url = {http://ieeexplore.ieee.org/document/6451097/},
  urldate = {2023-03-13},
  file = {/home/cristi/Zotero/storage/N2MJE5PU/Li et al. - 2013 - Spoken Language Recognition From Fundamentals to .pdf}
}

@inproceedings{liuIsolationForest2008,
  title = {Isolation {{Forest}}},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  date = {2008-12},
  pages = {413--422},
  publisher = {IEEE},
  location = {Pisa, Italy},
  doi = {10.1109/ICDM.2008.17},
  url = {http://ieeexplore.ieee.org/document/4781136/},
  urldate = {2024-01-04},
  eventtitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  isbn = {978-0-7695-3502-9},
  file = {/home/cristi/Zotero/storage/YBC447IB/Liu et al. - 2008 - Isolation Forest.pdf}
}

@inproceedings{liuIsolationForest2008a,
  title = {Isolation {{Forest}}},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  date = {2008-12},
  pages = {413--422},
  publisher = {IEEE},
  location = {Pisa, Italy},
  doi = {10.1109/ICDM.2008.17},
  url = {http://ieeexplore.ieee.org/document/4781136/},
  urldate = {2024-01-04},
  eventtitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  isbn = {978-0-7695-3502-9},
  file = {/home/cristi/Zotero/storage/ZGPT8LBQ/Liu et al. - 2008 - Isolation Forest.pdf}
}

@book{longhoferSocialTheoryRewired2023,
  title = {Social Theory Re-Wired: New Connections to Classical and Contemporary Perspectives},
  shorttitle = {Social Theory Re-Wired},
  editor = {Longhofer, Wesley and Winchester, Daniel},
  date = {2023},
  series = {Sociology Re-Wired},
  edition = {3rd edition},
  publisher = {Routledge},
  location = {New York, NY},
  abstract = {"This third edition of Social Theory Re-Wired is significantly revised and its unique web learning interactive programs that "allow us to go farther into theory and to build student skills than ever before," according to many teachers. Vital political and social updates are reflected both in the text and the online supplements"--},
  isbn = {978-1-032-34113-2 978-1-032-34111-8},
  keywords = {Philosophy,Social sciences,Sociology}
}

@online{luongEffectiveApproachesAttentionbased2015,
  title = {Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  date = {2015-09-20},
  eprint = {1508.04025},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1508.04025},
  urldate = {2024-01-05},
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/cristi/Zotero/storage/G86WEPRH/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf;/home/cristi/Zotero/storage/KDL68QCX/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf;/home/cristi/Zotero/storage/J7968QWJ/1508.html}
}

@online{luongEffectiveApproachesAttentionbased2015a,
  title = {Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  date = {2015-09-20},
  eprint = {1508.04025},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1508.04025},
  urldate = {2024-01-05},
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  keywords = {Computer Science - Computation and Language},
  file = {/home/cristi/Zotero/storage/R3RUSR5X/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf;/home/cristi/Zotero/storage/WJCJQLT9/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf;/home/cristi/Zotero/storage/NH94PWHN/1508.html}
}

@article{luppiLSDAltersDynamic2021,
  title = {{{LSD}} Alters Dynamic Integration and Segregation in the Human Brain},
  author = {Luppi, Andrea I. and Carhart-Harris, Robin L. and Roseman, Leor and Pappas, Ioannis and Menon, David K. and Stamatakis, Emmanuel A.},
  date = {2021-02},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {227},
  pages = {117653},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2020.117653},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811920311381},
  urldate = {2024-05-17},
  langid = {english},
  file = {/home/cristi/Zotero/storage/VLWH95TE/Luppi et al. - 2021 - LSD alters dynamic integration and segregation in .pdf}
}

@article{luppiLSDAltersDynamic2021a,
  title = {{{LSD Alters Dynamic Integration}} and {{Segregation}} in the {{Human Brain}}},
  author = {Luppi, Andrea I. and Carhart-Harris, Robin L. and Roseman, Leor and Pappas, Ioannis and Menon, David K. and Stamatakis, Emmanuel A.},
  date = {2021-02},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {227},
  pages = {117653},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2020.117653},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811920311381},
  urldate = {2024-05-17},
  file = {/home/cristi/Zotero/storage/EW4IFBKB/Luppi et al. - 2021 - LSD alters dynamic integration and segregation in .pdf}
}

@article{macdonaldMRIHealthyBrain2021,
  title = {{{MRI}} of Healthy Brain Aging: {{A}} Review},
  shorttitle = {{{MRI}} of Healthy Brain Aging},
  author = {MacDonald, M. Ethan and Pike, G. Bruce},
  date = {2021-09},
  journaltitle = {NMR in Biomedicine},
  shortjournal = {NMR in Biomedicine},
  volume = {34},
  number = {9},
  pages = {e4564},
  issn = {0952-3480, 1099-1492},
  doi = {10.1002/nbm.4564},
  url = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/nbm.4564},
  urldate = {2024-05-17},
  abstract = {We present a review of the characterization of healthy brain aging using MRI with an emphasis on morphology, lesions, and quantitative MR parameters. A scope review found 6612 articles encompassing the keywords “Brain Aging” and “Magnetic Resonance”; papers involving functional MRI or not involving imaging of healthy human brain aging were discarded, leaving 2246 articles. We first consider some of the biogerontological mechanisms of aging, and the consequences of aging in terms of cognition and onset of disease. Morphological changes with aging are reviewed for the whole brain, cerebral cortex, white matter, subcortical gray matter, and other individual structures. In general, volume and cortical thickness decline with age, beginning in mid‐life. Prevalent silent lesions such as white matter hyperintensities, microbleeds, and lacunar infarcts are also observed with increasing frequency. The literature regarding quantitative MR parameter changes includes               T               1               ,               T               2               ,               T               2               *, magnetic susceptibility, spectroscopy, magnetization transfer, diffusion, and blood flow. We summarize the findings on how each of these parameters varies with aging. Finally, we examine how the aforementioned techniques have been used for age prediction. While relatively large in scope, we present a comprehensive review that should provide the reader with sound understanding of what MRI has been able to tell us about how the healthy brain ages.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/KPNETWRP/MacDonald and Pike - 2021 - MRI of healthy brain aging A review.pdf}
}

@article{macdonaldMRIHealthyBrain2021a,
  title = {{{MRI}} of {{Healthy Brain Aging}}: {{A Review}}},
  shorttitle = {{{MRI}} of {{Healthy Brain Aging}}},
  author = {MacDonald, M. Ethan and Pike, G. Bruce},
  date = {2021-09},
  journaltitle = {NMR in Biomedicine},
  shortjournal = {NMR in Biomedicine},
  volume = {34},
  number = {9},
  pages = {e4564},
  issn = {0952-3480, 1099-1492},
  doi = {10.1002/nbm.4564},
  url = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/nbm.4564},
  urldate = {2024-05-17},
  abstract = {We present a review of the characterization of healthy brain aging using MRI with an emphasis on morphology, lesions, and quantitative MR parameters. A scope review found 6612 articles encompassing the keywords “Brain Aging” and “Magnetic Resonance”; papers involving functional MRI or not involving imaging of healthy human brain aging were discarded, leaving 2246 articles. We first consider some of the biogerontological mechanisms of aging, and the consequences of aging in terms of cognition and onset of disease. Morphological changes with aging are reviewed for the whole brain, cerebral cortex, white matter, subcortical gray matter, and other individual structures. In general, volume and cortical thickness decline with age, beginning in mid‐life. Prevalent silent lesions such as white matter hyperintensities, microbleeds, and lacunar infarcts are also observed with increasing frequency. The literature regarding quantitative MR parameter changes includes T 1 , T 2 , T 2 *, magnetic susceptibility, spectroscopy, magnetization transfer, diffusion, and blood flow. We summarize the findings on how each of these parameters varies with aging. Finally, we examine how the aforementioned techniques have been used for age prediction. While relatively large in scope, we present a comprehensive review that should provide the reader with sound understanding of what MRI has been able to tell us about how the healthy brain ages.},
  file = {/home/cristi/Zotero/storage/9PWIFJFH/MacDonald and Pike - 2021 - MRI of healthy brain aging A review.pdf}
}

@incollection{malmiercaAuditorySystem2012,
  title = {Auditory {{System}}},
  booktitle = {The {{Mouse Nervous System}}},
  author = {Malmierca, Manuel S. and Ryugo, David K.},
  date = {2012},
  pages = {607--645},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-12-369497-3.10024-X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B978012369497310024X},
  urldate = {2025-03-01},
  isbn = {978-0-12-369497-3},
  langid = {english},
  file = {/home/cristi/Zotero/storage/CF53PCPJ/Malmierca and Ryugo - 2012 - Auditory System.pdf}
}

@incollection{malmiercaAuditorySystem2012a,
  title = {Auditory {{System}}},
  booktitle = {The {{Mouse Nervous System}}},
  author = {Malmierca, Manuel S. and Ryugo, David K.},
  date = {2012},
  pages = {607--645},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-12-369497-3.10024-X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B978012369497310024X},
  urldate = {2025-03-01},
  isbn = {978-0-12-369497-3},
  file = {/home/cristi/Zotero/storage/5C6Y9YVV/Malmierca and Ryugo - 2012 - Auditory System.pdf}
}

@book{maurerAutonomousDriving2016,
  title = {Autonomous {{Driving}}},
  editor = {Maurer, Markus and Gerdes, J. Christian and Lenz, Barbara and Winner, Hermann},
  date = {2016},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-48847-8},
  url = {http://link.springer.com/10.1007/978-3-662-48847-8},
  urldate = {2024-01-07},
  isbn = {978-3-662-48845-4 978-3-662-48847-8},
  langid = {english},
  file = {/home/cristi/Zotero/storage/QQIBXKZD/Maurer et al. - 2016 - Autonomous Driving.pdf}
}

@book{maurerAutonomousDriving2016a,
  title = {Autonomous {{Driving}}},
  editor = {Maurer, Markus and Gerdes, J. Christian and Lenz, Barbara and Winner, Hermann},
  date = {2016},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-48847-8},
  url = {http://link.springer.com/10.1007/978-3-662-48847-8},
  urldate = {2024-01-07},
  isbn = {978-3-662-48845-4 978-3-662-48847-8},
  file = {/home/cristi/Zotero/storage/K78JNAGE/Maurer et al. - 2016 - Autonomous Driving.pdf}
}

@book{mckennaFreeWillContemporary2016,
  title = {Free {{Will}}: {{A Contemporary Introduction}}},
  shorttitle = {Free {{Will}}},
  author = {McKenna, Michael and Pereboom, Derk},
  date = {2016-07-01},
  edition = {1},
  publisher = {Routledge},
  location = {New York, NY: Routledge, 2016. | Series: Routledge contemporary introductions to philosophy},
  doi = {10.4324/9781315621548},
  url = {https://www.taylorfrancis.com/books/9781317220275},
  urldate = {2024-06-26},
  isbn = {978-1-315-62154-8},
  langid = {english}
}

@book{mckennaFreeWillContemporary2016a,
  title = {Free {{Will}}: {{A Contemporary Introduction}}},
  shorttitle = {Free {{Will}}},
  author = {McKenna, Michael and Pereboom, Derk},
  date = {2016-07-01},
  edition = {1},
  publisher = {Routledge},
  location = {New York, NY: Routledge, 2016. | Series: Routledge contemporary introductions to philosophy},
  doi = {10.4324/9781315621548},
  url = {https://www.taylorfrancis.com/books/9781317220275},
  urldate = {2024-06-26},
  isbn = {978-1-315-62154-8}
}

@article{mertesGANterfactualCounterfactualExplanations2022,
  title = {{{GANterfactual}}—{{Counterfactual Explanations}} for {{Medical Non-experts Using Generative Adversarial Learning}}},
  author = {Mertes, Silvan and Huber, Tobias and Weitz, Katharina and Heimerl, Alexander and André, Elisabeth},
  date = {2022-04-08},
  journaltitle = {Frontiers in Artificial Intelligence},
  shortjournal = {Front. Artif. Intell.},
  volume = {5},
  pages = {825565},
  issn = {2624-8212},
  doi = {10.3389/frai.2022.825565},
  url = {https://www.frontiersin.org/articles/10.3389/frai.2022.825565/full},
  urldate = {2025-03-07},
  abstract = {With the ongoing rise of machine learning, the need for methods for explaining decisions made by artificial intelligence systems is becoming a more and more important topic. Especially for image classification tasks, many state-of-the-art tools to explain such classifiers rely on visual highlighting of important areas of the input data. Contrary, counterfactual explanation systems try to enable a counterfactual reasoning by modifying the input image in a way such that the classifier would have made a different prediction. By doing so, the users of counterfactual explanation systems are equipped with a completely different kind of explanatory information. However, methods for generating realistic counterfactual explanations for image classifiers are still rare. Especially in medical contexts, where relevant information often consists of textural and structural information, high-quality counterfactual images have the potential to give meaningful insights into decision processes. In this work, we present               GANterfactual               , an approach to generate such counterfactual image explanations based on adversarial image-to-image translation techniques. Additionally, we conduct a user study to evaluate our approach in an exemplary medical use case. Our results show that, in the chosen medical use-case, counterfactual explanations lead to significantly better results regarding mental models, explanation satisfaction, trust, emotions, and self-efficacy than two state-of-the art systems that work with saliency maps, namely LIME and LRP.},
  file = {/home/cristi/Zotero/storage/X6JGCYRL/Mertes et al. - 2022 - GANterfactual—Counterfactual Explanations for Medical Non-experts Using Generative Adversarial Learn.pdf}
}

@article{mertesGANterfactualCounterfactualExplanations2022a,
  title = {{{GANterfactual}}—{{Counterfactual Explanations}} for {{Medical Non-experts Using Generative Adversarial Learning}}},
  author = {Mertes, Silvan and Huber, Tobias and Weitz, Katharina and Heimerl, Alexander and André, Elisabeth},
  date = {2022-04-08},
  journaltitle = {Frontiers in Artificial Intelligence},
  shortjournal = {Front. Artif. Intell.},
  volume = {5},
  pages = {825565},
  issn = {2624-8212},
  doi = {10.3389/frai.2022.825565},
  url = {https://www.frontiersin.org/articles/10.3389/frai.2022.825565/full},
  urldate = {2025-03-07},
  abstract = {With the ongoing rise of machine learning, the need for methods for explaining decisions made by artificial intelligence systems is becoming a more and more important topic. Especially for image classification tasks, many state-of-the-art tools to explain such classifiers rely on visual highlighting of important areas of the input data. Contrary, counterfactual explanation systems try to enable a counterfactual reasoning by modifying the input image in a way such that the classifier would have made a different prediction. By doing so, the users of counterfactual explanation systems are equipped with a completely different kind of explanatory information. However, methods for generating realistic counterfactual explanations for image classifiers are still rare. Especially in medical contexts, where relevant information often consists of textural and structural information, high-quality counterfactual images have the potential to give meaningful insights into decision processes. In this work, we present GANterfactual , an approach to generate such counterfactual image explanations based on adversarial image-to-image translation techniques. Additionally, we conduct a user study to evaluate our approach in an exemplary medical use case. Our results show that, in the chosen medical use-case, counterfactual explanations lead to significantly better results regarding mental models, explanation satisfaction, trust, emotions, and self-efficacy than two state-of-the art systems that work with saliency maps, namely LIME and LRP.},
  file = {/home/cristi/Zotero/storage/STU8KULU/Mertes et al. - 2022 - GANterfactual—Counterfactual Explanations for Medical Non-experts Using Generative Adversarial Learn.pdf}
}

@article{meyerModelsNeuronalStimulusResponse2017,
  title = {Models of {{Neuronal Stimulus-Response Functions}}: {{Elaboration}}, {{Estimation}}, and {{Evaluation}}},
  shorttitle = {Models of {{Neuronal Stimulus-Response Functions}}},
  author = {Meyer, Arne F. and Williamson, Ross S. and Linden, Jennifer F. and Sahani, Maneesh},
  date = {2017-01-12},
  journaltitle = {Frontiers in Systems Neuroscience},
  shortjournal = {Front. Syst. Neurosci.},
  volume = {10},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2016.00109},
  url = {http://journal.frontiersin.org/article/10.3389/fnsys.2016.00109/full},
  urldate = {2025-02-23},
  file = {/home/cristi/Zotero/storage/QMY7SXYD/Meyer et al. - 2017 - Models of Neuronal Stimulus-Response Functions Elaboration, Estimation, and Evaluation.pdf}
}

@article{meyerModelsNeuronalStimulusResponse2017a,
  title = {Models of {{Neuronal Stimulus-Response Functions}}: {{Elaboration}}, {{Estimation}}, and {{Evaluation}}},
  shorttitle = {Models of {{Neuronal Stimulus-Response Functions}}},
  author = {Meyer, Arne F. and Williamson, Ross S. and Linden, Jennifer F. and Sahani, Maneesh},
  date = {2017-01-12},
  journaltitle = {Frontiers in Systems Neuroscience},
  shortjournal = {Front. Syst. Neurosci.},
  volume = {10},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2016.00109},
  url = {http://journal.frontiersin.org/article/10.3389/fnsys.2016.00109/full},
  urldate = {2025-02-23},
  file = {/home/cristi/Zotero/storage/DATXS5AA/Meyer et al. - 2017 - Models of Neuronal Stimulus-Response Functions Elaboration, Estimation, and Evaluation.pdf}
}

@online{michaelisBenchmarkingRobustnessObject2020,
  title = {Benchmarking {{Robustness}} in {{Object Detection}}: {{Autonomous Driving}} When {{Winter}} Is {{Coming}}},
  shorttitle = {Benchmarking {{Robustness}} in {{Object Detection}}},
  author = {Michaelis, Claudio and Mitzkus, Benjamin and Geirhos, Robert and Rusak, Evgenia and Bringmann, Oliver and Ecker, Alexander S. and Bethge, Matthias and Brendel, Wieland},
  date = {2020-03-31},
  eprint = {1907.07484},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.07484},
  urldate = {2024-04-25},
  abstract = {The ability to detect objects regardless of image distortions or weather conditions is crucial for real-world applications of deep learning like autonomous driving. We here provide an easy-to-use benchmark to assess how object detection models perform when image quality degrades. The three resulting benchmark datasets, termed Pascal-C, Coco-C and Cityscapes-C, contain a large variety of image corruptions. We show that a range of standard object detection models suffer a severe performance loss on corrupted images (down to 30--60\textbackslash\% of the original performance). However, a simple data augmentation trick---stylizing the training images---leads to a substantial increase in robustness across corruption type, severity and dataset. We envision our comprehensive benchmark to track future progress towards building robust object detection models. Benchmark, code and data are publicly available.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/cristi/Zotero/storage/KV68TWHQ/Michaelis et al. - 2020 - Benchmarking Robustness in Object Detection Auton.pdf;/home/cristi/Zotero/storage/M3UMPBM4/1907.html}
}

@online{michaelisBenchmarkingRobustnessObject2020a,
  title = {Benchmarking {{Robustness}} in {{Object Detection}}: {{Autonomous Driving When Winter Is Coming}}},
  shorttitle = {Benchmarking {{Robustness}} in {{Object Detection}}},
  author = {Michaelis, Claudio and Mitzkus, Benjamin and Geirhos, Robert and Rusak, Evgenia and Bringmann, Oliver and Ecker, Alexander S. and Bethge, Matthias and Brendel, Wieland},
  date = {2020-03-31},
  eprint = {1907.07484},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1907.07484},
  urldate = {2024-04-25},
  abstract = {The ability to detect objects regardless of image distortions or weather conditions is crucial for real-world applications of deep learning like autonomous driving. We here provide an easy-to-use benchmark to assess how object detection models perform when image quality degrades. The three resulting benchmark datasets, termed Pascal-C, Coco-C and Cityscapes-C, contain a large variety of image corruptions. We show that a range of standard object detection models suffer a severe performance loss on corrupted images (down to 30–60\textbackslash textbackslash\% of the original performance). However, a simple data augmentation trick—stylizing the training images—leads to a substantial increase in robustness across corruption type, severity and dataset. We envision our comprehensive benchmark to track future progress towards building robust object detection models. Benchmark, code and data are publicly available.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/cristi/Zotero/storage/UPQP2FW7/Michaelis et al. - 2020 - Benchmarking Robustness in Object Detection Auton.pdf;/home/cristi/Zotero/storage/ZHMWS6MX/1907.html}
}

@article{mitchellDoesNeuroscienceLeave2018,
  title = {Does {{Neuroscience Leave Room}} for {{Free Will}}?},
  author = {Mitchell, Kevin J.},
  date = {2018-09},
  journaltitle = {Trends in Neurosciences},
  shortjournal = {Trends in Neurosciences},
  volume = {41},
  number = {9},
  pages = {573--576},
  issn = {01662236},
  doi = {10.1016/j.tins.2018.05.008},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0166223618301553},
  urldate = {2024-01-07},
  langid = {english},
  file = {/home/cristi/Zotero/storage/QHPCSKTT/Mitchell - 2018 - Does Neuroscience Leave Room for Free Will.pdf}
}

@article{mitchellDoesNeuroscienceLeave2018a,
  title = {Does {{Neuroscience Leave Room}} for {{Free Will}}?},
  author = {Mitchell, Kevin J.},
  date = {2018-09},
  journaltitle = {Trends in Neurosciences},
  shortjournal = {Trends in Neurosciences},
  volume = {41},
  number = {9},
  pages = {573--576},
  issn = {01662236},
  doi = {10.1016/j.tins.2018.05.008},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0166223618301553},
  urldate = {2024-01-07},
  file = {/home/cristi/Zotero/storage/J2CIQUWH/Mitchell - 2018 - Does Neuroscience Leave Room for Free Will.pdf}
}

@article{mittelstadtPrinciplesAloneCannot2019,
  title = {Principles Alone Cannot Guarantee Ethical {{AI}}},
  author = {Mittelstadt, Brent},
  date = {2019-11-04},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {1},
  number = {11},
  pages = {501--507},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0114-4},
  url = {https://www.nature.com/articles/s42256-019-0114-4},
  urldate = {2024-01-07},
  langid = {english},
  file = {/home/cristi/Zotero/storage/GAM2PE49/Mittelstadt - 2019 - Principles alone cannot guarantee ethical AI.pdf}
}

@article{mittelstadtPrinciplesAloneCannot2019a,
  title = {Principles {{Alone Cannot Guarantee Ethical AI}}},
  author = {Mittelstadt, Brent},
  date = {2019-11-04},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {1},
  number = {11},
  pages = {501--507},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0114-4},
  url = {https://www.nature.com/articles/s42256-019-0114-4},
  urldate = {2024-01-07},
  file = {/home/cristi/Zotero/storage/Y6R369KJ/Mittelstadt - 2019 - Principles alone cannot guarantee ethical AI.pdf}
}

@article{mocellaRevealingLettersRolled2015,
  title = {Revealing Letters in Rolled {{Herculaneum}} Papyri by {{X-ray}} Phase-Contrast Imaging},
  author = {Mocella, Vito and Brun, Emmanuel and Ferrero, Claudio and Delattre, Daniel},
  date = {2015-01-20},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {6},
  number = {1},
  pages = {5895},
  issn = {2041-1723},
  doi = {10.1038/ncomms6895},
  url = {https://www.nature.com/articles/ncomms6895},
  urldate = {2023-10-29},
  langid = {english},
  file = {/home/cristi/Zotero/storage/ALW82SMA/Mocella et al. - 2015 - Revealing letters in rolled Herculaneum papyri by .pdf}
}

@article{mocellaRevealingLettersRolled2015a,
  title = {Revealing {{Letters}} in {{Rolled Herculaneum Papyri}} by {{X-ray Phase-Contrast Imaging}}},
  author = {Mocella, Vito and Brun, Emmanuel and Ferrero, Claudio and Delattre, Daniel},
  date = {2015-01-20},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {6},
  number = {1},
  pages = {5895},
  issn = {2041-1723},
  doi = {10.1038/ncomms6895},
  url = {https://www.nature.com/articles/ncomms6895},
  urldate = {2023-10-29},
  file = {/home/cristi/Zotero/storage/873MCMNI/Mocella et al. - 2015 - Revealing letters in rolled Herculaneum papyri by .pdf}
}

@article{monahanSurveillanceCulturalPractice2011,
  title = {Surveillance as {{Cultural Practice}}},
  author = {Monahan, Torin},
  date = {2011-09},
  journaltitle = {The Sociological Quarterly},
  shortjournal = {The Sociological Quarterly},
  volume = {52},
  number = {4},
  pages = {495--508},
  issn = {0038-0253, 1533-8525},
  doi = {10.1111/j.1533-8525.2011.01216.x},
  url = {https://www.tandfonline.com/doi/full/10.1111/j.1533-8525.2011.01216.x},
  urldate = {2025-03-28},
  langid = {english}
}

@online{morcosImportanceSingleDirections2018,
  title = {On the Importance of Single Directions for Generalization},
  author = {Morcos, Ari S. and Barrett, David G. T. and Rabinowitz, Neil C. and Botvinick, Matthew},
  date = {2018-05-22},
  eprint = {1803.06959},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.06959},
  urldate = {2024-04-25},
  abstract = {Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/cristi/Zotero/storage/KCDARQL7/Morcos et al. - 2018 - On the importance of single directions for general.pdf;/home/cristi/Zotero/storage/EQPV4AUH/1803.html}
}

@online{morcosImportanceSingleDirections2018a,
  title = {On the {{Importance}} of {{Single Directions}} for {{Generalization}}},
  author = {Morcos, Ari S. and Barrett, David G. T. and Rabinowitz, Neil C. and Botvinick, Matthew},
  date = {2018-05-22},
  eprint = {1803.06959},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1803.06959},
  urldate = {2024-04-25},
  abstract = {Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/cristi/Zotero/storage/JRCSC43E/Morcos et al. - 2018 - On the importance of single directions for general.pdf;/home/cristi/Zotero/storage/I3484U6X/1803.html}
}

@article{muellerAcquisitionKoreanHonorific2013,
  title = {The {{Acquisition}} of the {{Korean Honorific Affix}} {\mkbibemph{(u)Si}} by {{Advanced L2 Learners}}},
  author = {Mueller, Jeansue and Jiang, Nan},
  date = {2013-06},
  journaltitle = {The Modern Language Journal},
  shortjournal = {Modern Language Journal},
  volume = {97},
  number = {2},
  pages = {318--339},
  issn = {00267902},
  doi = {10.1111/j.1540-4781.2013.12005.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1540-4781.2013.12005.x},
  urldate = {2023-02-19},
  langid = {english},
  file = {/home/cristi/Zotero/storage/ECPTHZNR/Mueller and Jiang - 2013 - The Acquisition of the Korean Honorific Affix (.pdf}
}

@article{muellerAcquisitionKoreanHonorific2013a,
  title = {The {{Acquisition}} of the {{Korean Honorific Affix}} \textbackslash mkbibemph(u){{Si}} by {{Advanced L2 Learners}}},
  author = {Mueller, Jeansue and Jiang, Nan},
  date = {2013-06},
  journaltitle = {The Modern Language Journal},
  shortjournal = {Modern Language Journal},
  volume = {97},
  number = {2},
  pages = {318--339},
  issn = {00267902},
  doi = {10.1111/j.1540-4781.2013.12005.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1540-4781.2013.12005.x},
  urldate = {2023-02-19},
  file = {/home/cristi/Zotero/storage/XMABDSGR/Mueller and Jiang - 2013 - The Acquisition of the Korean Honorific Affix (.pdf}
}

@book{myersonGameTheoryAnalysis2013,
  title = {Game {{Theory}}: {{Analysis}} of {{Conflict}}},
  shorttitle = {Game {{Theory}}},
  author = {Myerson, Roger B.},
  date = {2013-03-01},
  eprint = {10.2307/j.ctvjsf522},
  eprinttype = {jstor},
  publisher = {Harvard University Press},
  doi = {10.2307/j.ctvjsf522},
  url = {http://www.jstor.org/stable/10.2307/j.ctvjsf522},
  urldate = {2025-03-28},
  isbn = {978-0-674-72861-5 978-0-674-34116-6}
}

@article{NAJAFIAN202044,
  title = {Automatic Accent Identification as an Analytical Tool for Accent Robust Automatic Speech Recognition},
  author = {Najafian, Maryam and Russell, Martin},
  date = {2020},
  journaltitle = {Speech Communication},
  volume = {122},
  pages = {44--55},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2020.05.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639317300043},
  abstract = {We present a novel study of relationships between automatic accent identification (AID) and accent-robust automatic speech recognition (ASR), using i-vector based AID and deep neural network, hidden Markov Model (DNN-HMM) based ASR. A visualization of the AID i-vector space and a novel analysis of the accent content of the WSJCAM0 corpus are presented. Accents that occur at the periphery of AID space are referred to as “extreme”. We demonstrate a negative correlation, with respect to accent, between AID and ASR accuracy, where extreme accents exhibit the highest AID and lowest ASR performance. These relationships between accents inform a set of ASR experiments in which a generic training set (WSJCAM0) is supplemented with a fixed amount of accented data from the ABI (Accents of the British Isles) corpus. The best performance across all accents, a 32\% relative reduction in errors compared with the baseline ASR system, is obtained when the supplementary data comprises extreme accented speech, even though this accent accounts for just 14\% of the test data. We conclude that i-vector based AID analysis provides a principled approach to the selection of training material for accent robust ASR. We speculate that this may generalize to other detection technologies and other types of variability, such as Speaker Identification (SI) and speaker variability.},
  keywords = {Accent identification,British accents,I-vector,Speech recognition}
}

@article{NAJAFIAN202044,
  title = {Automatic Accent Identification as an Analytical Tool for Accent Robust Automatic Speech Recognition},
  author = {Najafian, Maryam and Russell, Martin},
  date = {2020},
  journaltitle = {Speech Communication},
  volume = {122},
  pages = {44--55},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2020.05.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639317300043},
  abstract = {We present a novel study of relationships between automatic accent identification (AID) and accent-robust automatic speech recognition (ASR), using i-vector based AID and deep neural network, hidden Markov Model (DNN-HMM) based ASR. A visualization of the AID i-vector space and a novel analysis of the accent content of the WSJCAM0 corpus are presented. Accents that occur at the periphery of AID space are referred to as “extreme”. We demonstrate a negative correlation, with respect to accent, between AID and ASR accuracy, where extreme accents exhibit the highest AID and lowest ASR performance. These relationships between accents inform a set of ASR experiments in which a generic training set (WSJCAM0) is supplemented with a fixed amount of accented data from the ABI (Accents of the British Isles) corpus. The best performance across all accents, a 32\% relative reduction in errors compared with the baseline ASR system, is obtained when the supplementary data comprises extreme accented speech, even though this accent accounts for just 14\% of the test data. We conclude that i-vector based AID analysis provides a principled approach to the selection of training material for accent robust ASR. We speculate that this may generalize to other detection technologies and other types of variability, such as Speaker Identification (SI) and speaker variability.},
  keywords = {Accent identification,British accents,I-vector,Speech recognition}
}

@article{NAJAFIAN202044,
  title = {Automatic Accent Identification as an Analytical Tool for Accent Robust Automatic Speech Recognition},
  author = {Najafian, Maryam and Russell, Martin},
  date = {2020},
  journaltitle = {Speech Communication},
  volume = {122},
  pages = {44--55},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2020.05.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639317300043},
  abstract = {We present a novel study of relationships between automatic accent identification (AID) and accent-robust automatic speech recognition (ASR), using i-vector based AID and deep neural network, hidden Markov Model (DNN-HMM) based ASR. A visualization of the AID i-vector space and a novel analysis of the accent content of the WSJCAM0 corpus are presented. Accents that occur at the periphery of AID space are referred to as “extreme”. We demonstrate a negative correlation, with respect to accent, between AID and ASR accuracy, where extreme accents exhibit the highest AID and lowest ASR performance. These relationships between accents inform a set of ASR experiments in which a generic training set (WSJCAM0) is supplemented with a fixed amount of accented data from the ABI (Accents of the British Isles) corpus. The best performance across all accents, a 32\% relative reduction in errors compared with the baseline ASR system, is obtained when the supplementary data comprises extreme accented speech, even though this accent accounts for just 14\% of the test data. We conclude that i-vector based AID analysis provides a principled approach to the selection of training material for accent robust ASR. We speculate that this may generalize to other detection technologies and other types of variability, such as Speaker Identification (SI) and speaker variability.},
  keywords = {Accent identification,British accents,I-vector,Speech recognition}
}

@article{NAJAFIAN202044,
  title = {Automatic Accent Identification as an Analytical Tool for Accent Robust Automatic Speech Recognition},
  author = {Najafian, Maryam and Russell, Martin},
  date = {2020},
  journaltitle = {Speech Communication},
  volume = {122},
  pages = {44--55},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2020.05.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639317300043},
  abstract = {We present a novel study of relationships between automatic accent identification (AID) and accent-robust automatic speech recognition (ASR), using i-vector based AID and deep neural network, hidden Markov Model (DNN-HMM) based ASR. A visualization of the AID i-vector space and a novel analysis of the accent content of the WSJCAM0 corpus are presented. Accents that occur at the periphery of AID space are referred to as “extreme”. We demonstrate a negative correlation, with respect to accent, between AID and ASR accuracy, where extreme accents exhibit the highest AID and lowest ASR performance. These relationships between accents inform a set of ASR experiments in which a generic training set (WSJCAM0) is supplemented with a fixed amount of accented data from the ABI (Accents of the British Isles) corpus. The best performance across all accents, a 32\% relative reduction in errors compared with the baseline ASR system, is obtained when the supplementary data comprises extreme accented speech, even though this accent accounts for just 14\% of the test data. We conclude that i-vector based AID analysis provides a principled approach to the selection of training material for accent robust ASR. We speculate that this may generalize to other detection technologies and other types of variability, such as Speaker Identification (SI) and speaker variability.},
  keywords = {Accent identification,British accents,I-vector,Speech recognition}
}

@article{najafianAutomaticAccentIdentification2020,
  title = {Automatic Accent Identification as an Analytical Tool for Accent Robust Automatic Speech Recognition},
  author = {Najafian, Maryam and Russell, Martin},
  date = {2020-09},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {122},
  pages = {44--55},
  issn = {01676393},
  doi = {10.1016/j.specom.2020.05.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639317300043},
  urldate = {2023-02-15},
  langid = {english},
  file = {/home/cristi/Zotero/storage/DQ4BTQSM/Najafian and Russell - 2020 - Automatic accent identification as an analytical t.pdf}
}

@article{najafianAutomaticAccentIdentification2020a,
  title = {Automatic {{Accent Identification}} as an {{Analytical Tool}} for {{Accent Robust Automatic Speech Recognition}}},
  author = {Najafian, Maryam and Russell, Martin},
  date = {2020-09},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {122},
  pages = {44--55},
  issn = {01676393},
  doi = {10.1016/j.specom.2020.05.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639317300043},
  urldate = {2023-02-15},
  file = {/home/cristi/Zotero/storage/QLHLPHQ2/Najafian and Russell - 2020 - Automatic accent identification as an analytical t.pdf}
}

@article{najafianAutomaticAccentIdentification2020b,
  title = {Automatic {{Accent Identification}} as an {{Analytical Tool}} for {{Accent Robust Automatic Speech Recognition}}},
  author = {Najafian, Maryam and Russell, Martin},
  date = {2020},
  journaltitle = {Speech Communication},
  volume = {122},
  pages = {44--55},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2020.05.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639317300043},
  abstract = {We present a novel study of relationships between automatic accent identification (AID) and accent-robust automatic speech recognition (ASR), using i-vector based AID and deep neural network, hidden Markov Model (DNN-HMM) based ASR. A visualization of the AID i-vector space and a novel analysis of the accent content of the WSJCAM0 corpus are presented. Accents that occur at the periphery of AID space are referred to as “extreme”. We demonstrate a negative correlation, with respect to accent, between AID and ASR accuracy, where extreme accents exhibit the highest AID and lowest ASR performance. These relationships between accents inform a set of ASR experiments in which a generic training set (WSJCAM0) is supplemented with a fixed amount of accented data from the ABI (Accents of the British Isles) corpus. The best performance across all accents, a 32\% relative reduction in errors compared with the baseline ASR system, is obtained when the supplementary data comprises extreme accented speech, even though this accent accounts for just 14\% of the test data. We conclude that i-vector based AID analysis provides a principled approach to the selection of training material for accent robust ASR. We speculate that this may generalize to other detection technologies and other types of variability, such as Speaker Identification (SI) and speaker variability.},
  keywords = {Accent identification,British accents,I-vector,Speech recognition}
}

@article{najafianAutomaticAccentIdentification2020c,
  title = {Automatic {{Accent Identification}} as an {{Analytical Tool}} for {{Accent Robust Automatic Speech Recognition}}},
  author = {Najafian, Maryam and Russell, Martin},
  date = {2020},
  journaltitle = {Speech Communication},
  volume = {122},
  pages = {44--55},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2020.05.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639317300043},
  abstract = {We present a novel study of relationships between automatic accent identification (AID) and accent-robust automatic speech recognition (ASR), using i-vector based AID and deep neural network, hidden Markov Model (DNN-HMM) based ASR. A visualization of the AID i-vector space and a novel analysis of the accent content of the WSJCAM0 corpus are presented. Accents that occur at the periphery of AID space are referred to as “extreme”. We demonstrate a negative correlation, with respect to accent, between AID and ASR accuracy, where extreme accents exhibit the highest AID and lowest ASR performance. These relationships between accents inform a set of ASR experiments in which a generic training set (WSJCAM0) is supplemented with a fixed amount of accented data from the ABI (Accents of the British Isles) corpus. The best performance across all accents, a 32\% relative reduction in errors compared with the baseline ASR system, is obtained when the supplementary data comprises extreme accented speech, even though this accent accounts for just 14\% of the test data. We conclude that i-vector based AID analysis provides a principled approach to the selection of training material for accent robust ASR. We speculate that this may generalize to other detection technologies and other types of variability, such as Speaker Identification (SI) and speaker variability.},
  keywords = {Accent identification,British accents,I-vector,Speech recognition}
}

@article{najafianAutomaticAccentIdentification2020d,
  title = {Automatic {{Accent Identification}} as an {{Analytical Tool}} for {{Accent Robust Automatic Speech Recognition}}},
  author = {Najafian, Maryam and Russell, Martin},
  date = {2020},
  journaltitle = {Speech Communication},
  volume = {122},
  pages = {44--55},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2020.05.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639317300043},
  abstract = {We present a novel study of relationships between automatic accent identification (AID) and accent-robust automatic speech recognition (ASR), using i-vector based AID and deep neural network, hidden Markov Model (DNN-HMM) based ASR. A visualization of the AID i-vector space and a novel analysis of the accent content of the WSJCAM0 corpus are presented. Accents that occur at the periphery of AID space are referred to as “extreme”. We demonstrate a negative correlation, with respect to accent, between AID and ASR accuracy, where extreme accents exhibit the highest AID and lowest ASR performance. These relationships between accents inform a set of ASR experiments in which a generic training set (WSJCAM0) is supplemented with a fixed amount of accented data from the ABI (Accents of the British Isles) corpus. The best performance across all accents, a 32\% relative reduction in errors compared with the baseline ASR system, is obtained when the supplementary data comprises extreme accented speech, even though this accent accounts for just 14\% of the test data. We conclude that i-vector based AID analysis provides a principled approach to the selection of training material for accent robust ASR. We speculate that this may generalize to other detection technologies and other types of variability, such as Speaker Identification (SI) and speaker variability.},
  keywords = {Accent identification,British accents,I-vector,Speech recognition}
}

@article{najafianAutomaticAccentIdentification2020e,
  title = {Automatic {{Accent Identification}} as an {{Analytical Tool}} for {{Accent Robust Automatic Speech Recognition}}},
  author = {Najafian, Maryam and Russell, Martin},
  date = {2020},
  journaltitle = {Speech Communication},
  volume = {122},
  pages = {44--55},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2020.05.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639317300043},
  abstract = {We present a novel study of relationships between automatic accent identification (AID) and accent-robust automatic speech recognition (ASR), using i-vector based AID and deep neural network, hidden Markov Model (DNN-HMM) based ASR. A visualization of the AID i-vector space and a novel analysis of the accent content of the WSJCAM0 corpus are presented. Accents that occur at the periphery of AID space are referred to as “extreme”. We demonstrate a negative correlation, with respect to accent, between AID and ASR accuracy, where extreme accents exhibit the highest AID and lowest ASR performance. These relationships between accents inform a set of ASR experiments in which a generic training set (WSJCAM0) is supplemented with a fixed amount of accented data from the ABI (Accents of the British Isles) corpus. The best performance across all accents, a 32\% relative reduction in errors compared with the baseline ASR system, is obtained when the supplementary data comprises extreme accented speech, even though this accent accounts for just 14\% of the test data. We conclude that i-vector based AID analysis provides a principled approach to the selection of training material for accent robust ASR. We speculate that this may generalize to other detection technologies and other types of variability, such as Speaker Identification (SI) and speaker variability.},
  keywords = {Accent identification,British accents,I-vector,Speech recognition}
}

@article{nassifSpeechRecognitionUsing2019,
  title = {Speech {{Recognition Using Deep Neural Networks}}: {{A Systematic Review}}},
  shorttitle = {Speech {{Recognition Using Deep Neural Networks}}},
  author = {Nassif, Ali Bou and Shahin, Ismail and Attili, Imtinan and Azzeh, Mohammad and Shaalan, Khaled},
  date = {2019},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {7},
  pages = {19143--19165},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2896880},
  url = {https://ieeexplore.ieee.org/document/8632885/},
  urldate = {2023-02-15},
  file = {/home/cristi/Zotero/storage/IXC47I2B/Nassif et al. - 2019 - Speech Recognition Using Deep Neural Networks A S.pdf}
}

@article{nassifSpeechRecognitionUsing2019a,
  title = {Speech {{Recognition Using Deep Neural Networks}}: {{A Systematic Review}}},
  shorttitle = {Speech {{Recognition Using Deep Neural Networks}}},
  author = {Nassif, Ali Bou and Shahin, Ismail and Attili, Imtinan and Azzeh, Mohammad and Shaalan, Khaled},
  date = {2019},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {7},
  pages = {19143--19165},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2896880},
  url = {https://ieeexplore.ieee.org/document/8632885/},
  urldate = {2023-02-15},
  file = {/home/cristi/Zotero/storage/ECIU43VH/Nassif et al. - 2019 - Speech Recognition Using Deep Neural Networks A S.pdf}
}

@online{NeurotechnologyHealthcareRadboud,
  title = {Neurotechnology and {{Healthcare}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-intelligent-technology/neurotechnology-and-healthcare},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/Z67MLAX7/neurotechnology-and-healthcare.html}
}

@online{NeurotechnologyHealthcareRadbouda,
  title = {Neurotechnology and {{Healthcare}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-intelligent-technology/neurotechnology-and-healthcare},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/KZ23XHDD/neurotechnology-and-healthcare.html}
}

@inproceedings{NIPS2017_3f5ee243,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  file = {/home/cristi/Zotero/storage/HIMUJSBN/Vaswani et al. - 2017 - Attention is all you need.pdf}
}

@book{nobleAlgorithmsOppressionHow2020,
  title = {Algorithms of {{Oppression}}: {{How Search Engines Reinforce Racism}}},
  shorttitle = {Algorithms of {{Oppression}}},
  author = {Noble, Safiya Umoja},
  date = {2020-12-31},
  publisher = {New York University Press},
  doi = {10.18574/nyu/9781479833641.001.0001},
  url = {https://www.degruyter.com/document/doi/10.18574/nyu/9781479833641.001.0001/html},
  urldate = {2025-03-28},
  isbn = {978-1-4798-3364-1}
}

@article{northcottFreeWillNot2019,
  title = {Free {{Will}} Is {{Not}} a {{Testable Hypothesis}}},
  author = {Northcott, Robert},
  date = {2019-06},
  journaltitle = {Erkenntnis},
  shortjournal = {Erkenn},
  volume = {84},
  number = {3},
  pages = {617--631},
  issn = {0165-0106, 1572-8420},
  doi = {10.1007/s10670-018-9974-y},
  url = {http://link.springer.com/10.1007/s10670-018-9974-y},
  urldate = {2024-06-24},
  langid = {english},
  file = {/home/cristi/Zotero/storage/9M672X8Q/Northcott - 2019 - Free Will is Not a Testable Hypothesis.pdf}
}

@article{northcottFreeWillNot2019a,
  title = {Free {{Will Is Not}} a {{Testable Hypothesis}}},
  author = {Northcott, Robert},
  date = {2019-06},
  journaltitle = {Erkenntnis},
  shortjournal = {Erkenn},
  volume = {84},
  number = {3},
  pages = {617--631},
  issn = {0165-0106, 1572-8420},
  doi = {10.1007/s10670-018-9974-y},
  url = {http://link.springer.com/10.1007/s10670-018-9974-y},
  urldate = {2024-06-24},
  file = {/home/cristi/Zotero/storage/43EKWYHK/Northcott - 2019 - Free Will is Not a Testable Hypothesis.pdf}
}

@article{novakovskyObtainingGeneticsInsights2023,
  title = {Obtaining Genetics Insights from Deep Learning via Explainable Artificial Intelligence},
  author = {Novakovsky, Gherman and Dexter, Nick and Libbrecht, Maxwell W. and Wasserman, Wyeth W. and Mostafavi, Sara},
  date = {2023-02},
  journaltitle = {Nature Reviews Genetics},
  shortjournal = {Nat Rev Genet},
  volume = {24},
  number = {2},
  pages = {125--137},
  issn = {1471-0056, 1471-0064},
  doi = {10.1038/s41576-022-00532-2},
  url = {https://www.nature.com/articles/s41576-022-00532-2},
  urldate = {2025-01-31},
  langid = {english},
  file = {/home/cristi/Zotero/storage/KFB7FNEI/Novakovsky et al. - 2023 - Obtaining genetics insights from deep learning via explainable artificial intelligence.pdf}
}

@article{novakovskyObtainingGeneticsInsights2023a,
  title = {Obtaining {{Genetics Insights}} from {{Deep Learning}} via {{Explainable Artificial Intelligence}}},
  author = {Novakovsky, Gherman and Dexter, Nick and Libbrecht, Maxwell W. and Wasserman, Wyeth W. and Mostafavi, Sara},
  date = {2023-02},
  journaltitle = {Nature Reviews Genetics},
  shortjournal = {Nat Rev Genet},
  volume = {24},
  number = {2},
  pages = {125--137},
  issn = {1471-0056, 1471-0064},
  doi = {10.1038/s41576-022-00532-2},
  url = {https://www.nature.com/articles/s41576-022-00532-2},
  urldate = {2025-01-31},
  file = {/home/cristi/Zotero/storage/2LPILR8X/Novakovsky et al. - 2023 - Obtaining genetics insights from deep learning via explainable artificial intelligence.pdf}
}

@inbook{oconnorWhatFreeWill2022,
  title = {What Is Free Will?},
  booktitle = {Free {{Will}}},
  author = {O’Connor, Timothy},
  date = {2022-02-23},
  pages = {41--48},
  publisher = {Oxford University Press},
  doi = {10.1093/oso/9780197572153.003.0005},
  url = {https://academic.oup.com/book/38928/chapter/338106716},
  urldate = {2024-06-26},
  abstract = {This chapter explains the concept of free will and then explores a number of conceptual and empirical puzzles to which it gives rise: Is it compatible with causal determinism, with non-deterministic “chance,” or with the apparent fact that much human action is automated rather than being consciously controlled? Does it admit of degrees? On what basis might we distinguish freedom-diminishing causal influences from neutral or freedom-enhancing influences? And what evidence could establish that we ever exert “top-down” control over the sub-personal neural processes that underlie all human choice and action?},
  bookauthor = {O’Connor, Timothy},
  isbn = {978-0-19-757215-3 978-0-19-757219-1},
  langid = {english}
}

@incollection{oconnorWhatFreeWill2022a,
  title = {What {{Is Free Will}}?},
  booktitle = {Free {{Will}}},
  author = {O’Connor, Timothy},
  date = {2022-02-23},
  pages = {41--48},
  publisher = {Oxford University Press},
  doi = {10.1093/oso/9780197572153.003.0005},
  url = {https://academic.oup.com/book/38928/chapter/338106716},
  urldate = {2024-06-26},
  abstract = {This chapter explains the concept of free will and then explores a number of conceptual and empirical puzzles to which it gives rise: Is it compatible with causal determinism, with non-deterministic “chance,” or with the apparent fact that much human action is automated rather than being consciously controlled? Does it admit of degrees? On what basis might we distinguish freedom-diminishing causal influences from neutral or freedom-enhancing influences? And what evidence could establish that we ever exert “top-down” control over the sub-personal neural processes that underlie all human choice and action?},
  isbn = {978-0-19-757215-3 978-0-19-757219-1}
}

@online{OldEnglish,
  title = {Old {{English}}},
  publisher = {The British Library},
  url = {https://www.bl.uk/medieval-literature/articles/old-english},
  urldate = {2022-04-16},
  abstract = {David Crystal charts the evolution of Old English through the 700 years during which it was written and spoken.},
  langid = {english},
  organization = {The British Library},
  file = {/home/cristi/Zotero/storage/ACYE6WV4/old-english.html}
}

@inreference{OldEnglish2022,
  title = {Old {{English}}},
  booktitle = {Wikipedia},
  date = {2022-04-12T14:52:07Z},
  url = {https://en.wikipedia.org/w/index.php?title=Old_English&oldid=1082323694},
  urldate = {2022-04-16},
  abstract = {Old English (Englisċ, pronounced [ˈeŋɡliʃ]), or Anglo-Saxon, is the earliest recorded form of the English language, spoken in England and southern and eastern Scotland in the early Middle Ages. It was brought to Great Britain by Anglo-Saxon settlers in the mid-5th century, and the first Old English literary works date from the mid-7th century. After the Norman conquest of 1066, English was replaced, for a time, by Anglo-Norman (a relative of French) as the language of the upper classes. This is regarded as marking the end of the Old English era, since during this period the English language was heavily influenced by Anglo-Norman, developing into a phase known now as Middle English in England and Early Scots in Scotland. Old English developed from a set of Anglo-Frisian or Ingvaeonic dialects originally spoken by Germanic tribes traditionally known as the Angles, Saxons and Jutes. As the Germanic settlers became dominant in England, their language replaced the languages of Roman Britain: Common Brittonic, a Celtic language; and Latin, brought to Britain by Roman invasion. Old English had four main dialects, associated with particular Anglo-Saxon kingdoms: Mercian, Northumbrian, Kentish and West Saxon. It was West Saxon that formed the basis for the literary standard of the later Old English period, although the dominant forms of Middle and Modern English would develop mainly from Mercian, and Scots from Northumbrian. The speech of eastern and northern parts of England was subject to strong Old Norse influence due to Scandinavian rule and settlement beginning in the 9th century. Old English is one of the West Germanic languages, and its closest relatives are Old Frisian and Old Saxon. Like other old Germanic languages, it is very different from Modern English and Modern Scots, and largely incomprehensible for Modern English or Modern Scots speakers without study.  Within Old English grammar nouns, adjectives, pronouns and verbs have many inflectional endings and forms, and word order is much freer. The oldest Old English inscriptions were written using a runic system, but from about the 8th century this was replaced by a version of the Latin alphabet.},
  langid = {english},
  annotation = {Page Version ID: 1082323694},
  file = {/home/cristi/Zotero/storage/AF4CGFM6/Old_English.html}
}

@inreference{OldEnglish2022a,
  title = {Old {{English}}},
  booktitle = {Wikipedia},
  date = {2022-04-12T14:52:07Z},
  url = {https://en.wikipedia.org/w/index.php?title=Old_English&oldid=1082323694},
  urldate = {2022-04-16},
  abstract = {Old English (Englisċ, pronounced [ˈeŋɡliʃ]), or Anglo-Saxon, is the earliest recorded form of the English language, spoken in England and southern and eastern Scotland in the early Middle Ages. It was brought to Great Britain by Anglo-Saxon settlers in the mid-5th century, and the first Old English literary works date from the mid-7th century. After the Norman conquest of 1066, English was replaced, for a time, by Anglo-Norman (a relative of French) as the language of the upper classes. This is regarded as marking the end of the Old English era, since during this period the English language was heavily influenced by Anglo-Norman, developing into a phase known now as Middle English in England and Early Scots in Scotland. Old English developed from a set of Anglo-Frisian or Ingvaeonic dialects originally spoken by Germanic tribes traditionally known as the Angles, Saxons and Jutes. As the Germanic settlers became dominant in England, their language replaced the languages of Roman Britain: Common Brittonic, a Celtic language; and Latin, brought to Britain by Roman invasion. Old English had four main dialects, associated with particular Anglo-Saxon kingdoms: Mercian, Northumbrian, Kentish and West Saxon. It was West Saxon that formed the basis for the literary standard of the later Old English period, although the dominant forms of Middle and Modern English would develop mainly from Mercian, and Scots from Northumbrian. The speech of eastern and northern parts of England was subject to strong Old Norse influence due to Scandinavian rule and settlement beginning in the 9th century. Old English is one of the West Germanic languages, and its closest relatives are Old Frisian and Old Saxon. Like other old Germanic languages, it is very different from Modern English and Modern Scots, and largely incomprehensible for Modern English or Modern Scots speakers without study. Within Old English grammar nouns, adjectives, pronouns and verbs have many inflectional endings and forms, and word order is much freer. The oldest Old English inscriptions were written using a runic system, but from about the 8th century this was replaced by a version of the Latin alphabet.},
  file = {/home/cristi/Zotero/storage/5S2ACC7I/Old_English.html}
}

@online{OldEnglisha,
  title = {Old {{English}}},
  publisher = {The British Library},
  url = {https://www.bl.uk/medieval-literature/articles/old-english},
  urldate = {2022-04-16},
  abstract = {David Crystal charts the evolution of Old English through the 700 years during which it was written and spoken.},
  annotation = {Backup Publisher: The British Library},
  file = {/home/cristi/Zotero/storage/YR2KXRHJ/old-english.html}
}

@book{papadakisCurrentMedicalDiagnosis2024,
  title = {Current Medical Diagnosis \& Treatment 2024},
  editor = {Papadakis, Maxine A. and McPhee, Stephen J. and Rabow, Michael W. and McQuaid, Kenneth R. and Gandhi, Monica},
  date = {2024},
  edition = {Sixty-third edition},
  publisher = {McGraw Hill},
  location = {New York, NY},
  abstract = {Current Medical Diagnosis \& Treatment 2024 reflects the latest developments in medicine, guidelines, references, and more. It has authoritative, evidence-based coverage of more than 1,000 diseases and disorders along with a concise, yet thorough synopsis of diagnosis and treatment. Current Medical Diagnosis \& Treatment 2024 features: A comprehensive approach to patient care, focusing on the diagnostic tools relevant to daily practice; coverage of more than 1,000 diseases and disorders; hundreds of drug treatment tables for quick access to indexed trade names; annual updates to topics in all chapters in a consistent format},
  isbn = {978-1-265-55624-2},
  langid = {english},
  annotation = {OCLC: 1380382399}
}

@article{parasuramanHumansAutomationUse1997,
  title = {Humans and {{Automation}}: {{Use}}, {{Misuse}}, {{Disuse}}, {{Abuse}}},
  shorttitle = {Humans and {{Automation}}},
  author = {Parasuraman, Raja and Riley, Victor},
  date = {1997-06},
  journaltitle = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
  shortjournal = {Hum Factors},
  volume = {39},
  number = {2},
  pages = {230--253},
  issn = {0018-7208, 1547-8181},
  doi = {10.1518/001872097778543886},
  url = {https://journals.sagepub.com/doi/10.1518/001872097778543886},
  urldate = {2025-03-28},
  abstract = {This paper addresses theoretical, empirical, and analytical studies pertaining to human use, misuse, disuse, and abuse of automation technology. Use refers to the voluntary activation or disengagement of automation by human operators. Trust, mental workload, and risk can influence automation use, but interactions between factors and large individual differences make prediction of automation use difficult. Misuse refers to over reliance on automation, which can result in failures of monitoring or decision biases. Factors affecting the monitoring of automation include workload, automation reliability and consistency, and the saliency of automation state indicators. Disuse, or the neglect or underutilization of automation, is commonly caused by alarms that activate falsely. This often occurs because the base rate of the condition to be detected is not considered in setting the trade-off between false alarms and omissions. Automation abuse, or the automation of functions by designers and implementation by managers without due regard for the consequences for human performance, tends to define the operator's roles as by-products of the automation. Automation abuse can also promote misuse and disuse of automation by human operators. Understanding the factors associated with each of these aspects of human use of automation can lead to improved system design, effective training methods, and judicious policies and procedures involving automation use.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/YIW43HH9/Parasuraman and Riley - 1997 - Humans and Automation Use, Misuse, Disuse, Abuse.pdf}
}

@article{pastoreSignalDetectionTheory1974,
  title = {Signal Detection Theory: {{Considerations}} for General Application.},
  shorttitle = {Signal Detection Theory},
  author = {Pastore, R. E. and Scheirer, C. J.},
  date = {1974-12},
  journaltitle = {Psychological Bulletin},
  shortjournal = {Psychological Bulletin},
  volume = {81},
  number = {12},
  pages = {945--958},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/h0037357},
  url = {https://doi.apa.org/doi/10.1037/h0037357},
  urldate = {2025-03-28},
  langid = {english},
  file = {/home/cristi/Zotero/storage/G9A3C7VG/Pastore and Scheirer - 1974 - Signal detection theory Considerations for general application..pdf}
}

@book{pierceGeneticsConceptualApproach2020,
  title = {Genetics: A Conceptual Approach},
  shorttitle = {Genetics},
  author = {Pierce, Benjamin A.},
  date = {2020},
  edition = {Seventh edition},
  publisher = {Macmillan Learning},
  location = {Austin},
  isbn = {978-1-319-21680-1},
  pagetotal = {1},
  keywords = {Genetics},
  file = {/home/cristi/Zotero/storage/YRHYE737/Pierce - 2020 - Genetics a conceptual approach.pdf}
}

@book{pierceGeneticsConceptualApproach2020a,
  title = {Genetics: {{A Conceptual Approach}}},
  shorttitle = {Genetics},
  author = {Pierce, Benjamin A.},
  date = {2020},
  edition = {Seventh edition},
  publisher = {Macmillan Learning},
  location = {Austin},
  isbn = {978-1-319-21680-1},
  pagetotal = {1},
  keywords = {Genetics},
  file = {/home/cristi/Zotero/storage/HSPYXC6C/Pierce - 2020 - Genetics a conceptual approach.pdf}
}

@article{pnevmatikakisSimultaneousDenoisingDeconvolution2016,
  title = {Simultaneous {{Denoising}}, {{Deconvolution}}, and {{Demixing}} of {{Calcium Imaging Data}}},
  author = {Pnevmatikakis, Eftychios~A. and Soudry, Daniel and Gao, Yuanjun and Machado, Timothy A. and Merel, Josh and Pfau, David and Reardon, Thomas and Mu, Yu and Lacefield, Clay and Yang, Weijian and Ahrens, Misha and Bruno, Randy and Jessell, Thomas M. and Peterka, Darcy~S. and Yuste, Rafael and Paninski, Liam},
  date = {2016-01},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {89},
  number = {2},
  pages = {285--299},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.11.037},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627315010843},
  urldate = {2025-03-06},
  langid = {english},
  file = {/home/cristi/Zotero/storage/NK8EK4VY/Pnevmatikakis et al. - 2016 - Simultaneous Denoising, Deconvolution, and Demixing of Calcium Imaging Data.pdf}
}

@article{pnevmatikakisSimultaneousDenoisingDeconvolution2016a,
  title = {Simultaneous {{Denoising}}, {{Deconvolution}}, and {{Demixing}} of {{Calcium Imaging Data}}},
  author = {Pnevmatikakis, Eftychios A. and Soudry, Daniel and Gao, Yuanjun and Machado, Timothy A. and Merel, Josh and Pfau, David and Reardon, Thomas and Mu, Yu and Lacefield, Clay and Yang, Weijian and Ahrens, Misha and Bruno, Randy and Jessell, Thomas M. and Peterka, Darcy S. and Yuste, Rafael and Paninski, Liam},
  date = {2016-01},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {89},
  number = {2},
  pages = {285--299},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.11.037},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627315010843},
  urldate = {2025-03-06},
  file = {/home/cristi/Zotero/storage/AWABKX24/Pnevmatikakis et al. - 2016 - Simultaneous Denoising, Deconvolution, and Demixing of Calcium Imaging Data.pdf}
}

@article{podoletzWeHaveTalk2023,
  title = {We Have to Talk about Emotional {{AI}} and Crime},
  author = {Podoletz, Lena},
  date = {2023-06},
  journaltitle = {AI \& SOCIETY},
  shortjournal = {AI \& Soc},
  volume = {38},
  number = {3},
  pages = {1067--1082},
  issn = {0951-5666, 1435-5655},
  doi = {10.1007/s00146-022-01435-w},
  url = {https://link.springer.com/10.1007/s00146-022-01435-w},
  urldate = {2025-03-24},
  abstract = {Abstract             Emotional AI is an emerging technology used to make probabilistic predictions about the emotional states of people using data sources, such as facial (micro)-movements, body language, vocal tone or the choice of words. The performance of such systems is heavily debated and so are the underlying scientific methods that serve as the basis for many such technologies. In this article I will engage with this new technology, and with the debates and literature that surround it. Working at the intersection of criminology, policing, surveillance and the study of emotional AI this paper explores and offers a framework of understanding the various issues that these technologies present particularly to liberal democracies. I argue that these technologies should not be deployed within public spaces because there is only a very weak evidence-base as to their effectiveness in a policing and security context, and even more importantly represent a major intrusion to people’s private lives and also represent a worrying extension of policing power because of the possibility that intentions and attitudes may be inferred. Further to this, the danger in the use of such invasive surveillance for the purpose of policing and crime prevention in urban spaces is that it potentially leads to a highly regulated and control-oriented society. I argue that emotion recognition has severe impacts on the right to the city by not only undertaking surveillance of existing situations but also making inferences and probabilistic predictions about future events as well as emotions and intentions.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/RMMSZI74/Podoletz - 2023 - We have to talk about emotional AI and crime.pdf}
}

@article{pontiModelingLanguageVariation2019,
  title = {Modeling {{Language Variation}} and {{Universals}}: {{A Survey}} on {{Typological Linguistics}} for {{Natural Language Processing}}},
  shorttitle = {Modeling {{Language Variation}} and {{Universals}}},
  author = {Ponti, Edoardo Maria and O’Horan, Helen and Berzak, Yevgeni and Vulić, Ivan and Reichart, Roi and Poibeau, Thierry and Shutova, Ekaterina and Korhonen, Anna},
  date = {2019-09},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  volume = {45},
  number = {3},
  pages = {559--601},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00357},
  url = {https://direct.mit.edu/coli/article/45/3/559-601/93372},
  urldate = {2023-02-12},
  abstract = {Linguistic typology aims to capture structural and semantic variation across the world’s languages. A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-utilization of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP. In particular, we suggest that such an approach could be facilitated by recent developments in data-driven induction of typological knowledge.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/RDYJWDQI/Ponti et al. - 2019 - Modeling Language Variation and Universals A Surv.pdf}
}

@article{pontiModelingLanguageVariation2019a,
  title = {Modeling {{Language Variation}} and {{Universals}}: {{A Survey}} on {{Typological Linguistics}} for {{Natural Language Processing}}},
  shorttitle = {Modeling {{Language Variation}} and {{Universals}}},
  author = {Ponti, Edoardo Maria and O’Horan, Helen and Berzak, Yevgeni and Vulić, Ivan and Reichart, Roi and Poibeau, Thierry and Shutova, Ekaterina and Korhonen, Anna},
  date = {2019-09},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  volume = {45},
  number = {3},
  pages = {559--601},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00357},
  url = {https://direct.mit.edu/coli/article/45/3/559-601/93372},
  urldate = {2023-02-12},
  abstract = {Linguistic typology aims to capture structural and semantic variation across the world’s languages. A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-utilization of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP. In particular, we suggest that such an approach could be facilitated by recent developments in data-driven induction of typological knowledge.},
  file = {/home/cristi/Zotero/storage/3KMS3X5W/Ponti et al. - 2019 - Modeling Language Variation and Universals A Surv.pdf}
}

@book{putnamCambridgeHandbookGermanic2020,
  title = {The {{Cambridge}} Handbook of {{Germanic}} Linguistics},
  author = {Putnam, Michael T and Page, B. Richard},
  date = {2020},
  publisher = {Cambridge University Press},
  url = {https://doi.org/10.1017/9781108378291},
  urldate = {2022-04-17},
  abstract = {The Germanic language family ranges from national languages with standardized varieties, including German, Dutch and Danish, to minority languages with relatively few speakers, such as Frisian, Yiddish and Pennsylvania German. Written by internationally renowned experts of Germanic linguistics, this Handbook provides a detailed overview and analysis of the structure of modern Germanic languages and dialects. Organized thematically, it addresses key topics in the phonology, morphology, syntax, and semantics of standard and nonstandard varieties of Germanic languages from a comparative perspective. It also includes chapters on second language acquisition, heritage and minority languages, pidgins, and urban vernaculars. The first comprehensive survey of this vast topic, the Handbook is a vital resource for students and researchers investigating the Germanic family of languages and dialects.},
  isbn = {978-1-108-37829-1},
  langid = {english},
  annotation = {OCLC: 1153979007}
}

@book{putnamCambridgeHandbookGermanic2020a,
  title = {The {{Cambridge Handbook}} of {{Germanic Linguistics}}},
  author = {Putnam, Michael T and Page, B. Richard},
  date = {2020},
  publisher = {Cambridge University Press},
  url = {https://doi.org/10.1017/9781108378291},
  urldate = {2022-04-17},
  abstract = {The Germanic language family ranges from national languages with standardized varieties, including German, Dutch and Danish, to minority languages with relatively few speakers, such as Frisian, Yiddish and Pennsylvania German. Written by internationally renowned experts of Germanic linguistics, this Handbook provides a detailed overview and analysis of the structure of modern Germanic languages and dialects. Organized thematically, it addresses key topics in the phonology, morphology, syntax, and semantics of standard and nonstandard varieties of Germanic languages from a comparative perspective. It also includes chapters on second language acquisition, heritage and minority languages, pidgins, and urban vernaculars. The first comprehensive survey of this vast topic, the Handbook is a vital resource for students and researchers investigating the Germanic family of languages and dialects.},
  isbn = {978-1-108-37829-1}
}

@article{qianVeryDeepConvolutional2016,
  title = {Very {{Deep Convolutional Neural Networks}} for {{Noise Robust Speech Recognition}}},
  author = {Qian, Yanmin and Bi, Mengxiao and Tan, Tian and Yu, Kai},
  date = {2016-12},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {24},
  number = {12},
  pages = {2263--2276},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2016.2602884},
  url = {http://ieeexplore.ieee.org/document/7552554/},
  urldate = {2023-02-17},
  file = {/home/cristi/Zotero/storage/5CD7L8CW/Qian et al. - 2016 - Very Deep Convolutional Neural Networks for Noise .pdf}
}

@article{qianVeryDeepConvolutional2016a,
  title = {Very {{Deep Convolutional Neural Networks}} for {{Noise Robust Speech Recognition}}},
  author = {Qian, Yanmin and Bi, Mengxiao and Tan, Tian and Yu, Kai},
  date = {2016-12},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {24},
  number = {12},
  pages = {2263--2276},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2016.2602884},
  url = {http://ieeexplore.ieee.org/document/7552554/},
  urldate = {2023-02-17},
  file = {/home/cristi/Zotero/storage/A5FVK22E/Qian et al. - 2016 - Very Deep Convolutional Neural Networks for Noise .pdf}
}

@book{radfordLinguisticsIntroduction1999,
  title = {Linguistics: An Introduction},
  shorttitle = {Linguistics},
  editor = {Radford, Andrew},
  date = {1999},
  publisher = {Cambridge University Press},
  location = {Cambridge, UK ; New York, NY},
  isbn = {978-0-521-47261-6 978-0-521-47854-0},
  pagetotal = {438},
  keywords = {Linguistics}
}

@book{radfordLinguisticsIntroduction1999a,
  title = {Linguistics: {{An Introduction}}},
  shorttitle = {Linguistics},
  editor = {Radford, Andrew},
  date = {1999},
  publisher = {Cambridge University Press},
  location = {Cambridge, UK ; New York, NY},
  isbn = {978-0-521-47261-6 978-0-521-47854-0},
  pagetotal = {438},
  keywords = {Linguistics}
}

@article{realeTonotopicOrganizationAuditory1980,
  title = {Tonotopic Organization in Auditory Cortex of the Cat},
  author = {Reale, Richard A. and Imig, Thomas J.},
  date = {1980-07-15},
  journaltitle = {Journal of Comparative Neurology},
  shortjournal = {J of Comparative Neurology},
  volume = {192},
  number = {2},
  pages = {265--291},
  issn = {0021-9967, 1096-9861},
  doi = {10.1002/cne.901920207},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/cne.901920207},
  urldate = {2025-02-23},
  abstract = {Abstract             Microelectrode mapping techniques were employed in the cat's auditory cortex to relate the best frequencies of a large population of neurons with their spatial loci. Based upon the best‐frequency distribution, the auditory region was divided into four complete and orderly tonotopic representations and a surrounding belt of cortex in which the tonotopic organization was more complex. The four auditory fields occupy a crescent‐shaped band of tissue which comprises portions of both the exposed gyral surfaces and sulcal banks of the ectosylvian cortex. The anterior auditory field (A) is situated most rostrally upon the anterior ectosylvian gyrus. It extends upon the ventral bank of the suprasylvian sulcus and upon the banks of the anterior ectosylvian sulcus. Adjoining field A caudally is the primary auditory field (AI), which extends across the middle ectosylvian gyrus and portions of both banks of the posterior ectosylvian sulcus. The representations of the highest best frequencies in fields A and AI are contiguous. Caudal and ventral to AI are located the posterior (P) and ventroposterior (VP) auditory fields. They lie mainly upon the caudal bank of the posterior ectosylvian sulcus but also extend upon the rostral bank and upon the posterior ectosylvian gyrus. The low best‐frequency representations of fields AI and P are contiguous, whereas the low best‐frequency representation of field VP lies near the ventral end of the posterior ectosylvian sulcus. Fields P and VP are joined along their middle and high best‐frequency representations. Within each auditory field isofrequency lines defined by the spatial loci of neurons with similar best frequencies are oriented orthogonal to the low‐to‐high best‐frequency gradients.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/YPNXQJIS/Reale and Imig - 1980 - Tonotopic organization in auditory cortex of the cat.pdf}
}

@article{realeTonotopicOrganizationAuditory1980a,
  title = {Tonotopic {{Organization}} in {{Auditory Cortex}} of the {{Cat}}},
  author = {Reale, Richard A. and Imig, Thomas J.},
  date = {1980-07-15},
  journaltitle = {Journal of Comparative Neurology},
  shortjournal = {J of Comparative Neurology},
  volume = {192},
  number = {2},
  pages = {265--291},
  issn = {0021-9967, 1096-9861},
  doi = {10.1002/cne.901920207},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/cne.901920207},
  urldate = {2025-02-23},
  abstract = {Abstract Microelectrode mapping techniques were employed in the cat's auditory cortex to relate the best frequencies of a large population of neurons with their spatial loci. Based upon the best‐frequency distribution, the auditory region was divided into four complete and orderly tonotopic representations and a surrounding belt of cortex in which the tonotopic organization was more complex. The four auditory fields occupy a crescent‐shaped band of tissue which comprises portions of both the exposed gyral surfaces and sulcal banks of the ectosylvian cortex. The anterior auditory field (A) is situated most rostrally upon the anterior ectosylvian gyrus. It extends upon the ventral bank of the suprasylvian sulcus and upon the banks of the anterior ectosylvian sulcus. Adjoining field A caudally is the primary auditory field (AI), which extends across the middle ectosylvian gyrus and portions of both banks of the posterior ectosylvian sulcus. The representations of the highest best frequencies in fields A and AI are contiguous. Caudal and ventral to AI are located the posterior (P) and ventroposterior (VP) auditory fields. They lie mainly upon the caudal bank of the posterior ectosylvian sulcus but also extend upon the rostral bank and upon the posterior ectosylvian gyrus. The low best‐frequency representations of fields AI and P are contiguous, whereas the low best‐frequency representation of field VP lies near the ventral end of the posterior ectosylvian sulcus. Fields P and VP are joined along their middle and high best‐frequency representations. Within each auditory field isofrequency lines defined by the spatial loci of neurons with similar best frequencies are oriented orthogonal to the low‐to‐high best‐frequency gradients.},
  file = {/home/cristi/Zotero/storage/QM3LEML8/Reale and Imig - 1980 - Tonotopic organization in auditory cortex of the cat.pdf}
}

@article{roskiesNeuroscientificChallengesFree2006,
  title = {Neuroscientific Challenges to Free Will and Responsibility},
  author = {Roskies, Adina},
  date = {2006-09},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {9},
  pages = {419--423},
  issn = {13646613},
  doi = {10.1016/j.tics.2006.07.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661306001902},
  urldate = {2024-06-21},
  langid = {english}
}

@article{roskiesNeuroscientificChallengesFree2006a,
  title = {Neuroscientific Challenges to Free Will and Responsibility},
  author = {Roskies, Adina},
  date = {2006-09},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {9},
  pages = {419--423},
  issn = {13646613},
  doi = {10.1016/j.tics.2006.07.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661306001902},
  urldate = {2024-06-21},
  langid = {english},
  file = {/home/cristi/Zotero/storage/SEUK8WPM/Roskies - 2006 - Neuroscientific challenges to free will and respon.pdf}
}

@article{roskiesNeuroscientificChallengesFree2006b,
  title = {Neuroscientific {{Challenges}} to {{Free Will}} and {{Responsibility}}},
  author = {Roskies, Adina},
  date = {2006-09},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {9},
  pages = {419--423},
  issn = {13646613},
  doi = {10.1016/j.tics.2006.07.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661306001902},
  urldate = {2024-06-21}
}

@article{roskiesNeuroscientificChallengesFree2006c,
  title = {Neuroscientific {{Challenges}} to {{Free Will}} and {{Responsibility}}},
  author = {Roskies, Adina},
  date = {2006-09},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {9},
  pages = {419--423},
  issn = {13646613},
  doi = {10.1016/j.tics.2006.07.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661306001902},
  urldate = {2024-06-21},
  file = {/home/cristi/Zotero/storage/B3DW4UY3/Roskies - 2006 - Neuroscientific challenges to free will and respon.pdf}
}

@book{sacristanroy43rdAnnualInternational2021,
  title = {43rd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} Pre-Conference Workshops \& Social Events: {{Saturday}}, {{October}} 30, 2021, Conference Dates: {{Monday}}, {{November}} 1-{{Friday}}, {{November}} 5, 2021},
  shorttitle = {43rd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} Pre-Conference Workshops \& Social Events},
  author = {Sacristán Roy, Emilio},
  date = {2021},
  publisher = {IEEE},
  location = {Piscataway, NJ},
  isbn = {978-1-7281-1179-7},
  langid = {english},
  annotation = {OCLC: 1314126406}
}

@book{sacristanroy43rdAnnualInternational2021a,
  title = {43rd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society Pre-Conference Workshops}} \& {{Social Events}}: {{Saturday}}, {{October}} 30, 2021, {{Conference Dates}}: {{Monday}}, {{November}} 1-{{Friday}}, {{November}} 5, 2021},
  shorttitle = {43rd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society Pre-Conference Workshops}} \& {{Social Events}}},
  author = {Sacristán Roy, Emilio},
  date = {2021},
  publisher = {IEEE},
  location = {Piscataway, NJ},
  isbn = {978-1-7281-1179-7}
}

@article{saenzTonotopicMappingHuman2014,
  title = {Tonotopic Mapping of Human Auditory Cortex},
  author = {Saenz, Melissa and Langers, Dave R.M.},
  date = {2014-01},
  journaltitle = {Hearing Research},
  shortjournal = {Hearing Research},
  volume = {307},
  pages = {42--52},
  issn = {03785955},
  doi = {10.1016/j.heares.2013.07.016},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378595513001871},
  urldate = {2025-02-26},
  langid = {english},
  file = {/home/cristi/Zotero/storage/VY4D55HV/Saenz and Langers - 2014 - Tonotopic mapping of human auditory cortex.pdf}
}

@article{saenzTonotopicMappingHuman2014a,
  title = {Tonotopic {{Mapping}} of {{Human Auditory Cortex}}},
  author = {Saenz, Melissa and Langers, Dave R.M.},
  date = {2014-01},
  journaltitle = {Hearing Research},
  shortjournal = {Hearing Research},
  volume = {307},
  pages = {42--52},
  issn = {03785955},
  doi = {10.1016/j.heares.2013.07.016},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378595513001871},
  urldate = {2025-02-26},
  file = {/home/cristi/Zotero/storage/LSPJQASW/Saenz and Langers - 2014 - Tonotopic mapping of human auditory cortex.pdf}
}

@book{schultzTwentiethCenturyBorrowings2012,
  title = {Twentieth Century Borrowings from {{French}} to {{English}}: Their Reception and Development},
  shorttitle = {Twentieth Century Borrowings from {{French}} to {{English}}},
  author = {Schultz, Julia},
  date = {2012},
  publisher = {Cambridge Scholars Publishing},
  location = {Newcastle upon Tyne, UK},
  isbn = {978-1-4438-4066-8},
  pagetotal = {579},
  keywords = {English language,Foreign elements French}
}

@book{schultzTwentiethCenturyBorrowings2012a,
  title = {Twentieth {{Century Borrowings}} from {{French}} to {{English}}: {{Their Reception}} and {{Development}}},
  shorttitle = {Twentieth {{Century Borrowings}} from {{French}} to {{English}}},
  author = {Schultz, Julia},
  date = {2012},
  publisher = {Cambridge Scholars Publishing},
  location = {Newcastle upon Tyne, UK},
  isbn = {978-1-4438-4066-8},
  pagetotal = {579},
  keywords = {English language,Foreign elements French}
}

@article{shakyaEvaluationImmuneResponse2011,
  title = {Evaluation of Immune Response to Artificial Infections of {{Haemonchus}} Contortus in {{Gulf Coast Native}} Compared with {{Suffolk}} Lambs},
  author = {Shakya, K. P. and Miller, J. E. and Lomax, L. G. and Burnett, D. D.},
  date = {2011-09-27},
  journaltitle = {Veterinary Parasitology},
  shortjournal = {Vet Parasitol},
  volume = {181},
  number = {2--4},
  eprint = {21570191},
  eprinttype = {pubmed},
  pages = {239--247},
  issn = {1873-2550},
  doi = {10.1016/j.vetpar.2011.03.051},
  abstract = {The Gulf Coast Native (Native) breed of sheep among many others is identified as being relatively resistant to Haemonchus contortus, an abomasal nematode parasite of small ruminants. Understanding the mode of immune response that helps these breeds of sheep control infection could help design and implement appropriate control programs. In this experiment, the components of the immune response during the early infection period in resistant Native lambs were evaluated and compared with susceptible Suffolk breed of sheep. Groups (n=5) of six month old Native and Suffolk lambs were given infective larvae as one time (single) or trickle experimental infections. Fecal, blood, and serum samples were collected on days 0, 2, 7, 14 and 21 post-infection. Abomasal mucosa and regional lymph node samples were collected at the time of necropsy on days 14 and 21. There was no significant difference in number of worms recovered at necropsy but the ratio of adult versus larvae was significantly greater in single infected Suffolk than Native lambs. Native lambs had significantly greater numbers of mast cells and eosinophils in the abomasal mucosa and serum IgG production was significantly greater compared to Suffolk lambs. Native lambs also showed a trend of increased level of serum IgA and IgE compared to Suffolk lambs.},
  langid = {english},
  keywords = {Animals,Feces,Female,Genetic Predisposition to Disease,Haemonchiasis,Immunoglobulins,Mast Cells,Parasite Egg Count,Sheep,Sheep Diseases}
}

@article{shakyaEvaluationImmuneResponse2011a,
  title = {Evaluation of {{Immune Response}} to {{Artificial Infections}} of {{Haemonchus Contortus}} in {{Gulf Coast Native Compared}} with {{Suffolk Lambs}}},
  author = {Shakya, K. P. and Miller, J. E. and Lomax, L. G. and Burnett, D. D.},
  date = {2011-09-27},
  journaltitle = {Veterinary Parasitology},
  shortjournal = {Vet Parasitol},
  volume = {181},
  number = {2--4},
  pages = {239--247},
  issn = {1873-2550},
  doi = {10.1016/j.vetpar.2011.03.051},
  abstract = {The Gulf Coast Native (Native) breed of sheep among many others is identified as being relatively resistant to Haemonchus contortus, an abomasal nematode parasite of small ruminants. Understanding the mode of immune response that helps these breeds of sheep control infection could help design and implement appropriate control programs. In this experiment, the components of the immune response during the early infection period in resistant Native lambs were evaluated and compared with susceptible Suffolk breed of sheep. Groups (n=5) of six month old Native and Suffolk lambs were given infective larvae as one time (single) or trickle experimental infections. Fecal, blood, and serum samples were collected on days 0, 2, 7, 14 and 21 post-infection. Abomasal mucosa and regional lymph node samples were collected at the time of necropsy on days 14 and 21. There was no significant difference in number of worms recovered at necropsy but the ratio of adult versus larvae was significantly greater in single infected Suffolk than Native lambs. Native lambs had significantly greater numbers of mast cells and eosinophils in the abomasal mucosa and serum IgG production was significantly greater compared to Suffolk lambs. Native lambs also showed a trend of increased level of serum IgA and IgE compared to Suffolk lambs.},
  keywords = {Animals,Feces,Female,Genetic Predisposition to Disease,Haemonchiasis,Immunoglobulins,Mast Cells,Parasite Egg Count,Sheep,Sheep Diseases},
  annotation = {\_eprinttype: pmid}
}

@article{sharkeyAutonomousWeaponsSystems2019,
  title = {Autonomous Weapons Systems, Killer Robots and Human Dignity},
  author = {Sharkey, Amanda},
  date = {2019-06},
  journaltitle = {Ethics and Information Technology},
  shortjournal = {Ethics Inf Technol},
  volume = {21},
  number = {2},
  pages = {75--87},
  issn = {1388-1957, 1572-8439},
  doi = {10.1007/s10676-018-9494-0},
  url = {http://link.springer.com/10.1007/s10676-018-9494-0},
  urldate = {2024-01-07},
  langid = {english},
  file = {/home/cristi/Zotero/storage/QIT7QSAN/Sharkey - 2019 - Autonomous weapons systems, killer robots and huma.pdf}
}

@article{sharkeyAutonomousWeaponsSystems2019a,
  title = {Autonomous {{Weapons Systems}}, {{Killer Robots}} and {{Human Dignity}}},
  author = {Sharkey, Amanda},
  date = {2019-06},
  journaltitle = {Ethics and Information Technology},
  shortjournal = {Ethics Inf Technol},
  volume = {21},
  number = {2},
  pages = {75--87},
  issn = {1388-1957, 1572-8439},
  doi = {10.1007/s10676-018-9494-0},
  url = {http://link.springer.com/10.1007/s10676-018-9494-0},
  urldate = {2024-01-07},
  file = {/home/cristi/Zotero/storage/SPE64Z4T/Sharkey - 2019 - Autonomous weapons systems, killer robots and huma.pdf}
}

@article{shiEdgeComputingVision2016,
  title = {Edge {{Computing}}: {{Vision}} and {{Challenges}}},
  shorttitle = {Edge {{Computing}}},
  author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
  date = {2016-10},
  journaltitle = {IEEE Internet of Things Journal},
  shortjournal = {IEEE Internet Things J.},
  volume = {3},
  number = {5},
  pages = {637--646},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2016.2579198},
  url = {http://ieeexplore.ieee.org/document/7488250/},
  urldate = {2025-03-28}
}

@book{siderLibraryVillaPapiri2005,
  title = {The Library of the {{Villa}} Dei {{Papiri}} at {{Herculaneum}}},
  author = {Sider, David},
  date = {2005},
  publisher = {J. Paul Getty Museum},
  location = {Los Angeles},
  isbn = {978-0-89236-799-3},
  pagetotal = {123},
  keywords = {Herculaneum (Extinct city),Italy Herculaneum (Extinct city),Library,Manuscripts Greek (Papyri),Piso Caesoninus Lucius Calpurnius,Private libraries,Villa of the Papyri (Herculaneum)},
  annotation = {OCLC: ocm57193102}
}

@book{siderLibraryVillaPapiri2005a,
  title = {The {{Library}} of the {{Villa Dei Papiri}} at {{Herculaneum}}},
  author = {Sider, David},
  date = {2005},
  publisher = {J. Paul Getty Museum},
  location = {Los Angeles},
  isbn = {978-0-89236-799-3},
  pagetotal = {123},
  keywords = {Herculaneum (Extinct city),Italy Herculaneum (Extinct city),Library,Manuscripts Greek (Papyri),Piso Caesoninus Lucius Calpurnius,Private libraries,Villa of the Papyri (Herculaneum)}
}

@online{SocietalImpactAI,
  title = {Societal {{Impact}} of {{AI}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-intelligent-technology/societal-impact-of-ai},
  urldate = {2022-10-22},
  file = {/home/cristi/Zotero/storage/9GT3CY85/societal-impact-of-ai.html}
}

@online{SocietalImpactAIa,
  title = {Societal {{Impact}} of {{AI}} | {{Radboud University}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-intelligent-technology/societal-impact-of-ai},
  urldate = {2022-10-22},
  file = {/home/cristi/Zotero/storage/PI4TC6LR/societal-impact-of-ai.html}
}

@book{soloveFutureReputationGossip2007,
  title = {The Future of Reputation: Gossip, Rumor, and Privacy on the {{Internet}}},
  shorttitle = {The Future of Reputation},
  author = {Solove, Daniel J.},
  date = {2007},
  publisher = {Yale University Press},
  location = {New Haven},
  isbn = {978-0-300-12498-9},
  pagetotal = {247},
  keywords = {Internet,Law and legislation,Libel and slander,Personality (Law),Privacy Right of,Reputation (Law)},
  annotation = {OCLC: ocn122291821},
  file = {/home/cristi/Zotero/storage/GHUPFHBV/Solove - 2007 - The future of reputation gossip, rumor, and privacy on the Internet.pdf}
}

@inreference{SpectrotemporalReceptiveField2023,
  title = {Spectro-Temporal Receptive Field},
  booktitle = {Wikipedia},
  date = {2023-02-24T03:29:00Z},
  url = {https://en.wikipedia.org/w/index.php?title=Spectro-temporal_receptive_field&oldid=1141251249},
  urldate = {2025-03-12},
  abstract = {The spectro-temporal receptive field or spatio-temporal receptive field (STRF) of a neuron represents which types of stimuli excite or inhibit that neuron.  "Spectro-temporal" refers most commonly to audition, where the neuron's response depends on frequency versus time, while "spatio-temporal" refers to vision, where the neuron's response depends on spatial location versus time. Thus they are not exactly the same concept, but both are referred to as STRF and serve a similar role in the analysis of neural responses. If linearity is assumed, the neuron can be modeled as having a time-varying firing rate equal to the convolution of the stimulus with the STRF.},
  langid = {english},
  annotation = {Page Version ID: 1141251249},
  file = {/home/cristi/Zotero/storage/LH5UPDKI/Spectro-temporal_receptive_field.html}
}

@article{stabileComputationalPlatformVirtual2021,
  title = {A Computational Platform for the Virtual Unfolding of {{Herculaneum Papyri}}},
  author = {Stabile, Sara and Palermo, Francesca and Bukreeva, Inna and Mele, Daniela and Formoso, Vincenzo and Bartolino, Roberto and Cedola, Alessia},
  date = {2021-01-18},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {1695},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-80458-z},
  url = {https://www.nature.com/articles/s41598-020-80458-z},
  urldate = {2023-10-29},
  abstract = {Abstract             Ancient Herculaneum papyrus scrolls, hopelessly charred in the 79 A.D. Vesuvius eruption, contain valuable writings of the Greek philosophers of the day, including works of the Epicurean Philodemus. X-ray~phase~contrast~tomography has recently begun unlocking their secrets. However, only small portions of the text hidden inside the scroll have been recover. One of the challenging tasks in Herculaneum papyri investigation is their virtual unfolding because of their highly complicated structure and three-dimensional arrangement. Although this procedure is feasible, problems in segmentation and flattening hinder the unrolling of a large portion of papyrus. We propose a computational platform for the virtual unfolding procedure, and we show the results of its application on two Herculaneum papyrus fragments. This work paves the way to a comprehensive survey and to further interpretation of larger portions of text hidden inside the carbonized Herculaneum papyri.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/WLB9AVK8/Stabile et al. - 2021 - A computational platform for the virtual unfolding.pdf}
}

@article{stabileComputationalPlatformVirtual2021a,
  title = {A {{Computational Platform}} for the {{Virtual Unfolding}} of {{Herculaneum Papyri}}},
  author = {Stabile, Sara and Palermo, Francesca and Bukreeva, Inna and Mele, Daniela and Formoso, Vincenzo and Bartolino, Roberto and Cedola, Alessia},
  date = {2021-01-18},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {1695},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-80458-z},
  url = {https://www.nature.com/articles/s41598-020-80458-z},
  urldate = {2023-10-29},
  abstract = {Abstract Ancient Herculaneum papyrus scrolls, hopelessly charred in the 79 A.D. Vesuvius eruption, contain valuable writings of the Greek philosophers of the day, including works of the Epicurean Philodemus. X-ray phase contrast tomography has recently begun unlocking their secrets. However, only small portions of the text hidden inside the scroll have been recover. One of the challenging tasks in Herculaneum papyri investigation is their virtual unfolding because of their highly complicated structure and three-dimensional arrangement. Although this procedure is feasible, problems in segmentation and flattening hinder the unrolling of a large portion of papyrus. We propose a computational platform for the virtual unfolding procedure, and we show the results of its application on two Herculaneum papyrus fragments. This work paves the way to a comprehensive survey and to further interpretation of larger portions of text hidden inside the carbonized Herculaneum papyri.},
  file = {/home/cristi/Zotero/storage/CIP7ILPK/Stabile et al. - 2021 - A computational platform for the virtual unfolding.pdf}
}

@article{stanforduniversitygraduateschoolofbusinessandnberusaandizagermanyGigEconomy2020,
  title = {The Gig Economy},
  author = {{Stanford University Graduate School of Business, and NBER, USA, and IZA, Germany} and Oyer, Paul},
  date = {2020},
  journaltitle = {IZA World of Labor},
  shortjournal = {izawol},
  issn = {20549571},
  doi = {10.15185/izawol.471},
  url = {https://wol.iza.org/articles/the-gig-economy/long},
  urldate = {2024-01-07},
  file = {/home/cristi/Zotero/storage/E24JY6DZ/Stanford University Graduate School of Business, and NBER, USA, and IZA, Germany and Oyer - 2020 - The gig economy.pdf}
}

@article{stanforduniversitygraduateschoolofbusinessandnberusaandizagermanyGigEconomy2020a,
  title = {The {{Gig Economy}}},
  author = {{Stanford University Graduate School of Business, and NBER, USA, and IZA, Germany} and Oyer, Paul},
  date = {2020},
  journaltitle = {IZA World of Labor},
  shortjournal = {izawol},
  issn = {20549571},
  doi = {10.15185/izawol.471},
  url = {https://wol.iza.org/articles/the-gig-economy/long},
  urldate = {2024-01-07},
  file = {/home/cristi/Zotero/storage/6DDFLTKB/Stanford University Graduate School of Business, and NBER, USA, and IZA, Germany and Oyer - 2020 - The gig economy.pdf}
}

@online{StudyProgramme,
  title = {Study Programme},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-intelligent-technology/study-programme},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/6BMEE3GD/study-programme.html}
}

@online{StudyProgrammea,
  title = {Study Programme},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-intelligent-technology/study-programme},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/JYMCCCBB/study-programme.html}
}

@online{StudyProgrammeArtificial,
  title = {Study Programme {{Artificial Intelligence}}},
  url = {https://www.ru.nl/en/education/bachelors/artificial-intelligence/study-programme-artificial-intelligence},
  urldate = {2022-09-12},
  file = {/home/cristi/Zotero/storage/VFIF3J9N/study-programme-artificial-intelligence.html}
}

@online{StudyProgrammeArtificiala,
  title = {Study {{Programme Artificial Intelligence}}},
  url = {https://www.ru.nl/en/education/bachelors/artificial-intelligence/study-programme-artificial-intelligence},
  urldate = {2022-09-12},
  file = {/home/cristi/Zotero/storage/AX886FKI/study-programme-artificial-intelligence.html}
}

@online{StudyProgrammeb,
  title = {Study Programme},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-cognitive-computing/study-programme},
  urldate = {2022-10-08},
  file = {/home/cristi/Zotero/storage/9T3YEZHM/study-programme.html}
}

@online{StudyProgrammec,
  title = {Study Programme},
  url = {https://www.ru.nl/en/education/pre-masters/pre-masters-artificial-intelligence-intelligent-technology/study-programme},
  urldate = {2022-10-08},
  file = {/home/cristi/Zotero/storage/5RWF3MUV/study-programme.html}
}

@online{StudyProgrammed,
  title = {Study {{Programme}}},
  url = {https://www.ru.nl/en/education/pre-masters/pre-masters-artificial-intelligence-intelligent-technology/study-programme},
  urldate = {2022-10-08},
  file = {/home/cristi/Zotero/storage/GHTC2SZZ/study-programme.html}
}

@online{StudyProgrammee,
  title = {Study {{Programme}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-cognitive-computing/study-programme},
  urldate = {2022-10-08},
  file = {/home/cristi/Zotero/storage/TYVBKPBS/study-programme.html}
}

@online{StudyProgrammef,
  title = {Study {{Programme}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-intelligent-technology/study-programme},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/7B6I6XFE/study-programme.html}
}

@online{StudyProgrammeg,
  title = {Study {{Programme}}},
  url = {https://www.ru.nl/en/education/masters/artificial-intelligence-intelligent-technology/study-programme},
  urldate = {2022-09-25},
  file = {/home/cristi/Zotero/storage/66JK4ISP/study-programme.html}
}

@article{sunImageNettrainedDeepNeural2021,
  title = {{{ImageNet-trained}} Deep Neural Networks Exhibit Illusion-like Response to the {{Scintillating}} Grid},
  author = {Sun, Eric D. and Dekel, Ron},
  date = {2021-10-22},
  journaltitle = {Journal of Vision},
  shortjournal = {Journal of Vision},
  volume = {21},
  number = {11},
  pages = {15},
  issn = {1534-7362},
  doi = {10.1167/jov.21.11.15},
  url = {https://jov.arvojournals.org/article.aspx?articleid=2778014},
  urldate = {2024-06-16},
  langid = {english},
  file = {/home/cristi/Zotero/storage/L3SLRZII/Sun and Dekel - 2021 - ImageNet-trained deep neural networks exhibit illu.pdf}
}

@article{sunImageNettrainedDeepNeural2021a,
  title = {{{ImageNet-trained Deep Neural Networks Exhibit Illusion-like Response}} to the {{Scintillating Grid}}},
  author = {Sun, Eric D. and Dekel, Ron},
  date = {2021-10-22},
  journaltitle = {Journal of Vision},
  shortjournal = {Journal of Vision},
  volume = {21},
  number = {11},
  pages = {15},
  issn = {1534-7362},
  doi = {10.1167/jov.21.11.15},
  url = {https://jov.arvojournals.org/article.aspx?articleid=2778014},
  urldate = {2024-06-16},
  file = {/home/cristi/Zotero/storage/7U4ZL4IX/Sun and Dekel - 2021 - ImageNet-trained deep neural networks exhibit illu.pdf}
}

@artwork{surangBitcoinWalletFree,
  title = {Bitcoin {{Wallet}} Free Icon},
  author = {{surang}},
  url = {https://cdn-icons-png.flaticon.com/512/2810/2810114.png},
  urldate = {2023-02-21},
  file = {/home/cristi/Zotero/storage/539FVXPC/2810114.html}
}

@artwork{surangBitcoinWalletFreea,
  title = {Bitcoin {{Wallet Free Icon}}},
  author = {{surang}},
  url = {https://cdn-icons-png.flaticon.com/512/2810/2810114.png},
  urldate = {2023-02-21},
  file = {/home/cristi/Zotero/storage/U9K9L28F/2810114.html}
}

@article{syedaFacemapFrameworkModeling2024,
  title = {Facemap: A Framework for Modeling Neural Activity Based on Orofacial Tracking},
  shorttitle = {Facemap},
  author = {Syeda, Atika and Zhong, Lin and Tung, Renee and Long, Will and Pachitariu, Marius and Stringer, Carsen},
  date = {2024-01},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {27},
  number = {1},
  pages = {187--195},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-023-01490-6},
  url = {https://www.nature.com/articles/s41593-023-01490-6},
  urldate = {2025-02-28},
  abstract = {Abstract             Recent studies in mice have shown that orofacial behaviors drive a large fraction of neural activity across the brain. To understand the nature and function of these signals, we need better computational models to characterize the behaviors and relate them to neural activity. Here we developed Facemap, a framework consisting of a keypoint tracker and a deep neural network encoder for predicting neural activity. Our algorithm for tracking mouse orofacial behaviors was more accurate than existing pose estimation tools, while the processing speed was several times faster, making it a powerful tool for real-time experimental interventions. The Facemap tracker was easy to adapt to data from new labs, requiring as few as 10 annotated frames for near-optimal performance. We used the keypoints as inputs to a deep neural network which predicts the activity of \textasciitilde 50,000 simultaneously-recorded neurons and, in visual cortex, we doubled the amount of explained variance compared to previous methods. Using this model, we found that the neuronal activity clusters that were well predicted from behavior were more spatially spread out across cortex. We also found that the deep behavioral features from the model had stereotypical, sequential dynamics that were not reversible in time. In summary, Facemap provides a stepping stone toward understanding the function of the brain-wide neural signals and their relation to behavior.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/G96SG2RH/Syeda et al. - 2024 - Facemap a framework for modeling neural activity based on orofacial tracking.pdf}
}

@article{syedaFacemapFrameworkModeling2024a,
  title = {Facemap: {{A Framework}} for {{Modeling Neural Activity Based}} on {{Orofacial Tracking}}},
  shorttitle = {Facemap},
  author = {Syeda, Atika and Zhong, Lin and Tung, Renee and Long, Will and Pachitariu, Marius and Stringer, Carsen},
  date = {2024-01},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {27},
  number = {1},
  pages = {187--195},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-023-01490-6},
  url = {https://www.nature.com/articles/s41593-023-01490-6},
  urldate = {2025-02-28},
  abstract = {Abstract Recent studies in mice have shown that orofacial behaviors drive a large fraction of neural activity across the brain. To understand the nature and function of these signals, we need better computational models to characterize the behaviors and relate them to neural activity. Here we developed Facemap, a framework consisting of a keypoint tracker and a deep neural network encoder for predicting neural activity. Our algorithm for tracking mouse orofacial behaviors was more accurate than existing pose estimation tools, while the processing speed was several times faster, making it a powerful tool for real-time experimental interventions. The Facemap tracker was easy to adapt to data from new labs, requiring as few as 10 annotated frames for near-optimal performance. We used the keypoints as inputs to a deep neural network which predicts the activity of \textbackslash textasciitilde 50,000 simultaneously-recorded neurons and, in visual cortex, we doubled the amount of explained variance compared to previous methods. Using this model, we found that the neuronal activity clusters that were well predicted from behavior were more spatially spread out across cortex. We also found that the deep behavioral features from the model had stereotypical, sequential dynamics that were not reversible in time. In summary, Facemap provides a stepping stone toward understanding the function of the brain-wide neural signals and their relation to behavior.},
  file = {/home/cristi/Zotero/storage/8DQZTFHA/Syeda et al. - 2024 - Facemap a framework for modeling neural activity based on orofacial tracking.pdf}
}

@inproceedings{tanakaVirtualAgentDesign2021,
  title = {Virtual {{Agent Design}} for {{Social Skills Training Considering Autistic Traits}}},
  booktitle = {2021 43rd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} \& {{Biology Society}} ({{EMBC}})},
  author = {Tanaka, Hiroki and Nakamura, Satoshi},
  date = {2021-11-01},
  pages = {4953--4956},
  publisher = {IEEE},
  location = {Mexico},
  doi = {10.1109/EMBC46164.2021.9630741},
  url = {https://ieeexplore.ieee.org/document/9630741/},
  urldate = {2022-10-06},
  eventtitle = {2021 43rd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} \& {{Biology Society}} ({{EMBC}})},
  isbn = {978-1-7281-1179-7}
}

@inproceedings{tanakaVirtualAgentDesign2021a,
  title = {Virtual {{Agent Design}} for {{Social Skills Training Considering Autistic Traits}}},
  booktitle = {2021 43rd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} \& {{Biology Society}} ({{EMBC}})},
  author = {Tanaka, Hiroki and Nakamura, Satoshi},
  date = {2021-11-01},
  pages = {4953--4956},
  publisher = {IEEE},
  location = {Mexico},
  doi = {10.1109/EMBC46164.2021.9630741},
  url = {https://ieeexplore.ieee.org/document/9630741/},
  urldate = {2022-10-06},
  eventtitle = {2021 43rd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} \& {{Biology Society}} ({{EMBC}})},
  isbn = {978-1-7281-1179-7}
}

@article{thagardCoherenceConstraintSatisfaction1998,
  title = {Coherence as {{Constraint Satisfaction}}},
  author = {Thagard, Paul and Verbeurgt, Karsten},
  date = {1998-01},
  journaltitle = {Cognitive Science},
  shortjournal = {Cognitive Science},
  volume = {22},
  number = {1},
  pages = {1--24},
  issn = {0364-0213, 1551-6709},
  doi = {10.1207/s15516709cog2201_1},
  url = {https://onlinelibrary.wiley.com/doi/10.1207/s15516709cog2201_1},
  urldate = {2024-10-01},
  abstract = {This paper provides a computational characterization of coherence that applies to a wide range of philosophical problems and psychological phenomena. Maximizing coherence is a matter of maximizing satisfaction of a set of positive and negative constraints. After comparing five algorithms for maximizing coherence, we show how our characterization of coherence overcomes traditional philosophical objections about circularity and truth.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/Y4LFKJU2/Thagard and Verbeurgt - 1998 - Coherence as Constraint Satisfaction.pdf}
}

@article{thagardCoherenceConstraintSatisfaction1998a,
  title = {Coherence as {{Constraint Satisfaction}}},
  author = {Thagard, Paul and Verbeurgt, Karsten},
  date = {1998-01},
  journaltitle = {Cognitive Science},
  shortjournal = {Cognitive Science},
  volume = {22},
  number = {1},
  pages = {1--24},
  issn = {0364-0213, 1551-6709},
  doi = {10.1207/s15516709cog2201_1},
  url = {https://onlinelibrary.wiley.com/doi/10.1207/s15516709cog2201_1},
  urldate = {2024-10-01},
  abstract = {This paper provides a computational characterization of coherence that applies to a wide range of philosophical problems and psychological phenomena. Maximizing coherence is a matter of maximizing satisfaction of a set of positive and negative constraints. After comparing five algorithms for maximizing coherence, we show how our characterization of coherence overcomes traditional philosophical objections about circularity and truth.},
  file = {/home/cristi/Zotero/storage/TRFDDY6U/Thagard and Verbeurgt - 1998 - Coherence as Constraint Satisfaction.pdf}
}

@article{theunissenEstimatingSpatiotemporalReceptive2001,
  title = {Estimating Spatio-Temporal Receptive Fields of Auditory and Visual Neurons from Their Responses to Natural Stimuli},
  author = {Theunissen, F.E. and David, S.V. and Singh, N.C. and Hsu, A. and Vinje, W.E. and Gallant, J.L.},
  date = {2001-01},
  journaltitle = {Network: Computation in Neural Systems},
  shortjournal = {Network: Computation in Neural Systems},
  volume = {12},
  number = {3},
  pages = {289--316},
  issn = {0954-898X, 1361-6536},
  doi = {10.1080/net.12.3.289.316},
  url = {https://www.tandfonline.com/doi/full/10.1080/net.12.3.289.316},
  urldate = {2025-01-16},
  langid = {english},
  file = {/home/cristi/Zotero/storage/FKGBNTM9/Theunissen et al. - 2001 - Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to n.pdf}
}

@article{theunissenEstimatingSpatioTemporalReceptive2001a,
  title = {Estimating {{Spatio-Temporal Receptive Fields}} of {{Auditory}} and {{Visual Neurons}} from {{Their Responses}} to {{Natural Stimuli}}},
  author = {Theunissen, F.E. and David, S.V. and Singh, N.C. and Hsu, A. and Vinje, W.E. and Gallant, J.L.},
  date = {2001-01},
  journaltitle = {Network: Computation in Neural Systems},
  shortjournal = {Network: Computation in Neural Systems},
  volume = {12},
  number = {3},
  pages = {289--316},
  issn = {0954-898X, 1361-6536},
  doi = {10.1080/net.12.3.289.316},
  url = {https://www.tandfonline.com/doi/full/10.1080/net.12.3.289.316},
  urldate = {2025-01-16},
  file = {/home/cristi/Zotero/storage/CCUQ7SI8/Theunissen et al. - 2001 - Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to n.pdf}
}

@book{trappenbergFundamentalsComputationalNeuroscience2023,
  title = {Fundamentals of Computational Neuroscience},
  author = {Trappenberg, Thomas P.},
  date = {2023},
  edition = {Third edition},
  publisher = {Oxford University Press},
  location = {Oxford New York, NY},
  isbn = {978-0-19-286936-4 978-0-19-269612-0 978-0-19-196541-8 978-0-19-269613-7},
  langid = {english},
  pagetotal = {1},
  file = {/home/cristi/Zotero/storage/8TVJZMUU/Trappenberg - 2023 - Fundamentals of computational neuroscience.pdf}
}

@book{trappenbergFundamentalsComputationalNeuroscience2023a,
  title = {Fundamentals of {{Computational Neuroscience}}},
  author = {Trappenberg, Thomas P.},
  date = {2023},
  edition = {Third edition},
  publisher = {Oxford University Press},
  location = {Oxford New York, NY},
  isbn = {978-0-19-286936-4 978-0-19-269612-0 978-0-19-196541-8 978-0-19-269613-7},
  pagetotal = {1},
  file = {/home/cristi/Zotero/storage/8IEXQYBQ/Trappenberg - 2023 - Fundamentals of computational neuroscience.pdf}
}

@article{treffertSavantSyndromeExtraordinary2009,
  title = {The Savant Syndrome: An Extraordinary Condition. {{A}} Synopsis: Past, Present, Future},
  shorttitle = {The Savant Syndrome},
  author = {Treffert, Darold A.},
  date = {2009-05-27},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Phil. Trans. R. Soc. B},
  volume = {364},
  number = {1522},
  pages = {1351--1357},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2008.0326},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2008.0326},
  urldate = {2024-01-10},
  abstract = {Savant syndrome is a rare, but extraordinary, condition in which persons with serious mental disabilities, including autistic disorder, have some ‘island of genius’ which stands in marked, incongruous contrast to overall handicap. As many as one in 10 persons with autistic disorder have such remarkable abilities in varying degrees, although savant syndrome occurs in other developmental disabilities or in other types of central nervous system injury or disease as well. Whatever the particular savant skill, it is always linked to massive memory. This paper presents a brief review of the phenomenology of savant skills, the history of the concept and implications for education and future research.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/TYVIJSP6/Treffert - 2009 - The savant syndrome an extraordinary condition. A.pdf}
}

@article{treffertSavantSyndromeExtraordinary2009a,
  title = {The {{Savant Syndrome}}: {{An Extraordinary Condition}}. {{A Synopsis}}: {{Past}}, {{Present}}, {{Future}}},
  shorttitle = {The {{Savant Syndrome}}},
  author = {Treffert, Darold A.},
  date = {2009-05-27},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Phil. Trans. R. Soc. B},
  volume = {364},
  number = {1522},
  pages = {1351--1357},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2008.0326},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2008.0326},
  urldate = {2024-01-10},
  abstract = {Savant syndrome is a rare, but extraordinary, condition in which persons with serious mental disabilities, including autistic disorder, have some ‘island of genius’ which stands in marked, incongruous contrast to overall handicap. As many as one in 10 persons with autistic disorder have such remarkable abilities in varying degrees, although savant syndrome occurs in other developmental disabilities or in other types of central nervous system injury or disease as well. Whatever the particular savant skill, it is always linked to massive memory. This paper presents a brief review of the phenomenology of savant skills, the history of the concept and implications for education and future research.},
  file = {/home/cristi/Zotero/storage/DIQLNLP5/Treffert - 2009 - The savant syndrome an extraordinary condition. A.pdf}
}

@article{tyagiReviewDeepLearning2022,
  title = {A {{Review}} of {{Deep Learning Techniques}} for {{Crowd Behavior Analysis}}},
  author = {Tyagi, Bhawana and Nigam, Swati and Singh, Rajiv},
  date = {2022-11},
  journaltitle = {Archives of Computational Methods in Engineering},
  shortjournal = {Arch Computat Methods Eng},
  volume = {29},
  number = {7},
  pages = {5427--5455},
  issn = {1134-3060, 1886-1784},
  doi = {10.1007/s11831-022-09772-1},
  url = {https://link.springer.com/10.1007/s11831-022-09772-1},
  urldate = {2025-03-28},
  langid = {english},
  file = {/home/cristi/Zotero/storage/7MXRW3F3/Tyagi et al. - 2022 - A Review of Deep Learning Techniques for Crowd Behavior Analysis.pdf}
}

@book{upwardHistoryEnglishSpelling2011,
  title = {The History of {{English}} Spelling},
  author = {Upward, Christopher and Davidson, George},
  date = {2011},
  series = {The Language Library},
  edition = {1. publ},
  publisher = {Wiley-Blackwell},
  location = {Chichester},
  isbn = {978-1-4443-4296-3 978-1-4051-9023-7 978-1-4051-9024-4},
  langid = {english},
  pagetotal = {376}
}

@book{upwardHistoryEnglishSpelling2011a,
  title = {The {{History}} of {{English Spelling}}},
  author = {Upward, Christopher and Davidson, George},
  date = {2011},
  series = {The {{Language Library}}},
  edition = {1. publ},
  publisher = {Wiley-Blackwell},
  location = {Chichester},
  isbn = {978-1-4443-4296-3 978-1-4051-9023-7 978-1-4051-9024-4},
  pagetotal = {376}
}

@article{vanbergenSensoryUncertaintyDecoded2015,
  title = {Sensory Uncertainty Decoded from Visual Cortex Predicts Behavior},
  author = {Van Bergen, Ruben S and Ji Ma, Wei and Pratte, Michael S and Jehee, Janneke F M},
  date = {2015-12},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {18},
  number = {12},
  pages = {1728--1730},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4150},
  url = {https://www.nature.com/articles/nn.4150},
  urldate = {2024-10-06},
  langid = {english},
  file = {/home/cristi/Zotero/storage/LULCK89U/Van Bergen et al. - 2015 - Sensory uncertainty decoded from visual cortex pre.pdf}
}

@article{vanbergenSensoryUncertaintyDecoded2015a,
  title = {Sensory {{Uncertainty Decoded}} from {{Visual Cortex Predicts Behavior}}},
  author = {Van Bergen, Ruben S and Ji Ma, Wei and Pratte, Michael S and Jehee, Janneke F M},
  date = {2015-12},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {18},
  number = {12},
  pages = {1728--1730},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4150},
  url = {https://www.nature.com/articles/nn.4150},
  urldate = {2024-10-06},
  file = {/home/cristi/Zotero/storage/VHGLB8JG/Van Bergen et al. - 2015 - Sensory uncertainty decoded from visual cortex pre.pdf}
}

@article{vanderheijdenBrightSideInfluence2021,
  title = {On the Bright Side: {{The}} Influence of Brightness on Overall Taste Intensity Perception},
  shorttitle = {On the Bright Side},
  author = {family=Heijden, given=Kimberley, prefix=van der, useprefix=true and Festjens, Anouk and Goukens, Caroline},
  date = {2021-03},
  journaltitle = {Food Quality and Preference},
  shortjournal = {Food Quality and Preference},
  volume = {88},
  pages = {104099},
  issn = {09503293},
  doi = {10.1016/j.foodqual.2020.104099},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0950329320303682},
  urldate = {2023-02-08},
  langid = {english},
  file = {/home/cristi/Zotero/storage/UZXJYHVF/van der Heijden et al. - 2021 - On the bright side The influence of brightness on.pdf}
}

@article{vanwynsbergheCritiquingReasonsMaking2019,
  title = {Critiquing the {{Reasons}} for {{Making Artificial Moral Agents}}},
  author = {Van Wynsberghe, Aimee and Robbins, Scott},
  date = {2019-06},
  journaltitle = {Science and Engineering Ethics},
  shortjournal = {Sci Eng Ethics},
  volume = {25},
  number = {3},
  pages = {719--735},
  issn = {1353-3452, 1471-5546},
  doi = {10.1007/s11948-018-0030-8},
  url = {http://link.springer.com/10.1007/s11948-018-0030-8},
  urldate = {2024-01-07},
  langid = {english},
  file = {/home/cristi/Zotero/storage/5Z7FBA34/Van Wynsberghe and Robbins - 2019 - Critiquing the Reasons for Making Artificial Moral.pdf}
}

@article{vanwynsbergheCritiquingReasonsMaking2019a,
  title = {Critiquing the {{Reasons}} for {{Making Artificial Moral Agents}}},
  author = {Van Wynsberghe, Aimee and Robbins, Scott},
  date = {2019-06},
  journaltitle = {Science and Engineering Ethics},
  shortjournal = {Sci Eng Ethics},
  volume = {25},
  number = {3},
  pages = {719--735},
  issn = {1353-3452, 1471-5546},
  doi = {10.1007/s11948-018-0030-8},
  url = {http://link.springer.com/10.1007/s11948-018-0030-8},
  urldate = {2024-01-07},
  file = {/home/cristi/Zotero/storage/EA7WKBIC/Van Wynsberghe and Robbins - 2019 - Critiquing the Reasons for Making Artificial Moral.pdf}
}

@online{VariabeleEliminationAlgorithm,
  title = {Variabele Elimination Algorithm - 2324 {{AI}}: {{Principles}} \& {{Techniques}} ({{SEM1 V}})},
  url = {https://brightspace.ru.nl/d2l/le/content/431290/viewContent/2259255/View},
  urldate = {2023-12-21},
  file = {/home/cristi/Zotero/storage/8KA7UHFI/View.html}
}

@online{VariabeleEliminationAlgorithma,
  title = {Variabele {{Elimination Algorithm}} - 2324 {{AI}}: {{Principles}} \& {{Techniques}} ({{SEM1 V}})},
  url = {https://brightspace.ru.nl/d2l/le/content/431290/viewContent/2259255/View},
  urldate = {2023-12-21},
  file = {/home/cristi/Zotero/storage/Y6CSPL2Z/View.html}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  file = {/home/cristi/Zotero/storage/W2H82PD5/Vaswani et al. - 2017 - Attention is all you need.pdf}
}

@article{veltmeijerLegalEthicalImplications2025,
  title = {Legal and Ethical Implications of {{AI-based}} Crowd Analysis: The {{AI Act}} and Beyond},
  shorttitle = {Legal and Ethical Implications of {{AI-based}} Crowd Analysis},
  author = {Veltmeijer, Emmeke and Gerritsen, Charlotte},
  date = {2025-01-07},
  journaltitle = {AI and Ethics},
  shortjournal = {AI Ethics},
  issn = {2730-5953, 2730-5961},
  doi = {10.1007/s43681-024-00644-x},
  url = {https://link.springer.com/10.1007/s43681-024-00644-x},
  urldate = {2025-03-24},
  abstract = {Abstract             The increasing global population and the consequent rise in crowded environments have amplified the risks of accidents and tragedies. This underscores the need for effective crowd management strategies, with Artificial Intelligence (AI) holding potential to complement traditional methods. While AI offers promise in analysing crowd dynamics and predicting escalations, its deployment raises significant ethical concerns, regarding privacy, bias, accuracy, and accountability. This paper investigates the legal and ethical implications of AI in automated crowd analysis, with a focus on the European perspective. We examine the effect of the GDPR and the recently accepted AI Act on the field. The study then delves into remaining concerns post-legislation and proposes recommendations for ethical deployment. Key findings highlight challenges in notifying individuals of data usage, protecting vulnerable groups, balancing privacy with safety, and mitigating biased outcomes. Recommendations advocate for non-invasive data collection methods, refraining from predicting and decision-making AI systems, contextual considerations, and individual responsibility. The recommendations offer a foundational framework for ethical AI deployment, with universal applicability to benefit citizens globally.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/8W2I535Q/Veltmeijer and Gerritsen - 2025 - Legal and ethical implications of AI-based crowd analysis the AI Act and beyond.pdf}
}

@article{verdaasdonkEndingRulesWidefield2014,
  title = {\textsc{B} Ending the {{Rules}}: {{Widefield Microscopy}} and the {{Abbe Limit}} of {{Resolution}}},
  author = {Verdaasdonk, Jolien S. and Stephens, Andrew D. and Haase, Julian and Bloom, Kerry},
  date = {2014-02},
  journaltitle = {Journal of Cellular Physiology},
  shortjournal = {Journal Cellular Physiology},
  volume = {229},
  number = {2},
  pages = {132--138},
  issn = {0021-9541, 1097-4652},
  doi = {10.1002/jcp.24439},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/jcp.24439},
  urldate = {2025-03-23},
  abstract = {Abstract                                           One of the most fundamental concepts of microscopy is that of resolution–the ability to clearly distinguish two objects as separate. Recent advances such as structured illumination microscopy (SIM) and point localization techniques including photoactivated localization microscopy (PALM), and stochastic optical reconstruction microscopy (STORM) strive to overcome the inherent limits of resolution of the modern light microscope. These techniques, however, are not always feasible or optimal for live cell imaging. Thus, in this review, we explore three techniques for extracting high resolution data from images acquired on a widefield microscope–deconvolution, model convolution, and Gaussian fitting. Deconvolution is a powerful tool for restoring a blurred image using knowledge of the point spread function (PSF) describing the blurring of light by the microscope, although care must be taken to ensure accuracy of subsequent quantitative analysis. The process of model convolution also requires knowledge of the PSF to blur a simulated image which can then be compared to the experimentally acquired data to reach conclusions regarding its geometry and fluorophore distribution. Gaussian fitting is the basis for point localization microscopy, and can also be applied to tracking spot motion over time or measuring spot shape and size. All together, these three methods serve as powerful tools for high‐resolution imaging using widefield microscopy. J. Cell. Physiol. 229: 132–138, 2014. © 2013 Wiley Periodicals, Inc.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/33YZA893/Verdaasdonk et al. - 2014 - B ending the Rules Widefield Microscopy and the Abbe.pdf}
}

@article{vogtPredictingNeuralActivity2024,
  title = {Predicting Neural Activity from Facial Expressions},
  author = {Vogt, Nina},
  date = {2024-01},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {21},
  number = {1},
  pages = {9--9},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-023-02154-w},
  url = {https://www.nature.com/articles/s41592-023-02154-w},
  urldate = {2025-02-28},
  langid = {english},
  file = {/home/cristi/Zotero/storage/TYNPAPCB/Vogt - 2024 - Predicting neural activity from facial expressions.pdf}
}

@article{vogtPredictingNeuralActivity2024a,
  title = {Predicting {{Neural Activity}} from {{Facial Expressions}}},
  author = {Vogt, Nina},
  date = {2024-01},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  volume = {21},
  number = {1},
  pages = {9--9},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-023-02154-w},
  url = {https://www.nature.com/articles/s41592-023-02154-w},
  urldate = {2025-02-28},
  file = {/home/cristi/Zotero/storage/BH5VEVEA/Vogt - 2024 - Predicting neural activity from facial expressions.pdf}
}

@article{wangOverviewEndtoEndAutomatic2019,
  title = {An {{Overview}} of {{End-to-End Automatic Speech Recognition}}},
  author = {Wang, Dong and Wang, Xiaodong and Lv, Shaohe},
  date = {2019-08-07},
  journaltitle = {Symmetry},
  shortjournal = {Symmetry},
  volume = {11},
  number = {8},
  pages = {1018},
  issn = {2073-8994},
  doi = {10.3390/sym11081018},
  url = {https://www.mdpi.com/2073-8994/11/8/1018},
  urldate = {2023-02-15},
  abstract = {Automatic speech recognition, especially large vocabulary continuous speech recognition, is an important issue in the field of machine learning. For a long time, the hidden Markov model (HMM)-Gaussian mixed model (GMM) has been the mainstream speech recognition framework. But recently, HMM-deep neural network (DNN) model and the end-to-end model using deep learning has achieved performance beyond HMM-GMM. Both using deep learning techniques,},
  langid = {english},
  file = {/home/cristi/Zotero/storage/Q6HXKZVF/Wang et al. - 2019 - An Overview of End-to-End Automatic Speech Recogni.pdf}
}

@article{wangOverviewEndtoEndAutomatic2019a,
  title = {An {{Overview}} of {{End-to-End Automatic Speech Recognition}}},
  author = {Wang, Dong and Wang, Xiaodong and Lv, Shaohe},
  date = {2019-08-07},
  journaltitle = {Symmetry},
  shortjournal = {Symmetry},
  volume = {11},
  number = {8},
  pages = {1018},
  issn = {2073-8994},
  doi = {10.3390/sym11081018},
  url = {https://www.mdpi.com/2073-8994/11/8/1018},
  urldate = {2023-02-15},
  abstract = {Automatic speech recognition, especially large vocabulary continuous speech recognition, is an important issue in the field of machine learning. For a long time, the hidden Markov model (HMM)-Gaussian mixed model (GMM) has been the mainstream speech recognition framework. But recently, HMM-deep neural network (DNN) model and the end-to-end model using deep learning has achieved performance beyond HMM-GMM. Both using deep learning techniques,},
  file = {/home/cristi/Zotero/storage/3U7D6LYR/Wang et al. - 2019 - An Overview of End-to-End Automatic Speech Recogni.pdf}
}

@article{watanabeHybridCTCAttention2017,
  title = {Hybrid {{CTC}}/{{Attention Architecture}} for {{End-to-End Speech Recognition}}},
  author = {Watanabe, Shinji and Hori, Takaaki and Kim, Suyoun and Hershey, John R. and Hayashi, Tomoki},
  date = {2017-12},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  shortjournal = {IEEE J. Sel. Top. Signal Process.},
  volume = {11},
  number = {8},
  pages = {1240--1253},
  issn = {1932-4553, 1941-0484},
  doi = {10.1109/JSTSP.2017.2763455},
  url = {http://ieeexplore.ieee.org/document/8068205/},
  urldate = {2023-03-17},
  file = {/home/cristi/Zotero/storage/SSMSTB9Z/Watanabe et al. - 2017 - Hybrid CTCAttention Architecture for End-to-End S.pdf}
}

@article{watanabeHybridCTCAttention2017a,
  title = {Hybrid {{CTC}}/{{Attention Architecture}} for {{End-to-End Speech Recognition}}},
  author = {Watanabe, Shinji and Hori, Takaaki and Kim, Suyoun and Hershey, John R. and Hayashi, Tomoki},
  date = {2017-12},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  shortjournal = {IEEE J. Sel. Top. Signal Process.},
  volume = {11},
  number = {8},
  pages = {1240--1253},
  issn = {1932-4553, 1941-0484},
  doi = {10.1109/JSTSP.2017.2763455},
  url = {http://ieeexplore.ieee.org/document/8068205/},
  urldate = {2023-03-17},
  file = {/home/cristi/Zotero/storage/SRZR9FTF/Watanabe et al. - 2017 - Hybrid CTCAttention Architecture for End-to-End S.pdf}
}

@article{wenSenseAgencyDriving2019,
  title = {The {{Sense}} of {{Agency}} in {{Driving Automation}}},
  author = {Wen, Wen and Kuroki, Yoshihiro and Asama, Hajime},
  date = {2019-12-03},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {10},
  pages = {2691},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.02691},
  url = {https://www.frontiersin.org/article/10.3389/fpsyg.2019.02691/full},
  urldate = {2024-01-07},
  file = {/home/cristi/Zotero/storage/A73SPA2Q/Wen et al. - 2019 - The Sense of Agency in Driving Automation.pdf}
}

@article{wenSenseAgencyDriving2019a,
  title = {The {{Sense}} of {{Agency}} in {{Driving Automation}}},
  author = {Wen, Wen and Kuroki, Yoshihiro and Asama, Hajime},
  date = {2019-12-03},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {10},
  pages = {2691},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.02691},
  url = {https://www.frontiersin.org/article/10.3389/fpsyg.2019.02691/full},
  urldate = {2024-01-07},
  file = {/home/cristi/Zotero/storage/9QXKAP7P/Wen et al. - 2019 - The Sense of Agency in Driving Automation.pdf}
}

@article{wessingerTonotopyHumanAuditory1997,
  title = {Tonotopy in Human Auditory Cortex Examined with Functional Magnetic Resonance Imaging},
  author = {Wessinger, C. Mark and Buonocore, Michael H. and Kussmaul, Clif L. and Mangun, George R.},
  date = {1997},
  journaltitle = {Human Brain Mapping},
  shortjournal = {Hum. Brain Mapp.},
  volume = {5},
  number = {1},
  pages = {18--25},
  issn = {10659471},
  doi = {10.1002/(SICI)1097-0193(1997)5:1<18::AID-HBM3>3.0.CO;2-Q},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-0193(1997)5:1<18::AID-HBM3>3.0.CO;2-Q},
  urldate = {2025-02-26},
  langid = {english},
  file = {/home/cristi/Zotero/storage/ZEP8UXL8/Wessinger et al. - 1997 - Tonotopy in human auditory cortex examined with functional magnetic resonance imaging.pdf}
}

@article{wessingerTonotopyHumanAuditory1997a,
  title = {Tonotopy in {{Human Auditory Cortex Examined}} with {{Functional Magnetic Resonance Imaging}}},
  author = {Wessinger, C. Mark and Buonocore, Michael H. and Kussmaul, Clif L. and Mangun, George R.},
  date = {1997},
  journaltitle = {Human Brain Mapping},
  shortjournal = {Hum. Brain Mapp.},
  volume = {5},
  number = {1},
  pages = {18--25},
  issn = {10659471},
  doi = {10.1002/(SICI)1097-0193(1997)5:1<18::AID-HBM3>3.0.CO;2-Q},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-0193(1997)5:1<18::AID-HBM3>3.0.CO;2-Q},
  urldate = {2025-02-26},
  file = {/home/cristi/Zotero/storage/KTYC6FDF/Wessinger et al. - 1997 - Tonotopy in human auditory cortex examined with functional magnetic resonance imaging.pdf}
}

@article{wilburnIMPACTTECHNOLOGYBUSINESS2018,
  title = {{{THE IMPACT OF TECHNOLOGY ON BUSINESS AND SOCIETY}}},
  author = {Wilburn, Kathleen M and Wilburn, H Ralph},
  date = {2018},
  volume = {12},
  number = {1},
  abstract = {Technology, specifically the interrelationships of Artificial intelligence (AI), big data, and the Internet of things (IoT), is accelerating its ability to help businesses do more with less and provide better results. Businesses can use technology to decrease time from product idea to product creation and product creation to customer delivery, while using fewer workers. Costs can be cut as automation and robots replace humans who need wages and benefits. Although this will create more products and services at lower prices, it may also decrease the number of consumers for those products and services. There has been significant research in those jobs and activities that can be automated now and in the near future. With jobs disappearing, a new economy is growing that turns employees into contract workers who work from gig to gig in solitude. While this new structure of work may allow some people the work/life balance to pursue their creative goals, for others it may mean a life with no stability or future. The result may be a two-tiered society where the rich can afford expensive products and services, and the poor require governmental assistance because although products can be produced more cheaply, they cannot afford them and so they are not produced.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/3TXDT5MF/Wilburn and Wilburn - 2018 - THE IMPACT OF TECHNOLOGY ON BUSINESS AND SOCIETY.pdf}
}

@article{wilburnIMPACTTECHNOLOGYBUSINESS2018a,
  title = {{{THE IMPACT OF TECHNOLOGY ON BUSINESS AND SOCIETY}}},
  author = {Wilburn, Kathleen M and Wilburn, H Ralph},
  date = {2018},
  volume = {12},
  number = {1},
  abstract = {Technology, specifically the interrelationships of Artificial intelligence (AI), big data, and the Internet of things (IoT), is accelerating its ability to help businesses do more with less and provide better results. Businesses can use technology to decrease time from product idea to product creation and product creation to customer delivery, while using fewer workers. Costs can be cut as automation and robots replace humans who need wages and benefits. Although this will create more products and services at lower prices, it may also decrease the number of consumers for those products and services. There has been significant research in those jobs and activities that can be automated now and in the near future. With jobs disappearing, a new economy is growing that turns employees into contract workers who work from gig to gig in solitude. While this new structure of work may allow some people the work/life balance to pursue their creative goals, for others it may mean a life with no stability or future. The result may be a two-tiered society where the rich can afford expensive products and services, and the poor require governmental assistance because although products can be produced more cheaply, they cannot afford them and so they are not produced.},
  file = {/home/cristi/Zotero/storage/9E43ASR6/Wilburn and Wilburn - 2018 - THE IMPACT OF TECHNOLOGY ON BUSINESS AND SOCIETY.pdf}
}

@article{xueChallengingBayesianConfidence2024,
  title = {Challenging the {{Bayesian}} Confidence Hypothesis in Perceptual Decision-Making},
  author = {Xue, Kai and Shekhar, Medha and Rahnev, Dobromir},
  date = {2024-11-26},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {121},
  number = {48},
  pages = {e2410487121},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2410487121},
  url = {https://pnas.org/doi/10.1073/pnas.2410487121},
  urldate = {2025-01-20},
  abstract = {The Bayesian confidence hypothesis (BCH), which postulates that confidence reflects the posterior probability that a decision is correct, is currently the most prominent theory of confidence. Although several recent studies have found evidence against it in the context of relatively complex tasks, BCH remains dominant for simpler tasks. The major alternative to BCH is the confidence in raw evidence space (CRES) hypothesis, according to which confidence is based directly on the raw sensory evidence without explicit probability computations. Here, we tested these competing hypotheses in the context of perceptual tasks that are assumed to induce Gaussian evidence distributions. We show that providing information about task difficulty gives rise to a basic behavioral signature that distinguishes BCH from CRES models even for simple 2-choice tasks. We examined this signature in three experiments and found that all experiments exhibited behavioral signatures in line with CRES computations but contrary to BCH ones. We further performed an extensive comparison of 16 models that implemented either BCH or CRES confidence computations and systematically differed in their auxiliary assumptions. These model comparisons provided overwhelming support for the CRES models over their BCH counterparts across all model variants and across all three experiments. These observations challenge BCH and instead suggest that humans may make confidence judgments by placing criteria directly in the space of the sensory evidence.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/LUW5KUG9/Xue et al. - 2024 - Challenging the Bayesian confidence hypothesis in perceptual decision-making.pdf}
}

@article{xueChallengingBayesianConfidence2024a,
  title = {Challenging the {{Bayesian Confidence Hypothesis}} in {{Perceptual Decision-Making}}},
  author = {Xue, Kai and Shekhar, Medha and Rahnev, Dobromir},
  date = {2024-11-26},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {121},
  number = {48},
  pages = {e2410487121},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2410487121},
  url = {https://pnas.org/doi/10.1073/pnas.2410487121},
  urldate = {2025-01-20},
  abstract = {The Bayesian confidence hypothesis (BCH), which postulates that confidence reflects the posterior probability that a decision is correct, is currently the most prominent theory of confidence. Although several recent studies have found evidence against it in the context of relatively complex tasks, BCH remains dominant for simpler tasks. The major alternative to BCH is the confidence in raw evidence space (CRES) hypothesis, according to which confidence is based directly on the raw sensory evidence without explicit probability computations. Here, we tested these competing hypotheses in the context of perceptual tasks that are assumed to induce Gaussian evidence distributions. We show that providing information about task difficulty gives rise to a basic behavioral signature that distinguishes BCH from CRES models even for simple 2-choice tasks. We examined this signature in three experiments and found that all experiments exhibited behavioral signatures in line with CRES computations but contrary to BCH ones. We further performed an extensive comparison of 16 models that implemented either BCH or CRES confidence computations and systematically differed in their auxiliary assumptions. These model comparisons provided overwhelming support for the CRES models over their BCH counterparts across all model variants and across all three experiments. These observations challenge BCH and instead suggest that humans may make confidence judgments by placing criteria directly in the space of the sensory evidence.},
  file = {/home/cristi/Zotero/storage/6PIBB4XP/Xue et al. - 2024 - Challenging the Bayesian confidence hypothesis in perceptual decision-making.pdf}
}

@article{yadavSurveyMultilingualModels2022,
  title = {A {{Survey}} of {{Multilingual Models}} for {{Automatic Speech Recognition}}},
  author = {Yadav, Hemant and Sitaram, Sunayana},
  date = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2202.12576},
  url = {https://arxiv.org/abs/2202.12576},
  urldate = {2023-02-12},
  abstract = {Although Automatic Speech Recognition (ASR) systems have achieved human-like performance for a few languages, the majority of the world's languages do not have usable systems due to the lack of large speech datasets to train these models. Cross-lingual transfer is an attractive solution to this problem, because low-resource languages can potentially benefit from higher-resource languages either through transfer learning, or being jointly trained in the same multilingual model. The problem of cross-lingual transfer has been well studied in ASR, however, recent advances in Self Supervised Learning are opening up avenues for unlabeled speech data to be used in multilingual ASR models, which can pave the way for improved performance on low-resource languages. In this paper, we survey the state of the art in multilingual ASR models that are built with cross-lingual transfer in mind. We present best practices for building multilingual models from research across diverse languages and techniques, discuss open questions and provide recommendations for future work.},
  version = {1},
  keywords = {Audio and Speech Processing (eess.AS),Computation and Language (cs.CL),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Sound (cs.SD)},
  file = {/home/cristi/Zotero/storage/GXDY8YE8/Yadav and Sitaram - 2022 - A Survey of Multilingual Models for Automatic Spee.pdf}
}

@article{yadavSurveyMultilingualModels2022a,
  title = {A {{Survey}} of {{Multilingual Models}} for {{Automatic Speech Recognition}}},
  author = {Yadav, Hemant and Sitaram, Sunayana},
  date = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2202.12576},
  url = {https://arxiv.org/abs/2202.12576},
  urldate = {2023-02-12},
  abstract = {Although Automatic Speech Recognition (ASR) systems have achieved human-like performance for a few languages, the majority of the world's languages do not have usable systems due to the lack of large speech datasets to train these models. Cross-lingual transfer is an attractive solution to this problem, because low-resource languages can potentially benefit from higher-resource languages either through transfer learning, or being jointly trained in the same multilingual model. The problem of cross-lingual transfer has been well studied in ASR, however, recent advances in Self Supervised Learning are opening up avenues for unlabeled speech data to be used in multilingual ASR models, which can pave the way for improved performance on low-resource languages. In this paper, we survey the state of the art in multilingual ASR models that are built with cross-lingual transfer in mind. We present best practices for building multilingual models from research across diverse languages and techniques, discuss open questions and provide recommendations for future work.},
  version = {1},
  keywords = {Audio and Speech Processing (eess.AS),Computation and Language (cs.CL),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Sound (cs.SD)},
  file = {/home/cristi/Zotero/storage/LBT3PREK/Yadav and Sitaram - 2022 - A Survey of Multilingual Models for Automatic Spee.pdf}
}

@article{yaminsPerformanceoptimizedHierarchicalModels2014,
  title = {Performance-Optimized Hierarchical Models Predict Neural Responses in Higher Visual Cortex},
  author = {Yamins, Daniel L. K. and Hong, Ha and Cadieu, Charles F. and Solomon, Ethan A. and Seibert, Darren and DiCarlo, James J.},
  date = {2014-06-10},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {111},
  number = {23},
  pages = {8619--8624},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1403112111},
  url = {https://pnas.org/doi/full/10.1073/pnas.1403112111},
  urldate = {2024-04-25},
  abstract = {Significance             Humans and monkeys easily recognize objects in scenes. This ability is known to be supported by a network of hierarchically interconnected brain areas. However, understanding neurons in higher levels of this hierarchy has long remained a major challenge in visual systems neuroscience. We use computational techniques to identify a neural network model that matches human performance on challenging object categorization tasks. Although not explicitly constrained to match neural data, this model turns out to be highly predictive of neural responses in both the V4 and inferior temporal cortex, the top two layers of the ventral visual hierarchy. In addition to yielding greatly improved models of visual cortex, these results suggest that a process of biological performance optimization directly shaped neural mechanisms.           ,              The ventral visual stream underlies key human visual object recognition abilities. However, neural encoding in the higher areas of the ventral stream remains poorly understood. Here, we describe a modeling approach that yields a quantitatively accurate model of inferior temporal (IT) cortex, the highest ventral cortical area. Using high-throughput computational techniques, we discovered that, within a class of biologically plausible hierarchical neural network models, there is a strong correlation between a model’s categorization performance and its ability to predict individual IT neural unit response data. To pursue this idea, we then identified a high-performing neural network that matches human performance on a range of recognition tasks. Critically, even though we did not constrain this model to match neural data, its top output layer turns out to be highly predictive of IT spiking responses to complex naturalistic images at both the single site and population levels. Moreover, the model’s intermediate layers are highly predictive of neural responses in the V4 cortex, a midlevel visual area that provides the dominant cortical input to IT. These results show that performance optimization—applied in a biologically appropriate model class—can be used to build quantitative predictive models of neural processing.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/8I7KJXUT/Yamins et al. - 2014 - Performance-optimized hierarchical models predict .pdf}
}

@article{yaminsPerformanceOptimizedHierarchicalModels2014a,
  title = {Performance-{{Optimized Hierarchical Models Predict Neural Responses}} in {{Higher Visual Cortex}}},
  author = {Yamins, Daniel L. K. and Hong, Ha and Cadieu, Charles F. and Solomon, Ethan A. and Seibert, Darren and DiCarlo, James J.},
  date = {2014-06-10},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {111},
  number = {23},
  pages = {8619--8624},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1403112111},
  url = {https://pnas.org/doi/full/10.1073/pnas.1403112111},
  urldate = {2024-04-25},
  abstract = {Significance Humans and monkeys easily recognize objects in scenes. This ability is known to be supported by a network of hierarchically interconnected brain areas. However, understanding neurons in higher levels of this hierarchy has long remained a major challenge in visual systems neuroscience. We use computational techniques to identify a neural network model that matches human performance on challenging object categorization tasks. Although not explicitly constrained to match neural data, this model turns out to be highly predictive of neural responses in both the V4 and inferior temporal cortex, the top two layers of the ventral visual hierarchy. In addition to yielding greatly improved models of visual cortex, these results suggest that a process of biological performance optimization directly shaped neural mechanisms. , The ventral visual stream underlies key human visual object recognition abilities. However, neural encoding in the higher areas of the ventral stream remains poorly understood. Here, we describe a modeling approach that yields a quantitatively accurate model of inferior temporal (IT) cortex, the highest ventral cortical area. Using high-throughput computational techniques, we discovered that, within a class of biologically plausible hierarchical neural network models, there is a strong correlation between a model’s categorization performance and its ability to predict individual IT neural unit response data. To pursue this idea, we then identified a high-performing neural network that matches human performance on a range of recognition tasks. Critically, even though we did not constrain this model to match neural data, its top output layer turns out to be highly predictive of IT spiking responses to complex naturalistic images at both the single site and population levels. Moreover, the model’s intermediate layers are highly predictive of neural responses in the V4 cortex, a midlevel visual area that provides the dominant cortical input to IT. These results show that performance optimization—applied in a biologically appropriate model class—can be used to build quantitative predictive models of neural processing.},
  file = {/home/cristi/Zotero/storage/LMS9FU4S/Yamins et al. - 2014 - Performance-optimized hierarchical models predict .pdf}
}

@article{yangAuditoryRepresentationsAcoustic1992,
  title = {Auditory Representations of Acoustic Signals},
  author = {Yang, X. and Wang, K. and Shamma, S.A.},
  date = {1992-03},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Trans. Inform. Theory},
  volume = {38},
  number = {2},
  pages = {824--839},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/18.119739},
  url = {http://ieeexplore.ieee.org/document/119739/},
  urldate = {2025-02-14},
  file = {/home/cristi/Zotero/storage/M5VYFUMI/Yang et al. - 1992 - Auditory representations of acoustic signals.pdf}
}

@article{yangAuditoryRepresentationsAcoustic1992a,
  title = {Auditory {{Representations}} of {{Acoustic Signals}}},
  author = {Yang, X. and Wang, K. and Shamma, S.A.},
  date = {1992-03},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Trans. Inform. Theory},
  volume = {38},
  number = {2},
  pages = {824--839},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/18.119739},
  url = {http://ieeexplore.ieee.org/document/119739/},
  urldate = {2025-02-14},
  file = {/home/cristi/Zotero/storage/KWI4XCAY/Yang et al. - 1992 - Auditory representations of acoustic signals.pdf}
}

@article{yangHarnessingPowerLLMs2024,
  title = {Harnessing the {{Power}} of {{LLMs}} in {{Practice}}: {{A Survey}} on {{ChatGPT}} and {{Beyond}}},
  shorttitle = {Harnessing the {{Power}} of {{LLMs}} in {{Practice}}},
  author = {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia},
  date = {2024-07-31},
  journaltitle = {ACM Transactions on Knowledge Discovery from Data},
  shortjournal = {ACM Trans. Knowl. Discov. Data},
  volume = {18},
  number = {6},
  pages = {1--32},
  issn = {1556-4681, 1556-472X},
  doi = {10.1145/3649506},
  url = {https://dl.acm.org/doi/10.1145/3649506},
  urldate = {2025-04-06},
  abstract = {This article presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream Natural Language Processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. First, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at               https://github.com/Mooler0410/LLMsPracticalGuide               . An LLMs evolutionary tree, editable yet regularly updated, can be found at               llmtree.ai               .},
  langid = {english},
  file = {/home/cristi/Zotero/storage/ZHY24DVW/Yang et al. - 2024 - Harnessing the Power of LLMs in Practice A Survey on ChatGPT and Beyond.pdf}
}

@article{zhaoExplainabilityLargeLanguage2024,
  title = {Explainability for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Explainability for {{Large Language Models}}},
  author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  date = {2024-04-30},
  journaltitle = {ACM Transactions on Intelligent Systems and Technology},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  volume = {15},
  number = {2},
  pages = {1--38},
  issn = {2157-6904, 2157-6912},
  doi = {10.1145/3639372},
  url = {https://dl.acm.org/doi/10.1145/3639372},
  urldate = {2025-04-06},
  abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/4G2C4QZH/Zhao et al. - 2024 - Explainability for Large Language Models A Survey.pdf;/home/cristi/Zotero/storage/IMWSZ9GH/3639372.pdf}
}

@incollection{zotero-1,
  type = {incollection}
}

@book{zotero-356,
  type = {book}
}

@online{zotero-38,
  type = {online}
}

@online{zotero-568,
  type = {online}
}

@book{zotero-569,
  type = {book}
}

@online{zotero-571,
  type = {online}
}

@online{zotero-572,
  type = {online}
}

@incollection{zotero-573,
  type = {incollection}
}

@online{zotero-undefined,
  type = {online}
}

@online{zotero-undefineda,
  type = {online}
}

@article{zuboffBigOtherSurveillance2015,
  title = {Big Other: {{Surveillance Capitalism}} and the {{Prospects}} of an {{Information Civilization}}},
  shorttitle = {Big Other},
  author = {Zuboff, Shoshana},
  date = {2015-03},
  journaltitle = {Journal of Information Technology},
  shortjournal = {Journal of Information Technology},
  volume = {30},
  number = {1},
  pages = {75--89},
  issn = {0268-3962, 1466-4437},
  doi = {10.1057/jit.2015.5},
  url = {http://journals.sagepub.com/doi/10.1057/jit.2015.5},
  urldate = {2024-01-07},
  abstract = {This article describes an emergent logic of accumulation in the networked sphere, ‘surveillance capitalism,’ and considers its implications for ‘information civilization.’ The institutionalizing practices and operational assumptions of Google Inc. are the primary lens for this analysis as they are rendered in two recent articles authored by Google Chief Economist Hal Varian. Varian asserts four uses that follow from computer-mediated transactions: data extraction and analysis,’ ‘new contractual forms due to better monitoring,’ ‘personalization and customization, ’ and continuous experiments. ’ An examination of the nature and consequences of these uses sheds light on the implicit logic of surveillance capitalism and the global architecture of computer mediation upon which it depends. This architecture produces a distributed and largely uncontested new expression of power that I christen: Big Other. ’ It is constituted by unexpected and often illegible mechanisms of extraction, commodification, and control that effectively exile persons from their own behavior while producing new markets of behavioral prediction and modification. Surveillance capitalism challenges democratic norms and departs in key ways from the centuries-long evolution of market capitalism.},
  langid = {english},
  file = {/home/cristi/Zotero/storage/4CRUEKJT/Zuboff - 2015 - Big other Surveillance Capitalism and the Prospec.pdf}
}

@article{zuboffBigOtherSurveillance2015a,
  title = {Big {{Other}}: {{Surveillance Capitalism}} and the {{Prospects}} of an {{Information Civilization}}},
  shorttitle = {Big {{Other}}},
  author = {Zuboff, Shoshana},
  date = {2015-03},
  journaltitle = {Journal of Information Technology},
  shortjournal = {Journal of Information Technology},
  volume = {30},
  number = {1},
  pages = {75--89},
  issn = {0268-3962, 1466-4437},
  doi = {10.1057/jit.2015.5},
  url = {http://journals.sagepub.com/doi/10.1057/jit.2015.5},
  urldate = {2024-01-07},
  abstract = {This article describes an emergent logic of accumulation in the networked sphere, ‘surveillance capitalism,’ and considers its implications for ‘information civilization.’ The institutionalizing practices and operational assumptions of Google Inc. are the primary lens for this analysis as they are rendered in two recent articles authored by Google Chief Economist Hal Varian. Varian asserts four uses that follow from computer-mediated transactions: data extraction and analysis,’ ‘new contractual forms due to better monitoring,’ ‘personalization and customization, ’ and continuous experiments. ’ An examination of the nature and consequences of these uses sheds light on the implicit logic of surveillance capitalism and the global architecture of computer mediation upon which it depends. This architecture produces a distributed and largely uncontested new expression of power that I christen: Big Other. ’ It is constituted by unexpected and often illegible mechanisms of extraction, commodification, and control that effectively exile persons from their own behavior while producing new markets of behavioral prediction and modification. Surveillance capitalism challenges democratic norms and departs in key ways from the centuries-long evolution of market capitalism.},
  file = {/home/cristi/Zotero/storage/EPXMZX9D/Zuboff - 2015 - Big other Surveillance Capitalism and the Prospec.pdf}
}
